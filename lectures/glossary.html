<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Machine Learning Glossary – QTM 447 - Statistical Machine Learning II</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-4a990d8dcb58f517c7c86712b8f2ac7c.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-200c0835bd00cf5a6f75ba8d52a48539.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
<link rel="stylesheet" href="../additional-styles.css">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">QTM 447 - Statistical Machine Learning II</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../syllabus.html"> 
<span class="menu-text">Syllabus</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../lectures/index.html"> 
<span class="menu-text">Lectures</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link active" href="../lectures/glossary.html" aria-current="page"> 
<span class="menu-text">Glossary</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../lectures/resources.html"> 
<span class="menu-text">Resources</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../lectures/search.html"> <i class="bi bi-search" role="img">
</i> 
<span class="menu-text">Search Course Materials</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
        
    </div>
<!-- main -->
<main class="content column-page" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Machine Learning Glossary</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i></button></div></div>
</div>



<div class="quarto-title-meta column-page">

    
  
    
  </div>
  


</header>


<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>This glossary provides definitions for key machine learning concepts covered in QTM 447. Each entry includes a concise, academically-focused explanation derived directly from course lectures, along with links to the specific lectures where the concept is discussed in detail. Use this resource as a reference guide throughout the course.</p>
</section>
<section id="a" class="level2">
<h2 class="anchored" data-anchor-id="a">A</h2>
<section id="accuracy" class="level3">
<h3 class="anchored" data-anchor-id="accuracy">Accuracy</h3>
<p>A performance metric quantifying the proportion of correct predictions (both true positives and true negatives) among all cases examined. While useful for balanced datasets, accuracy can be misleading when class distributions are skewed.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture3.html">Lecture 3</a> (Generalization Error), <a href="../lectures/lecture4.html">Lecture 4</a> (Linear Models and Likelihood)</p>
</section>
<section id="activation-function" class="level3">
<h3 class="anchored" data-anchor-id="activation-function">Activation Function</h3>
<p>Mathematical functions applied to neural network layer outputs that introduce non-linearity, enabling networks to learn complex patterns. Key activation functions include:</p>
<ul>
<li><strong>ReLU (Rectified Linear Unit):</strong> <span class="math inline">\(f(x) = \max(0, x)\)</span>, widely used due to efficient gradient computation and helping mitigate vanishing gradients</li>
<li><strong>Sigmoid:</strong> <span class="math inline">\(f(x) = \frac{1}{1+e^{-x}}\)</span>, maps values to [0,1], useful for binary classification outputs</li>
<li><strong>Tanh:</strong> <span class="math inline">\(f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}\)</span>, maps values to [-1,1], often performs better than sigmoid in hidden layers</li>
</ul>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture8.html">Lecture 8</a> (Introduction to Neural Networks), <a href="../lectures/lecture9.html">Lecture 9</a> (Deep Neural Networks)</p>
</section>
<section id="adam-optimizer" class="level3">
<h3 class="anchored" data-anchor-id="adam-optimizer">Adam Optimizer</h3>
<p>An adaptive learning rate optimization algorithm that combines ideas from momentum and RMSProp, maintaining per-parameter learning rates adapted based on first and second moments of gradients. Features include:</p>
<ul>
<li>Bias correction for moments</li>
<li>Adaptive step sizes for each parameter</li>
<li>Computational efficiency</li>
<li>Good performance across a wide range of problems without extensive hyperparameter tuning</li>
</ul>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture6.html">Lecture 6</a> (Adaptive Methods for Minimization)</p>
</section>
<section id="area-under-the-curve-auc" class="level3">
<h3 class="anchored" data-anchor-id="area-under-the-curve-auc">Area Under the Curve (AUC)</h3>
<p>A performance metric for binary classification that measures the area under the ROC curve. AUC represents the probability that a randomly chosen positive instance ranks higher than a randomly chosen negative instance. An AUC of 1.0 indicates perfect ranking, while 0.5 represents random chance.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture2.html">Lecture 2</a> (Machine Learning Fundamentals)</p>
</section>
<section id="attention-mechanism" class="level3">
<h3 class="anchored" data-anchor-id="attention-mechanism">Attention Mechanism</h3>
<p>A neural network component that allows models to focus on different parts of the input when producing outputs. Attention computes weighted sums of input elements based on relevance scores, enabling models to selectively emphasize important information. This mechanism is fundamental to modern sequence processing architectures like Transformers.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture18.html">Lecture 18</a> (Transformers and the Attention Revolution)</p>
</section>
<section id="autoencoder" class="level3">
<h3 class="anchored" data-anchor-id="autoencoder">Autoencoder</h3>
<p>Neural networks designed to learn efficient data representations (encodings) in an unsupervised manner by attempting to reconstruct their own inputs. The architecture consists of:</p>
<ul>
<li><strong>Encoder:</strong> Compresses input data into a lower-dimensional latent representation</li>
<li><strong>Bottleneck/latent space:</strong> The compressed representation capturing essential features</li>
<li><strong>Decoder:</strong> Reconstructs the original input from the latent representation</li>
</ul>
<p>Applications include dimensionality reduction, denoising, anomaly detection, and as components in generative models.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture20.html">Lecture 20</a> (Generative Models: Autoregressives and Autoencoders)</p>
</section>
<section id="autoregressive-models" class="level3">
<h3 class="anchored" data-anchor-id="autoregressive-models">Autoregressive Models</h3>
<p>Models that predict sequential outputs where each element depends on previously generated elements, modeling the probability distribution as:</p>
<p><span class="math display">\[p(x_1, x_2, ..., x_n) = \prod_{i=1}^n p(x_i | x_1, x_2, ..., x_{i-1})\]</span></p>
<p>Used extensively in sequential data modeling such as language models, time series forecasting, and some generative models. Examples include traditional AR models, RNN-based language models, and GPT architectures.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture16.html">Lecture 16</a> (Recurrent Neural Networks and Sequence Modeling), <a href="../lectures/lecture20.html">Lecture 20</a> (Generative Models: Autoregressives and Autoencoders)</p>
</section>
</section>
<section id="b" class="level2">
<h2 class="anchored" data-anchor-id="b">B</h2>
<section id="backpropagation" class="level3">
<h3 class="anchored" data-anchor-id="backpropagation">Backpropagation</h3>
<p>The primary algorithm for training neural networks by computing gradients of the loss function with respect to model parameters. Consists of two main phases:</p>
<ol type="1">
<li><strong>Forward pass:</strong> Computing all intermediate values through the network to calculate the output and loss</li>
<li><strong>Backward pass:</strong> Efficiently calculating gradients using the chain rule by propagating derivatives backward from the loss</li>
</ol>
<p>The algorithm leverages computational graphs to track dependencies and facilitates efficient parameter updates through gradient-based optimization.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture10.html">Lecture 10</a> (Backpropagation and Gradient Problems)</p>
</section>
<section id="batch-normalization" class="level3">
<h3 class="anchored" data-anchor-id="batch-normalization">Batch Normalization</h3>
<p>A technique that normalizes layer inputs across mini-batches during training, maintaining internal representations with zero mean and unit variance:</p>
<p><span class="math display">\[\hat{x}^{(k)} = \frac{x^{(k)} - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}\]</span></p>
<p>After normalization, learnable parameters <span class="math inline">\(\gamma\)</span> and <span class="math inline">\(\beta\)</span> allow the network to undo the normalization if optimal:</p>
<p><span class="math display">\[y^{(k)} = \gamma^{(k)} \hat{x}^{(k)} + \beta^{(k)}\]</span></p>
<p>Advantages include:</p>
<ul>
<li>Accelerated training by reducing internal covariate shift</li>
<li>Improved gradient flow</li>
<li>Regularizing effect that reduces overfitting</li>
<li>Enables higher learning rates</li>
</ul>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture11.html">Lecture 11</a> (Vanishing Gradients and Generalization), <a href="../lectures/lecture13.html">Lecture 13</a> (Advanced CNN Architectures and Transfer Learning)</p>
</section>
<section id="bayesian-machine-learning" class="level3">
<h3 class="anchored" data-anchor-id="bayesian-machine-learning">Bayesian Machine Learning</h3>
<p>A probabilistic approach to machine learning that applies Bayesian statistics to model uncertainty by:</p>
<ul>
<li>Incorporating prior beliefs about parameters using probability distributions</li>
<li>Updating these beliefs with observed data using Bayes’ theorem</li>
<li>Producing posterior distributions over parameters rather than point estimates</li>
<li>Enabling principled uncertainty quantification in predictions</li>
</ul>
<p>Key methods include Bayesian linear regression, Bayesian neural networks, and Gaussian processes.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture22.html">Lecture 22</a> (Bayesian Machine Learning)</p>
</section>
<section id="bias-variance-tradeoff" class="level3">
<h3 class="anchored" data-anchor-id="bias-variance-tradeoff">Bias-Variance Tradeoff</h3>
<p>A fundamental concept in statistical learning theory that decomposes prediction error into three components:</p>
<ul>
<li><strong>Bias:</strong> Systematic error from incorrect model assumptions or insufficient complexity</li>
<li><strong>Variance:</strong> Error from sensitivity to training data fluctuations</li>
<li><strong>Irreducible error:</strong> Inherent noise that cannot be eliminated</li>
</ul>
<p>As model complexity increases, bias typically decreases while variance increases. Finding the optimal complexity minimizes the combined error (bias² + variance) to achieve better generalization.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture3.html">Lecture 3</a> (Generalization Error)</p>
</section>
<section id="boosting" class="level3">
<h3 class="anchored" data-anchor-id="boosting">Boosting</h3>
<p>An ensemble learning approach where weak learners (typically shallow decision trees) are trained sequentially, with each new model focusing on the mistakes of previous models. Examples include:</p>
<ul>
<li><strong>AdaBoost:</strong> Adjusts instance weights to emphasize previously misclassified examples</li>
<li><strong>Gradient Boosting:</strong> Iteratively fits models to the residual errors of previous models</li>
<li><strong>XGBoost/LightGBM:</strong> Optimized implementations with regularization and efficient training</li>
</ul>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture27.html">Lecture 27</a> (Interpretable Machine Learning)</p>
</section>
</section>
<section id="c" class="level2">
<h2 class="anchored" data-anchor-id="c">C</h2>
<section id="categorical-cross-entropy-loss" class="level3">
<h3 class="anchored" data-anchor-id="categorical-cross-entropy-loss">Categorical Cross-Entropy Loss</h3>
<p>A loss function for multi-class classification problems measuring the difference between predicted probability distribution <span class="math inline">\(\hat{p}\)</span> and true distribution <span class="math inline">\(p\)</span> (usually one-hot encoded):</p>
<p><span class="math display">\[L = -\sum_{i=1}^C p_i \log(\hat{p}_i)\]</span></p>
<p>Where <span class="math inline">\(C\)</span> is the number of classes, <span class="math inline">\(p_i\)</span> is the true probability of class <span class="math inline">\(i\)</span> (typically 0 or 1), and <span class="math inline">\(\hat{p}_i\)</span> is the predicted probability. Minimizing this loss maximizes the log-likelihood of the correct classifications.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture2.html">Lecture 2</a> (Machine Learning Fundamentals), <a href="../lectures/lecture5.html">Lecture 5</a> (Optimization Foundations and Stochastic Gradient Descent), <a href="../lectures/lecture8.html">Lecture 8</a> (Introduction to Neural Networks)</p>
</section>
<section id="computational-graph" class="level3">
<h3 class="anchored" data-anchor-id="computational-graph">Computational Graph</h3>
<p>A directed acyclic graph representation of mathematical operations where nodes represent operations (addition, multiplication, activation functions) and edges represent data flow. This structure enables:</p>
<ul>
<li>Systematic computation of gradients using the chain rule</li>
<li>Efficient implementation of backpropagation by storing intermediate values</li>
<li>Automatic differentiation in deep learning frameworks</li>
</ul>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture10.html">Lecture 10</a> (Backpropagation and Gradient Problems)</p>
</section>
<section id="confusion-matrix" class="level3">
<h3 class="anchored" data-anchor-id="confusion-matrix">Confusion Matrix</h3>
<p>A table used to evaluate classification model performance, showing the counts of true positives, true negatives, false positives, and false negatives. Helps calculate precision, recall, F1 score, and other metrics.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture2.html">Lecture 2</a> (Machine Learning Fundamentals), <a href="../lectures/lecture3.html">Lecture 3</a> (Generalization Error)</p>
</section>
<section id="convolutional-neural-network-cnn" class="level3">
<h3 class="anchored" data-anchor-id="convolutional-neural-network-cnn">Convolutional Neural Network (CNN)</h3>
<p>Neural network architectures specialized for processing grid-structured data (especially images) through specialized layers:</p>
<ul>
<li><strong>Convolutional layers:</strong> Apply learned filters across spatial dimensions, detecting local patterns while sharing parameters</li>
<li><strong>Pooling layers:</strong> Reduce spatial dimensions by summarizing regions (max pooling, average pooling)</li>
<li><strong>Fully connected layers:</strong> Typically used in final stages for classification or regression</li>
</ul>
<p>Key benefits include translation invariance, parameter efficiency through weight sharing, and hierarchical feature learning. Notable architectures include LeNet, AlexNet, VGG, and ResNet.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture12.html">Lecture 12</a> (Image Classification with Convolutional Neural Networks), <a href="../lectures/lecture13.html">Lecture 13</a> (Advanced CNN Architectures and Transfer Learning)</p>
</section>
<section id="cross-validation" class="level3">
<h3 class="anchored" data-anchor-id="cross-validation">Cross-Validation</h3>
<p>A model evaluation technique that partitions data into training and validation sets multiple times to estimate performance. Common approaches include:</p>
<ul>
<li><strong>K-fold cross-validation:</strong> Divides data into k equal subsets (folds), trains on k-1 folds and tests on the remaining fold, rotating through all folds</li>
<li><strong>Leave-one-out cross-validation:</strong> Uses a single observation for validation and all others for training, repeated for each observation</li>
<li><strong>Stratified cross-validation:</strong> Maintains class distribution proportions in each fold</li>
</ul>
<p>Cross-validation provides more reliable performance estimates than single train-test splits, especially with limited data.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture3.html">Lecture 3</a> (Generalization Error)</p>
</section>
<section id="curriculum-learning" class="level3">
<h3 class="anchored" data-anchor-id="curriculum-learning">Curriculum Learning</h3>
<p>Training strategy where models are taught easier concepts before harder ones, similar to how humans learn. This approach can lead to better performance and faster convergence by gradually increasing task complexity during training.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture13.html">Lecture 13</a> (Advanced CNN Architectures and Transfer Learning)</p>
</section>
</section>
<section id="d" class="level2">
<h2 class="anchored" data-anchor-id="d">D</h2>
<section id="data-augmentation" class="level3">
<h3 class="anchored" data-anchor-id="data-augmentation">Data Augmentation</h3>
<p>A regularization technique that artificially expands training datasets by applying label-preserving transformations to existing samples. Common transformations include:</p>
<ul>
<li><strong>Image domain:</strong> Rotations, flips, crops, color adjustments, elastic distortions</li>
<li><strong>Text domain:</strong> Synonym substitution, back-translation, random deletion</li>
<li><strong>Audio domain:</strong> Time stretching, pitch shifting, adding noise</li>
</ul>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture13.html">Lecture 13</a> (Advanced CNN Architectures and Transfer Learning)</p>
</section>
<section id="decision-boundaries" class="level3">
<h3 class="anchored" data-anchor-id="decision-boundaries">Decision Boundaries</h3>
<p>The separating surfaces in feature space that demarcate different class regions in classification models. In linear models, these are hyperplanes; in nonlinear models, they can form complex shapes. Decision boundaries directly relate to model complexity - more flexible models can create more intricate decision boundaries.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture8.html">Lecture 8</a> (Introduction to Neural Networks), <a href="../lectures/lecture9.html">Lecture 9</a> (Deep Neural Networks)</p>
</section>
<section id="decision-trees" class="level3">
<h3 class="anchored" data-anchor-id="decision-trees">Decision Trees</h3>
<p>Non-parametric supervised learning models that recursively partition the feature space based on feature values to create a tree-like model of decisions. Key components include:</p>
<ul>
<li><strong>Nodes:</strong> Test a particular feature against a threshold</li>
<li><strong>Branches:</strong> Outcomes of the tests</li>
<li><strong>Leaf nodes:</strong> Final predictions or class assignments</li>
<li><strong>Splitting criteria:</strong> Metrics like entropy, information gain, or Gini impurity that determine optimal splits</li>
</ul>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture7.html">Lecture 7</a> (Nonlinearities and Expressive Learning Methods)</p>
</section>
<section id="deep-learning" class="level3">
<h3 class="anchored" data-anchor-id="deep-learning">Deep Learning</h3>
<p>A subset of machine learning using neural networks with multiple layers to progressively extract higher-level features from raw input. Characterized by:</p>
<ul>
<li>Automatic feature extraction without manual engineering</li>
<li>End-to-end learning from raw data to final output</li>
<li>Hierarchical representations increasing in abstraction with network depth</li>
<li>Capacity to model extremely complex, non-linear relationships</li>
</ul>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture8.html">Lecture 8</a> (Introduction to Neural Networks), <a href="../lectures/lecture9.html">Lecture 9</a> (Deep Neural Networks)</p>
</section>
<section id="diffusion-models" class="level3">
<h3 class="anchored" data-anchor-id="diffusion-models">Diffusion Models</h3>
<p>Generative models that learn to convert noise into structured data by reversing a gradual noising process. The approach involves:</p>
<ol type="1">
<li><strong>Forward process:</strong> Gradually adding Gaussian noise to training data according to a fixed schedule until it becomes pure noise</li>
<li><strong>Reverse process:</strong> Training a neural network to denoise images step by step</li>
<li><strong>Sampling:</strong> Generating new data by starting with random noise and iteratively denoising</li>
</ol>
<p>Key advantages include high-quality generation, stable training, and good sample diversity. Examples include DDPM and Stable Diffusion.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture26.html">Lecture 26</a> (Diffusion Models)</p>
</section>
<section id="dropout" class="level3">
<h3 class="anchored" data-anchor-id="dropout">Dropout</h3>
<p>A regularization technique where randomly selected neurons are temporarily removed during training iterations:</p>
<ol type="1">
<li>For each training batch, each neuron has probability <span class="math inline">\(p\)</span> of being retained</li>
<li>During testing, all neurons are used but outputs are scaled by <span class="math inline">\(p\)</span> to maintain expected activation levels</li>
</ol>
<p>This prevents co-adaptation of neurons, effectively training an ensemble of subnetworks, which reduces overfitting and improves generalization.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture11.html">Lecture 11</a> (Vanishing Gradients and Generalization)</p>
</section>
</section>
<section id="e" class="level2">
<h2 class="anchored" data-anchor-id="e">E</h2>
<section id="early-stopping" class="level3">
<h3 class="anchored" data-anchor-id="early-stopping">Early Stopping</h3>
<p>A regularization technique where training is halted when performance on a validation set stops improving. Implementation typically involves:</p>
<ol type="1">
<li>Monitoring validation loss/metric during training</li>
<li>Stopping when validation performance has not improved for a specified number of iterations (patience)</li>
<li>Reverting to the model weights that achieved best validation performance</li>
</ol>
<p>This prevents overfitting by not allowing the model to continue learning noise in the training data.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture11.html">Lecture 11</a> (Vanishing Gradients and Generalization)</p>
</section>
<section id="embedding" class="level3">
<h3 class="anchored" data-anchor-id="embedding">Embedding</h3>
<p>A learned mapping from discrete entities (words, categories, user IDs) to continuous vector spaces of lower dimensionality. Embeddings:</p>
<ul>
<li>Capture semantic relationships between entities in vector space</li>
<li>Enable neural networks to process categorical inputs</li>
<li>Place similar entities close together in the embedding space</li>
<li>Serve as dense, information-rich features for downstream tasks</li>
</ul>
<p>Common applications include word embeddings (Word2Vec, GloVe), entity embeddings, and graph embeddings.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture17.html">Lecture 17</a> (Sequence-to-Sequence Models and Attention Mechanisms), <a href="../lectures/lecture19.html">Lecture 19</a> (Representation Learning, Vision Transformers, and Autoregressive Generation)</p>
</section>
<section id="ensemble-methods" class="level3">
<h3 class="anchored" data-anchor-id="ensemble-methods">Ensemble Methods</h3>
<p>Techniques that combine multiple models to improve predictive performance and robustness. Major approaches include:</p>
<ul>
<li><strong>Bagging (Bootstrap Aggregating):</strong> Trains models on random subsets of data with replacement and averages predictions</li>
<li><strong>Boosting:</strong> Sequentially trains models with each focusing on errors from previous models</li>
<li><strong>Stacking:</strong> Uses a meta-model that learns how to best combine predictions from base models</li>
<li><strong>Random Forests:</strong> Combines bagging with random feature subset selection in decision trees</li>
</ul>
<p>Ensembles typically achieve better generalization by reducing variance or bias depending on the method used.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture11.html">Lecture 11</a> (Vanishing Gradients and Generalization), <a href="../lectures/lecture27.html">Lecture 27</a> (Interpretable Machine Learning)</p>
</section>
<section id="explainable-ai-xai" class="level3">
<h3 class="anchored" data-anchor-id="explainable-ai-xai">Explainable AI (XAI)</h3>
<p>Methods and techniques that make AI systems’ decisions understandable and interpretable to humans. Approaches include: - <strong>Post-hoc methods:</strong> Feature importance, SHAP values, LIME, attention visualization - <strong>Glass-box models:</strong> Inherently interpretable models like decision trees, linear models - <strong>Concept attribution:</strong> Mapping activations to human-understandable concepts - <strong>Counterfactual explanations:</strong> Identifying minimal changes to inputs that would change outputs</p>
<p>XAI is crucial for building trust, ensuring fairness, meeting regulatory requirements, and identifying potential biases in models.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture27.html">Lecture 27</a> (Interpretable Machine Learning)</p>
</section>
<section id="evidence-lower-bound-elbo" class="level3">
<h3 class="anchored" data-anchor-id="evidence-lower-bound-elbo">Evidence Lower Bound (ELBO)</h3>
<p>A key concept in variational inference that provides a tractable lower bound on the marginal log-likelihood of observed data:</p>
<p><span class="math display">\[\text{ELBO} = \mathbb{E}_Q[\log P(\mathbf{x}|\mathbf{z},\boldsymbol{\theta})] - D_{KL}(Q(\mathbf{z}|\mathbf{x},\boldsymbol{\phi}) || P(\mathbf{z}))\]</span></p>
<p>The ELBO consists of two components:</p>
<ul>
<li><strong>Reconstruction term</strong>: Expected log-likelihood of data given latent variables</li>
<li><strong>Regularization term</strong>: KL divergence between the approximate posterior and the prior</li>
</ul>
<p>Maximizing the ELBO is equivalent to minimizing the KL divergence between the approximate and true posterior distributions. This objective function is central to training variational autoencoders and other approximate Bayesian methods.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture23.html">Lecture 23</a> (Variational Autoencoders)</p>
</section>
</section>
<section id="f" class="level2">
<h2 class="anchored" data-anchor-id="f">F</h2>
<section id="feature-engineering" class="level3">
<h3 class="anchored" data-anchor-id="feature-engineering">Feature Engineering</h3>
<p>The process of using domain knowledge to extract or create features from raw data to improve machine learning model performance. Techniques include:</p>
<ul>
<li><strong>Transformation:</strong> Applying mathematical functions to create new representations</li>
<li><strong>Encoding:</strong> Converting categorical variables to numerical form</li>
<li><strong>Discretization:</strong> Binning continuous variables into categories</li>
<li><strong>Interaction features:</strong> Creating new features from combinations of existing ones</li>
<li><strong>Feature extraction:</strong> Deriving informative features from raw data</li>
</ul>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture8.html">Lecture 8</a> (Introduction to Neural Networks), <a href="../lectures/lecture9.html">Lecture 9</a> (Deep Neural Networks)</p>
</section>
<section id="feature-selection" class="level3">
<h3 class="anchored" data-anchor-id="feature-selection">Feature Selection</h3>
<p>Techniques for identifying and selecting the most relevant variables for model building. Common approaches include:</p>
<ul>
<li><strong>Filter methods:</strong> Evaluate features independently using statistical measures (correlation, chi-square)</li>
<li><strong>Wrapper methods:</strong> Test feature subsets using the model itself (recursive feature elimination)</li>
<li><strong>Embedded methods:</strong> Perform selection during model training (L1 regularization)</li>
</ul>
<p>Effective feature selection improves model performance, reduces training time, and enhances model interpretability.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture3.html">Lecture 3</a> (Generalization Error), <a href="../lectures/lecture4.html">Lecture 4</a> (Linear Models and Likelihood)</p>
</section>
<section id="forward-and-backward-passes" class="level3">
<h3 class="anchored" data-anchor-id="forward-and-backward-passes">Forward and Backward Passes</h3>
<p>The two fundamental computational phases in neural network training:</p>
<ul>
<li><strong>Forward Pass</strong>: The computation flow from input to output where:
<ul>
<li>Input data propagates through the network layer by layer</li>
<li>Each layer applies weights, biases, and activation functions</li>
<li>Intermediate values and final outputs are computed and stored</li>
<li>The loss function evaluates prediction quality</li>
</ul></li>
<li><strong>Backward Pass</strong>: The computation flow from output back to input where:
<ul>
<li>Gradients of the loss with respect to parameters are computed</li>
<li>The chain rule is applied to propagate error signals backward</li>
<li>Parameter updates are calculated for optimization algorithms</li>
<li>Weights and biases are adjusted to minimize the loss</li>
</ul></li>
</ul>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture10.html">Lecture 10</a> (Backpropagation and Gradient Problems)</p>
</section>
<section id="fine-tuning" class="level3">
<h3 class="anchored" data-anchor-id="fine-tuning">Fine-tuning</h3>
<p>The process of further training a pre-trained model on a specific target task with domain-specific data. The approach typically involves:</p>
<ol type="1">
<li>Starting with a model pre-trained on a large general dataset</li>
<li>Replacing or modifying the output layers for the target task</li>
<li>Training on the target dataset, typically with lower learning rates</li>
<li>Potentially freezing early layers to preserve general features</li>
</ol>
<p>Fine-tuning leverages knowledge transfer to achieve better performance with less task-specific data.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture19.html">Lecture 19</a> (Transfer Learning)</p>
</section>
<section id="f1-score" class="level3">
<h3 class="anchored" data-anchor-id="f1-score">F1 Score</h3>
<p>A performance metric that combines precision and recall into a single harmonized score:</p>
<p><span class="math display">\[F1 = 2 \cdot \frac{\text{precision} \cdot \text{recall}}{\text{precision} + \text{recall}}\]</span></p>
<p>F1 score ranges from 0 (worst) to 1 (best) and is particularly valuable for imbalanced classification problems where both false positives and false negatives are important considerations.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture6.html">Lecture 6</a> (Classification Performance Metrics)</p>
</section>
</section>
<section id="g" class="level2">
<h2 class="anchored" data-anchor-id="g">G</h2>
<section id="gated-recurrent-unit-gru" class="level3">
<h3 class="anchored" data-anchor-id="gated-recurrent-unit-gru">Gated Recurrent Unit (GRU)</h3>
<p>A type of recurrent neural network architecture that uses gating mechanisms to control information flow. Simpler than LSTMs with fewer parameters, GRUs use reset and update gates to handle the vanishing gradient problem while maintaining good performance.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture16.html">Lecture 16</a> (Recurrent Neural Networks and Sequence Modeling)</p>
</section>
<section id="generalization-error" class="level3">
<h3 class="anchored" data-anchor-id="generalization-error">Generalization Error</h3>
<p>The expected error when applying a model to new, unseen data. It measures how well a model performs beyond its training data and can be estimated using validation sets or cross-validation. Good generalization is the ultimate goal of machine learning.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture3.html">Lecture 3</a> (Generalization Error)</p>
</section>
<section id="generative-adversarial-networks-gans" class="level3">
<h3 class="anchored" data-anchor-id="generative-adversarial-networks-gans">Generative Adversarial Networks (GANs)</h3>
<p>Framework where two neural networks compete in a game-theoretic scenario:</p>
<ul>
<li><strong>Generator</strong>: Creates synthetic data samples trying to fool the discriminator</li>
<li><strong>Discriminator</strong>: Distinguishes between real and generated samples</li>
</ul>
<p>Through this adversarial process, the generator learns to produce increasingly realistic data, while the discriminator becomes better at detecting fakes. This minimax game continues until equilibrium, resulting in a generator that creates high-quality synthetic data.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture24.html">Lecture 24</a> (VAE Extensions and GANs Introduction), <a href="../lectures/lecture25.html">Lecture 25</a> (Generative Adversarial Networks)</p>
</section>
<section id="generative-models" class="level3">
<h3 class="anchored" data-anchor-id="generative-models">Generative Models</h3>
<p>Models that learn to generate new data samples from the same distribution as the training data. Types include:</p>
<ul>
<li><strong>Autoregressive models</strong>: Model sequential dependencies (e.g., PixelCNN, GPT)</li>
<li><strong>Variational Autoencoders</strong>: Learn probabilistic latent representations</li>
<li><strong>GANs</strong>: Use adversarial training to generate realistic samples</li>
<li><strong>Diffusion models</strong>: Generate data by reversing a noising process</li>
<li><strong>Flow-based models</strong>: Use invertible transformations to model probability distributions</li>
</ul>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture20.html">Lecture 20</a> (Generative Models: Autoregressives and Autoencoders), <a href="../lectures/lecture24.html">Lecture 24</a> (VAE Extensions and GANs Introduction), <a href="../lectures/lecture25.html">Lecture 25</a> (Generative Adversarial Networks), <a href="../lectures/lecture26.html">Lecture 26</a> (Diffusion Models)</p>
</section>
<section id="generalized-linear-models-glms" class="level3">
<h3 class="anchored" data-anchor-id="generalized-linear-models-glms">Generalized Linear Models (GLMs)</h3>
<p>Extension of linear regression that allows for response variables with non-normal distributions. GLMs consist of:</p>
<ul>
<li><strong>Random component</strong>: Specifies the distribution of the response variable (e.g., Gaussian, Binomial, Poisson)</li>
<li><strong>Systematic component</strong>: Linear combination of predictors</li>
<li><strong>Link function</strong>: Connects the expected value of the response to the linear predictor</li>
</ul>
<p>Common examples include logistic regression, Poisson regression, and log-linear models.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture4.html">Lecture 4</a> (Linear Models and Likelihood)</p>
</section>
<section id="gradient-descent" class="level3">
<h3 class="anchored" data-anchor-id="gradient-descent">Gradient Descent</h3>
<p>An optimization algorithm used to minimize loss functions by iteratively adjusting parameters in the direction of steepest descent of the gradient. Variants include:</p>
<ul>
<li><strong>Batch Gradient Descent</strong>: Uses entire dataset for each update</li>
<li><strong>Stochastic Gradient Descent (SGD)</strong>: Uses a single sample for each update</li>
<li><strong>Mini-batch Gradient Descent</strong>: Uses a small random subset of data for each update</li>
</ul>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture5.html">Lecture 5</a> (Loss Minimization and Optimization), <a href="../lectures/lecture10.html">Lecture 10</a> (Backpropagation and Gradient Problems)</p>
</section>
</section>
<section id="h" class="level2">
<h2 class="anchored" data-anchor-id="h">H</h2>
<section id="hyperparameter" class="level3">
<h3 class="anchored" data-anchor-id="hyperparameter">Hyperparameter</h3>
<p>Parameters set before training begins (unlike model parameters that are learned during training). Examples include learning rate, number of layers, number of neurons per layer, batch size, and regularization strength. Tuning hyperparameters typically requires techniques like grid search, random search, or Bayesian optimization.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture8.html">Lecture 8</a> (Introduction to Neural Networks), <a href="../lectures/lecture11.html">Lecture 11</a> (Vanishing Gradients and Generalization)</p>
</section>
</section>
<section id="i" class="level2">
<h2 class="anchored" data-anchor-id="i">I</h2>
<section id="information-gain" class="level3">
<h3 class="anchored" data-anchor-id="information-gain">Information Gain</h3>
<p>A metric used in decision trees to determine the quality of a split, based on the reduction in entropy after a dataset is split. Higher information gain indicates a more useful split.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture7.html">Lecture 7</a> (Nonlinearities and Expressive Learning Methods)</p>
</section>
<section id="instance-segmentation" class="level3">
<h3 class="anchored" data-anchor-id="instance-segmentation">Instance Segmentation</h3>
<p>Computer vision task that involves identifying each instance of each object in an image at the pixel level. It combines elements of object detection (finding and classifying objects) with semantic segmentation (determining the class of each pixel).</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture14.html">Lecture 14</a> (Object Detection in Computer Vision), <a href="../lectures/lecture15.html">Lecture 15</a> (Semantic Segmentation and Advanced CNN Applications)</p>
</section>
</section>
<section id="k" class="level2">
<h2 class="anchored" data-anchor-id="k">K</h2>
<section id="k-means-clustering" class="level3">
<h3 class="anchored" data-anchor-id="k-means-clustering">K-means Clustering</h3>
<p>An unsupervised learning algorithm that partitions data into K clusters by minimizing the sum of squared distances between data points and their assigned cluster centroids. The algorithm works by:</p>
<ol type="1">
<li>Randomly initializing K cluster centroids</li>
<li>Assigning each data point to the nearest centroid</li>
<li>Updating centroids based on assigned points</li>
<li>Repeating steps 2-3 until convergence</li>
</ol>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture2.html">Lecture 2</a> (Machine Learning Fundamentals)</p>
</section>
<section id="kernel-methods" class="level3">
<h3 class="anchored" data-anchor-id="kernel-methods">Kernel Methods</h3>
<p>Techniques that implicitly transform data into higher-dimensional spaces without explicitly computing the coordinates in that space. The kernel trick allows for efficient computation of inner products in these spaces, making non-linear patterns linearly separable. Support Vector Machines often use kernel methods.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture7.html">Lecture 7</a> (Nonlinearities and Expressive Learning Methods)</p>
</section>
<section id="kullback-leibler-kl-divergence" class="level3">
<h3 class="anchored" data-anchor-id="kullback-leibler-kl-divergence">Kullback-Leibler (KL) Divergence</h3>
<p>A statistical measure that quantifies the difference between two probability distributions P and Q:</p>
<p><span class="math display">\[D_{KL}(P||Q) = \sum_x P(x) \log \frac{P(x)}{Q(x)} \text{ or } \int P(x) \log \frac{P(x)}{Q(x)} dx\]</span></p>
<p>Key properties include:</p>
<ul>
<li>Always non-negative (≥ 0)</li>
<li>Equals zero only when the distributions are identical</li>
<li>Asymmetric: <span class="math inline">\(D_{KL}(P||Q) \neq D_{KL}(Q||P)\)</span></li>
</ul>
<p>In machine learning, KL divergence serves as:</p>
<ul>
<li>A regularization term in variational autoencoders</li>
<li>A component in the evidence lower bound (ELBO)</li>
<li>A loss function for matching probability distributions</li>
<li>A way to measure how much information is lost when approximating one distribution with another</li>
</ul>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture22.html">Lecture 22</a> (Bayesian Machine Learning), <a href="../lectures/lecture23.html">Lecture 23</a> (Variational Autoencoders)</p>
</section>
</section>
<section id="m" class="level2">
<h2 class="anchored" data-anchor-id="m">M</h2>
<section id="machine-learning-fundamentals" class="level3">
<h3 class="anchored" data-anchor-id="machine-learning-fundamentals">Machine Learning Fundamentals</h3>
<p>The core components that define machine learning as articulated by Tom Mitchell (1997):</p>
<ul>
<li><strong>Tasks (T)</strong>: The objectives the algorithm aims to achieve:
<ul>
<li><strong>Prediction</strong>: Forecasting future or unknown values</li>
<li><strong>Description</strong>: Uncovering patterns or structures in data</li>
<li><strong>Inference/Explanation</strong>: Understanding causal relationships and variable impacts</li>
</ul></li>
<li><strong>Experience (E)</strong>: The data that fuels learning:
<ul>
<li><strong>Supervised Learning</strong>: Uses labeled data</li>
<li><strong>Unsupervised Learning</strong>: Uses unlabeled data</li>
<li><strong>Semi-supervised Learning</strong>: Uses both labeled and unlabeled data</li>
<li><strong>Reinforcement Learning</strong>: Learning through environment interaction</li>
</ul></li>
<li><strong>Performance Measure (P)</strong>: Metrics quantifying algorithm performance, typically through loss functions</li>
</ul>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture2.html">Lecture 2</a> (Machine Learning Fundamentals)</p>
</section>
</section>
<section id="l" class="level2">
<h2 class="anchored" data-anchor-id="l">L</h2>
<section id="l1-regularization-lasso" class="level3">
<h3 class="anchored" data-anchor-id="l1-regularization-lasso">L1 Regularization (LASSO)</h3>
<p>Regularization technique that adds the sum of absolute values of coefficients to the loss function, promoting sparsity by shrinking less important feature coefficients to exactly zero. The regularization term is <span class="math inline">\(\lambda\sum|w_i|\)</span>, where <span class="math inline">\(\lambda\)</span> controls the strength of the penalty.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture4.html">Lecture 4</a> (Linear Models and Likelihood)</p>
</section>
<section id="l2-regularization-ridge" class="level3">
<h3 class="anchored" data-anchor-id="l2-regularization-ridge">L2 Regularization (Ridge)</h3>
<p>Regularization technique that adds the sum of squared coefficients to the loss function, preventing overfitting by shrinking all coefficients toward zero but rarely setting them exactly to zero. The regularization term is <span class="math inline">\(\lambda\sum w_i^2\)</span>, where <span class="math inline">\(\lambda\)</span> controls the strength of the penalty.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture4.html">Lecture 4</a> (Linear Models and Likelihood), <a href="../lectures/lecture11.html">Lecture 11</a> (Vanishing Gradients and Generalization)</p>
</section>
<section id="laplace-approximation" class="level3">
<h3 class="anchored" data-anchor-id="laplace-approximation">Laplace Approximation</h3>
<p>A method for approximating Bayesian posterior distributions with a multivariate Gaussian distribution centered at the maximum a posteriori (MAP) estimate:</p>
<p><span class="math display">\[P(\boldsymbol{\theta}|\mathcal{D}) \approx \mathcal{N}(\boldsymbol{\theta}|\hat{\boldsymbol{\mu}}, \hat{\boldsymbol{\Sigma}})\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(\hat{\boldsymbol{\mu}} = \arg\max_{\boldsymbol{\theta}} P(\boldsymbol{\theta}|\mathcal{D})\)</span> is the MAP estimate</li>
<li><span class="math inline">\(\hat{\boldsymbol{\Sigma}} = -\left[\frac{\partial^2 \log P(\hat{\boldsymbol{\mu}}|\mathcal{D})}{\partial \boldsymbol{\theta} \partial \boldsymbol{\theta}'}\right]^{-1}\)</span> is the inverse Hessian of the negative log posterior</li>
</ul>
<p>This approximation is particularly useful for models with intractable posterior distributions, providing both parameter estimates and uncertainty quantification. The Bernstein-von Mises theorem guarantees that under certain conditions, the posterior distribution converges to this normal approximation as sample size increases.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture22.html">Lecture 22</a> (Bayesian Machine Learning)</p>
</section>
<section id="latent-space" class="level3">
<h3 class="anchored" data-anchor-id="latent-space">Latent Space</h3>
<p>A compressed representation in lower dimensions that captures the essential features of the input data. In autoencoders and VAEs, this is the bottleneck layer. Points that are close in latent space should represent semantically similar inputs.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture20.html">Lecture 20</a> (Generative Models: Autoregressives and Autoencoders), <a href="../lectures/lecture24.html">Lecture 24</a> (VAE Extensions and GANs Introduction)</p>
</section>
<section id="learning-rate" class="level3">
<h3 class="anchored" data-anchor-id="learning-rate">Learning Rate</h3>
<p>A hyperparameter that controls how much to adjust model weights in response to the estimated error during training. Too high a learning rate can cause training to diverge; too low can cause it to converge too slowly or get stuck in suboptimal solutions.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture5.html">Lecture 5</a> (Loss Minimization and Optimization), <a href="../lectures/lecture10.html">Lecture 10</a> (Backpropagation and Gradient Problems)</p>
</section>
<section id="learning-rate-scheduling" class="level3">
<h3 class="anchored" data-anchor-id="learning-rate-scheduling">Learning Rate Scheduling</h3>
<p>Techniques to adjust the learning rate during training, typically reducing it over time. Common approaches include step decay, exponential decay, cosine annealing, and warm restarts.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture5.html">Lecture 5</a> (Loss Minimization and Optimization), <a href="../lectures/lecture6.html">Lecture 6</a> (Adaptive Methods for Minimization)</p>
</section>
<section id="likelihood" class="level3">
<h3 class="anchored" data-anchor-id="likelihood">Likelihood</h3>
<p>In statistical inference, the conditional probability of observing the data given specific parameter values, expressed as <span class="math inline">\(P(\mathcal{D}|\boldsymbol{\theta})\)</span>. The likelihood function:</p>
<ul>
<li>Measures how well different parameter values explain observed data</li>
<li>Forms the basis for maximum likelihood estimation (MLE) when maximizing <span class="math inline">\(P(\mathcal{D}|\boldsymbol{\theta})\)</span></li>
<li>Serves as a critical component in Bayes’ theorem: <span class="math inline">\(P(\boldsymbol{\theta}|\mathcal{D}) \propto P(\mathcal{D}|\boldsymbol{\theta})P(\boldsymbol{\theta})\)</span></li>
<li>Often expressed and optimized in logarithmic form (log-likelihood) for computational stability</li>
</ul>
<p>Common examples include Gaussian likelihood for regression and Bernoulli likelihood for binary classification.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture4.html">Lecture 4</a> (Linear Models and Likelihood), <a href="../lectures/lecture22.html">Lecture 22</a> (Bayesian Machine Learning)</p>
</section>
<section id="long-short-term-memory-lstm" class="level3">
<h3 class="anchored" data-anchor-id="long-short-term-memory-lstm">Long Short-Term Memory (LSTM)</h3>
<p>A specialized recurrent neural network architecture designed to address the vanishing gradient problem in standard RNNs. LSTMs use gating mechanisms (input, forget, and output gates) to retain information over long sequences.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture16.html">Lecture 16</a> (Recurrent Neural Networks and Sequence Modeling)</p>
</section>
<section id="logistic-regression" class="level3">
<h3 class="anchored" data-anchor-id="logistic-regression">Logistic Regression</h3>
<p>Statistical model used for binary classification that applies the logistic (sigmoid) function to a linear combination of features, transforming an unbounded output to a probability between 0 and 1. The formula is:</p>
<p><span class="math display">\[P(y = 1 | \mathbf x) = \frac{1}{1 + \exp[-\mathbf w^T \mathbf x - b]}\]</span></p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture4.html">Lecture 4</a> (Linear Models and Likelihood), <a href="../lectures/lecture5.html">Lecture 5</a> (Loss Minimization and Optimization)</p>
</section>
<section id="loss-function" class="level3">
<h3 class="anchored" data-anchor-id="loss-function">Loss Function</h3>
<p>A function that quantifies how well a model’s predictions match the true values. Common loss functions include:</p>
<ul>
<li><strong>Mean Squared Error (MSE)</strong>: For regression problems</li>
<li><strong>Cross-Entropy Loss</strong>: For classification problems</li>
<li><strong>Hinge Loss</strong>: Used in SVMs</li>
<li><strong>Kullback-Leibler Divergence</strong>: Measures difference between probability distributions</li>
</ul>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture2.html">Lecture 2</a> (Machine Learning Fundamentals), <a href="../lectures/lecture3.html">Lecture 3</a> (Generalization Error), <a href="../lectures/lecture5.html">Lecture 5</a> (Loss Minimization and Optimization)</p>
</section>
</section>
<section id="m-1" class="level2">
<h2 class="anchored" data-anchor-id="m-1">M</h2>
<section id="marginal-likelihood" class="level3">
<h3 class="anchored" data-anchor-id="marginal-likelihood">Marginal Likelihood</h3>
<p>Also known as model evidence, the marginal likelihood represents the probability of observing the data integrated over all possible parameter values:</p>
<p><span class="math display">\[P(\mathcal{D}) = \int P(\mathcal{D}|\boldsymbol{\theta})P(\boldsymbol{\theta})d\boldsymbol{\theta}\]</span></p>
<p>Key properties and applications include:</p>
<ul>
<li>Serves as the normalizing constant in Bayes’ theorem</li>
<li>Automatically implements Occam’s razor, balancing model fit and complexity</li>
<li>Used for model comparison and hyperparameter selection</li>
<li>Often computationally intractable, requiring approximation methods</li>
<li>For linear regression with normal prior: <span class="math inline">\(P(\mathbf{y}|\mathbf{X}) = \mathcal{N}(\mathbf{y}|\mathbf{X}\boldsymbol{\mu}_0, \sigma^2\mathbf{I} + \mathbf{X}\boldsymbol{\Sigma}_0\mathbf{X}^T)\)</span></li>
</ul>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture22.html">Lecture 22</a> (Bayesian Machine Learning), <a href="../lectures/lecture23.html">Lecture 23</a> (Variational Autoencoders)</p>
</section>
<section id="maximum-likelihood-estimation-mle" class="level3">
<h3 class="anchored" data-anchor-id="maximum-likelihood-estimation-mle">Maximum Likelihood Estimation (MLE)</h3>
<p>A method for estimating the parameters of a statistical model by finding values that maximize the likelihood function, which measures how probable the observed data is given the parameter values.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture4.html">Lecture 4</a> (Linear Models and Likelihood)</p>
</section>
<section id="model-calibration" class="level3">
<h3 class="anchored" data-anchor-id="model-calibration">Model Calibration</h3>
<p>The property that a model’s predicted probabilities accurately reflect true probabilities. A well-calibrated model’s confidence should match its accuracy. Techniques include Platt scaling and temperature scaling.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture4.html">Lecture 4</a> (Linear Models and Likelihood), <a href="../lectures/lecture27.html">Lecture 27</a> (Interpretable Machine Learning)</p>
</section>
<section id="multi-head-attention" class="level3">
<h3 class="anchored" data-anchor-id="multi-head-attention">Multi-Head Attention</h3>
<p>An extension of self-attention that runs multiple attention mechanisms in parallel, allowing the model to focus on different aspects of the input simultaneously. Each “head” projects the input into different subspaces before computing attention, capturing diverse relationships in the data. The outputs from all heads are concatenated and linearly transformed to produce the final result. This approach enhances model expressivity by enabling attention to different representation subspaces and positions.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture18.html">Lecture 18</a> (Transformers and the Attention Revolution)</p>
</section>
<section id="multi-layer-perceptron-mlp" class="level3">
<h3 class="anchored" data-anchor-id="multi-layer-perceptron-mlp">Multi-Layer Perceptron (MLP)</h3>
<p>A feedforward neural network with at least one hidden layer between input and output layers. Each neuron in one layer connects to every neuron in the next layer, using non-linear activation functions to learn complex patterns.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture8.html">Lecture 8</a> (Introduction to Neural Networks), <a href="../lectures/lecture9.html">Lecture 9</a> (Deep Neural Networks)</p>
</section>
</section>
<section id="n" class="level2">
<h2 class="anchored" data-anchor-id="n">N</h2>
<section id="neural-network-architecture" class="level3">
<h3 class="anchored" data-anchor-id="neural-network-architecture">Neural Network Architecture</h3>
<p>The specific arrangement of layers, neurons, and connections in a neural network. Includes the number of layers (depth), neurons per layer (width), types of layers, activation functions, and connectivity patterns.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture9.html">Lecture 9</a> (Deep Neural Networks), <a href="../lectures/lecture13.html">Lecture 13</a> (Advanced CNN Architectures and Transfer Learning)</p>
</section>
<section id="normalization" class="level3">
<h3 class="anchored" data-anchor-id="normalization">Normalization</h3>
<p>Techniques that adjust the scale of inputs or internal representations to improve neural network training. Examples include:</p>
<ul>
<li><strong>Batch Normalization</strong>: Normalizes layer inputs within a mini-batch</li>
<li><strong>Layer Normalization</strong>: Normalizes across features for each sample</li>
<li><strong>Instance Normalization</strong>: Normalizes across spatial dimensions for each sample</li>
<li><strong>Group Normalization</strong>: Normalizes across subsets of channels</li>
</ul>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture11.html">Lecture 11</a> (Vanishing Gradients and Generalization), <a href="../lectures/lecture13.html">Lecture 13</a> (Advanced CNN Architectures and Transfer Learning)</p>
</section>
</section>
<section id="o" class="level2">
<h2 class="anchored" data-anchor-id="o">O</h2>
<section id="object-detection" class="level3">
<h3 class="anchored" data-anchor-id="object-detection">Object Detection</h3>
<p>Computer vision task that involves identifying and localizing multiple objects in images. Combines classification (what objects are present) with localization (where they are). Common architectures include R-CNN, Fast R-CNN, Faster R-CNN, YOLO, and SSD.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture14.html">Lecture 14</a> (Object Detection in Computer Vision)</p>
</section>
<section id="one-hot-encoding" class="level3">
<h3 class="anchored" data-anchor-id="one-hot-encoding">One-Hot Encoding</h3>
<p>A representation of categorical variables as binary vectors where each position corresponds to a category, and only one position has a value of 1 (the category that applies), while all others are 0.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture2.html">Lecture 2</a> (Machine Learning Fundamentals), <a href="../lectures/lecture6.html">Lecture 6</a> (Adaptive Methods for Minimization)</p>
</section>
<section id="optimization-algorithms" class="level3">
<h3 class="anchored" data-anchor-id="optimization-algorithms">Optimization Algorithms</h3>
<p>Methods used to minimize the loss function and update model parameters. Beyond basic gradient descent, advanced algorithms include:</p>
<ul>
<li><strong>Momentum</strong>: Adds a fraction of the previous update to accelerate convergence</li>
<li><strong>RMSProp</strong>: Adapts learning rates using a moving average of squared gradients</li>
<li><strong>Adam</strong>: Combines momentum and RMSProp approaches</li>
<li><strong>AdamW</strong>: A variant of Adam with improved weight decay regularization</li>
</ul>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture5.html">Lecture 5</a> (Loss Minimization and Optimization), <a href="../lectures/lecture6.html">Lecture 6</a> (Adaptive Methods for Minimization)</p>
</section>
<section id="overfitting" class="level3">
<h3 class="anchored" data-anchor-id="overfitting">Overfitting</h3>
<p>When a model learns the training data too well, capturing noise instead of the underlying pattern, leading to poor generalization. Signs include:</p>
<ul>
<li>High accuracy on training data but low accuracy on validation/test data</li>
<li>Complex model with many parameters relative to the amount of training data</li>
<li>Model memorizes training examples rather than learning general patterns</li>
</ul>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture3.html">Lecture 3</a> (Generalization Error), <a href="../lectures/lecture4.html">Lecture 4</a> (Linear Models and Likelihood)</p>
</section>
</section>
<section id="p" class="level2">
<h2 class="anchored" data-anchor-id="p">P</h2>
<section id="parameter-sharing" class="level3">
<h3 class="anchored" data-anchor-id="parameter-sharing">Parameter Sharing</h3>
<p>A technique in neural networks where the same parameters are used across different parts of the model. Most prominently used in CNNs, where the same convolutional filters are applied across the entire image, drastically reducing the number of parameters and making the network translation invariant.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture12.html">Lecture 12</a> (Image Classification with Convolutional Neural Networks)</p>
</section>
<section id="pooling" class="level3">
<h3 class="anchored" data-anchor-id="pooling">Pooling</h3>
<p>Operations in convolutional neural networks that reduce spatial dimensions by summarizing values in local regions. Common pooling types include:</p>
<ul>
<li><strong>Max pooling</strong>: Takes the maximum value in each window</li>
<li><strong>Average pooling</strong>: Takes the average value in each window</li>
<li><strong>Global pooling</strong>: Reduces each feature map to a single value</li>
</ul>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture12.html">Lecture 12</a> (Image Classification with Convolutional Neural Networks)</p>
</section>
<section id="pre-training" class="level3">
<h3 class="anchored" data-anchor-id="pre-training">Pre-training</h3>
<p>Training a model on a large dataset for a general task before fine-tuning it on a specific task with less data. Enables transfer learning by learning general features that can be adapted to new tasks.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture13.html">Lecture 13</a> (Advanced CNN Architectures and Transfer Learning), <a href="../lectures/lecture19.html">Lecture 19</a> (Representation Learning, Vision Transformers, and Autoregressive Generation)</p>
</section>
<section id="principal-component-analysis-pca" class="level3">
<h3 class="anchored" data-anchor-id="principal-component-analysis-pca">Principal Component Analysis (PCA)</h3>
<p>A dimensionality reduction technique that transforms data into a new coordinate system where the greatest variance lies along the first coordinate (first principal component), the second greatest variance along the second coordinate, and so on.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture7.html">Lecture 7</a> (Nonlinearities and Expressive Learning Methods)</p>
</section>
<section id="prior" class="level3">
<h3 class="anchored" data-anchor-id="prior">Prior</h3>
<p>A probability distribution <span class="math inline">\(P(\boldsymbol{\theta})\)</span> that expresses beliefs about model parameters before observing data. Within Bayesian statistics, priors:</p>
<ul>
<li>Encode domain knowledge or subjective beliefs</li>
<li>Range from informative (strong beliefs) to diffuse/non-informative (minimal assumptions)</li>
<li>Are updated to posterior distributions through Bayes’ theorem</li>
<li>Can have significant impact with small datasets but diminish in influence as data increases</li>
</ul>
<p>Common examples include:</p>
<ul>
<li>Normal priors for regression coefficients</li>
<li>Laplace priors leading to L1 regularization (LASSO)</li>
<li>Normal priors leading to L2 regularization (Ridge)</li>
<li>Dirichlet priors for multinomial parameters</li>
<li>Conjugate priors that result in closed-form posteriors</li>
</ul>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture22.html">Lecture 22</a> (Bayesian Machine Learning)</p>
</section>
<section id="precision" class="level3">
<h3 class="anchored" data-anchor-id="precision">Precision</h3>
<p>A classification metric measuring the proportion of true positive predictions among all positive predictions: TP/(TP+FP). High precision means a low false positive rate, which is important when the cost of a false positive is high.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture3.html">Lecture 3</a> (Generalization Error)</p>
</section>
</section>
<section id="q" class="level2">
<h2 class="anchored" data-anchor-id="q">Q</h2>
<section id="q-learning" class="level3">
<h3 class="anchored" data-anchor-id="q-learning">Q-Learning</h3>
<p>A model-free reinforcement learning algorithm that learns the value of actions in states (Q-values) and selects actions based on these values. The goal is to find an optimal policy that maximizes the expected cumulative reward.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture27.html">Lecture 27</a> (Interpretable Machine Learning)</p>
</section>
</section>
<section id="r" class="level2">
<h2 class="anchored" data-anchor-id="r">R</h2>
<section id="random-forest" class="level3">
<h3 class="anchored" data-anchor-id="random-forest">Random Forest</h3>
<p>An ensemble learning method that constructs multiple decision trees during training and outputs the mode of the classes (for classification) or mean prediction (for regression) of the individual trees. Reduces overfitting by combining many trees trained on different subsets of data and features.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture7.html">Lecture 7</a> (Nonlinearities and Expressive Learning Methods)</p>
</section>
<section id="recall" class="level3">
<h3 class="anchored" data-anchor-id="recall">Recall</h3>
<p>A classification metric measuring the proportion of actual positive cases that were correctly identified: TP/(TP+FN). High recall means a low false negative rate, which is important when the cost of missing a positive case is high.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture3.html">Lecture 3</a> (Generalization Error)</p>
</section>
<section id="recurrent-neural-networks-rnns" class="level3">
<h3 class="anchored" data-anchor-id="recurrent-neural-networks-rnns">Recurrent Neural Networks (RNNs)</h3>
<p>Neural network architectures designed to process sequential data by maintaining internal state through recurrent connections. The defining feature is the recurrence relation: h_t = f_w(h_{t-1}, x_t), where h_t is the current hidden state, h_{t-1} is the previous hidden state, and x_t is the current input.</p>
<p>RNNs can handle various sequence relationships:</p>
<ul>
<li><strong>One-to-Many</strong>: Single input generates a sequence (e.g., image captioning)</li>
<li><strong>Many-to-One</strong>: Sequence produces single output (e.g., sentiment analysis)</li>
<li><strong>Many-to-Many (Unaligned)</strong>: Input sequence yields different-length output sequence (e.g., translation)</li>
<li><strong>Many-to-Many (Aligned)</strong>: Each input element corresponds to an output element (e.g., part-of-speech tagging)</li>
</ul>
<p>Challenges include vanishing and exploding gradients when processing long sequences, which specialized architectures like LSTMs and GRUs address.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture16.html">Lecture 16</a> (Recurrent Neural Networks and Sequence Modeling)</p>
</section>
<section id="regularization" class="level3">
<h3 class="anchored" data-anchor-id="regularization">Regularization</h3>
<p>Techniques to prevent overfitting by adding constraints or penalties to model complexity. Common methods include:</p>
<ul>
<li><strong>L1 regularization</strong>: Encourages sparse models by penalizing absolute weight values</li>
<li><strong>L2 regularization</strong>: Penalizes large weights using squared magnitude</li>
<li><strong>Dropout</strong>: Randomly ignores neurons during training</li>
<li><strong>Early stopping</strong>: Halts training when validation performance stops improving</li>
<li><strong>Data augmentation</strong>: Artificially expands training data with transformations</li>
<li><strong>Weight decay</strong>: Gradually reduces weights during training</li>
</ul>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture4.html">Lecture 4</a> (Linear Models and Likelihood), <a href="../lectures/lecture11.html">Lecture 11</a> (Vanishing Gradients and Generalization)</p>
</section>
<section id="resnet-residual-network" class="level3">
<h3 class="anchored" data-anchor-id="resnet-residual-network">ResNet (Residual Network)</h3>
<p>A deep neural network architecture that addresses the vanishing gradient problem by introducing skip connections (residual connections) that allow gradients to flow directly through the network. This innovation enables training of very deep networks with hundreds of layers.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture13.html">Lecture 13</a> (Advanced CNN Architectures and Transfer Learning)</p>
</section>
</section>
<section id="s" class="level2">
<h2 class="anchored" data-anchor-id="s">S</h2>
<section id="sequence-modeling" class="level3">
<h3 class="anchored" data-anchor-id="sequence-modeling">Sequence Modeling</h3>
<p>A machine learning paradigm focusing on data where order and temporal dependencies matter. Sequence models can handle various input-output relationships:</p>
<ul>
<li><strong>One-to-Many</strong>: Single input generates a sequence (e.g., image captioning)</li>
<li><strong>Many-to-One</strong>: Sequence produces single output (e.g., sentiment analysis)</li>
<li><strong>Many-to-Many (Unaligned)</strong>: Input sequence yields different-length output sequence (e.g., machine translation)</li>
<li><strong>Many-to-Many (Aligned)</strong>: Each input element corresponds to an output element (e.g., part-of-speech tagging)</li>
</ul>
<p>Architectures for sequence modeling include RNNs, LSTMs, GRUs, Transformers, and specialized convolutional architectures.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture16.html">Lecture 16</a> (Recurrent Neural Networks and Sequence Modeling)</p>
</section>
<section id="semantic-segmentation" class="level3">
<h3 class="anchored" data-anchor-id="semantic-segmentation">Semantic Segmentation</h3>
<p>Computer vision task that classifies each pixel in an image into a predefined category, assigning a semantic label to every pixel. Unlike instance segmentation, it doesn’t distinguish between different instances of the same class.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture15.html">Lecture 15</a> (Semantic Segmentation and Advanced CNN Applications)</p>
</section>
<section id="self-attention" class="level3">
<h3 class="anchored" data-anchor-id="self-attention">Self-Attention</h3>
<p>A neural network mechanism that allows models to directly connect any position in a sequence to any other position, enabling the model to focus on different parts of the input when producing outputs. The self-attention operation works by:</p>
<ol type="1">
<li><p>Computing three vectors for each token through learned linear projections:</p>
<ul>
<li><strong>Query vector (q_i)</strong>: Represents what information the token is “looking for”</li>
<li><strong>Key vector (k_i)</strong>: Represents what information the token “contains”</li>
<li><strong>Value vector (v_i)</strong>: Represents the actual content of the token</li>
</ul></li>
<li><p>Calculating attention weights by comparing queries to keys: e_{ij} = (q_i^T k_j)/√d_k</p></li>
<li><p>Normalizing weights using softmax function</p></li>
<li><p>Computing a weighted sum of all value vectors</p></li>
</ol>
<p>This mechanism forms the core of transformer architectures and enables constant computational complexity regardless of sequence distance.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture18.html">Lecture 18</a> (Transformers and the Attention Revolution)</p>
</section>
<section id="self-supervised-learning" class="level3">
<h3 class="anchored" data-anchor-id="self-supervised-learning">Self-Supervised Learning</h3>
<p>Training paradigm where models generate supervisory signals from the data itself without explicit labels. The model creates pretext tasks from unlabeled data (like predicting masked tokens, image rotations, or contrastive objectives) to learn useful representations, which can then be fine-tuned for downstream tasks with limited labeled data.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture19.html">Lecture 19</a> (Representation Learning, Vision Transformers, and Autoregressive Generation)</p>
</section>
<section id="softmax-function" class="level3">
<h3 class="anchored" data-anchor-id="softmax-function">Softmax Function</h3>
<p>A function that converts a vector of real numbers into a probability distribution. It exponentiates each input and normalizes by the sum of all exponentiated values, ensuring all outputs are between 0 and 1 and sum to 1. Commonly used for multi-class classification.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture4.html">Lecture 4</a> (Linear Models and Likelihood), <a href="../lectures/lecture8.html">Lecture 8</a> (Introduction to Neural Networks)</p>
</section>
<section id="stochastic-gradient-descent-sgd" class="level3">
<h3 class="anchored" data-anchor-id="stochastic-gradient-descent-sgd">Stochastic Gradient Descent (SGD)</h3>
<p>An optimization algorithm that updates model parameters using the gradient of the loss function with respect to those parameters, computed on randomly selected subsets (mini-batches) of the training data rather than the entire dataset.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture5.html">Lecture 5</a> (Loss Minimization and Optimization), <a href="../lectures/lecture10.html">Lecture 10</a> (Backpropagation and Gradient Problems)</p>
</section>
<section id="support-vector-machine-svm" class="level3">
<h3 class="anchored" data-anchor-id="support-vector-machine-svm">Support Vector Machine (SVM)</h3>
<p>A supervised learning algorithm that finds a hyperplane in an N-dimensional space that distinctly separates data points of different classes. SVMs maximize the margin between classes and can perform non-linear classification using the kernel trick.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture7.html">Lecture 7</a> (Nonlinearities and Expressive Learning Methods)</p>
</section>
</section>
<section id="t" class="level2">
<h2 class="anchored" data-anchor-id="t">T</h2>
<section id="temperature-scaling" class="level3">
<h3 class="anchored" data-anchor-id="temperature-scaling">Temperature Scaling</h3>
<p>A post-processing calibration technique for deep neural networks that divides the logits (pre-softmax outputs) by a scalar parameter called temperature. Higher temperatures produce softer probability distributions, while lower temperatures make them sharper.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture4.html">Lecture 4</a> (Linear Models and Likelihood), <a href="../lectures/lecture27.html">Lecture 27</a> (Interpretable Machine Learning)</p>
</section>
<section id="transfer-learning" class="level3">
<h3 class="anchored" data-anchor-id="transfer-learning">Transfer Learning</h3>
<p>A machine learning technique where knowledge gained from training a model on one task is applied to a different but related task. This approach is particularly effective when the target task has limited training data. Common strategies include:</p>
<ul>
<li><strong>Feature extraction</strong>: Reuse learned features from pre-trained models</li>
<li><strong>Fine-tuning</strong>: Adapt a pre-trained model by updating some or all of its parameters for a new task</li>
</ul>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture13.html">Lecture 13</a> (Advanced CNN Architectures and Transfer Learning), <a href="../lectures/lecture19.html">Lecture 19</a> (Representation Learning, Vision Transformers, and Autoregressive Generation)</p>
</section>
<section id="transformers" class="level3">
<h3 class="anchored" data-anchor-id="transformers">Transformers</h3>
<p>Neural network architecture introduced in “Attention is All You Need” (Vaswani et al., 2017) that overcomes limitations of recurrent networks through self-attention mechanisms. Transformers offer several key advantages:</p>
<ul>
<li>Parallel processing of sequences rather than sequential computation</li>
<li>Constant path length between any positions in a sequence</li>
<li>Direct modeling of long-range dependencies</li>
<li>Highly parallelizable computation</li>
</ul>
<p>The architecture consists of:</p>
<ul>
<li><strong>Multi-head self-attention</strong>: Allows the model to focus on different aspects of the input simultaneously</li>
<li><strong>Position encodings</strong>: Incorporate sequence order information since attention has no inherent position awareness</li>
<li><strong>Feed-forward networks</strong>: Process attention outputs with two linear transformations and a non-linearity</li>
<li><strong>Layer normalization</strong>: Stabilizes training by normalizing activations</li>
<li><strong>Residual connections</strong>: Help with gradient flow in deep networks</li>
</ul>
<p>Transformers have revolutionized NLP with models like BERT (bidirectional encoding), GPT (autoregressive generation), and T5 (sequence-to-sequence), and have been adapted for vision (ViT), audio, and multimodal tasks.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture18.html">Lecture 18</a> (Transformers and the Attention Revolution), <a href="../lectures/lecture19.html">Lecture 19</a> (Representation Learning, Vision Transformers, and Autoregressive Generation)</p>
</section>
</section>
<section id="u" class="level2">
<h2 class="anchored" data-anchor-id="u">U</h2>
<section id="underfitting" class="level3">
<h3 class="anchored" data-anchor-id="underfitting">Underfitting</h3>
<p>When a model is too simple to capture the underlying pattern in the data, resulting in poor performance on both training and test data. Signs include high bias and high error on training data.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture3.html">Lecture 3</a> (Generalization Error), <a href="../lectures/lecture4.html">Lecture 4</a> (Linear Models and Likelihood)</p>
</section>
<section id="universal-approximation-theorem" class="level3">
<h3 class="anchored" data-anchor-id="universal-approximation-theorem">Universal Approximation Theorem</h3>
<p>A mathematical result stating that a feedforward neural network with a single hidden layer containing a finite number of neurons can approximate any Borel measurable function from one finite-dimensional space to another with arbitrary precision, under mild assumptions about the activation function. Key aspects include:</p>
<ul>
<li>Provides the theoretical foundation for neural networks’ ability to model complex, nonlinear relationships</li>
<li>Explains why neural networks are considered “universal approximators”</li>
<li>Despite theoretical capability to represent any function, practical limitations arise from optimization challenges and overfitting</li>
<li>Indicates neural networks’ capacity to achieve zero training error given sufficient resources</li>
</ul>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture7.html">Lecture 7</a> (Nonlinearities and Expressive Learning Methods)</p>
</section>
<section id="unsupervised-learning" class="level3">
<h3 class="anchored" data-anchor-id="unsupervised-learning">Unsupervised Learning</h3>
<p>A type of machine learning where the algorithm is given data without explicit instructions on what to do with it. The system tries to learn the underlying structure or distribution of the data on its own. Common applications include clustering, dimensionality reduction, and density estimation.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture2.html">Lecture 2</a> (Machine Learning Fundamentals)</p>
</section>
</section>
<section id="v" class="level2">
<h2 class="anchored" data-anchor-id="v">V</h2>
<section id="validation-set" class="level3">
<h3 class="anchored" data-anchor-id="validation-set">Validation Set</h3>
<p>A portion of the data set aside from training data to tune hyperparameters and evaluate model performance during development. It helps detect overfitting and guides model selection without contaminating the test set.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture3.html">Lecture 3</a> (Generalization Error)</p>
</section>
<section id="variational-autoencoders-vaes" class="level3">
<h3 class="anchored" data-anchor-id="variational-autoencoders-vaes">Variational Autoencoders (VAEs)</h3>
<p>Probabilistic generative models that learn a latent space representation of data using an encoder-decoder architecture with a variational inference approach. VAEs:</p>
<ul>
<li>Encode inputs to parameters of probability distributions in latent space (typically mean and variance of Gaussian distributions)</li>
<li>Regularize the latent space by forcing it towards a standard normal distribution using KL divergence</li>
<li>Sample from the latent distribution using the reparameterization trick</li>
<li>Decode the sampled point to reconstruct the input</li>
<li>Enable both reconstruction and generation of new data points</li>
</ul>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture23.html">Lecture 23</a> (Variational Autoencoders)</p>
</section>
<section id="variational-bayesian-inference" class="level3">
<h3 class="anchored" data-anchor-id="variational-bayesian-inference">Variational Bayesian Inference</h3>
<p>An approximate Bayesian inference method that transforms posterior computation into an optimization problem by:</p>
<ol type="1">
<li>Introducing a simpler parametric family of distributions <span class="math inline">\(Q(\boldsymbol{\theta}|\boldsymbol{\phi})\)</span> to approximate the true posterior <span class="math inline">\(P(\boldsymbol{\theta}|\mathcal{D})\)</span></li>
<li>Finding the parameters <span class="math inline">\(\boldsymbol{\phi}\)</span> that minimize the KL divergence between the approximation and the true posterior</li>
<li>Optimizing the Evidence Lower Bound (ELBO): <span class="math inline">\(\mathcal{L}(\boldsymbol{\phi}) = \mathbb{E}_Q[\log P(\mathcal{D}|\boldsymbol{\theta})] - D_{KL}(Q(\boldsymbol{\theta}|\boldsymbol{\phi})||P(\boldsymbol{\theta}))\)</span></li>
</ol>
<p>Key advantages include:</p>
<ul>
<li>Scalability to large datasets through stochastic optimization</li>
<li>Applicability to models with non-conjugate priors</li>
<li>Ability to handle complex posterior distributions</li>
<li>Amortized inference in the case of variational autoencoders</li>
</ul>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture22.html">Lecture 22</a> (Bayesian Machine Learning), <a href="../lectures/lecture23.html">Lecture 23</a> (Variational Autoencoders)</p>
</section>
<section id="vanishingexploding-gradients" class="level3">
<h3 class="anchored" data-anchor-id="vanishingexploding-gradients">Vanishing/Exploding Gradients</h3>
<p>Challenges in training deep neural networks where gradients become extremely small (vanishing) or large (exploding) during backpropagation through many layers. These problems:</p>
<ul>
<li>Make it difficult to learn long-range dependencies</li>
<li>Cause unstable or stalled training</li>
<li>Can be addressed through techniques like careful weight initialization, gradient clipping, residual connections, batch normalization, and specialized architectures like LSTMs and GRUs</li>
</ul>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture10.html">Lecture 10</a> (Backpropagation and Gradient Problems), <a href="../lectures/lecture11.html">Lecture 11</a> (Vanishing Gradients and Generalization)</p>
</section>
<section id="vision-transformer-vit" class="level3">
<h3 class="anchored" data-anchor-id="vision-transformer-vit">Vision Transformer (ViT)</h3>
<p>An adaptation of the transformer architecture for computer vision tasks that treats an image as a sequence of patches, using self-attention to process relationships between all patches simultaneously. ViTs have achieved state-of-the-art performance on image classification and other vision tasks.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture19.html">Lecture 19</a> (Representation Learning, Vision Transformers, and Autoregressive Generation)</p>
</section>
</section>
<section id="w" class="level2">
<h2 class="anchored" data-anchor-id="w">W</h2>
<section id="wasserstein-gan-wgan" class="level3">
<h3 class="anchored" data-anchor-id="wasserstein-gan-wgan">Wasserstein GAN (WGAN)</h3>
<p>A variant of Generative Adversarial Networks that uses the Wasserstein distance (Earth Mover’s distance) instead of Jensen-Shannon divergence to measure the difference between the generated and real data distributions. WGANs offer more stable training and better quality gradients.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture25.html">Lecture 25</a> (Generative Adversarial Networks)</p>
</section>
<section id="weight-initialization" class="level3">
<h3 class="anchored" data-anchor-id="weight-initialization">Weight Initialization</h3>
<p>Strategies for setting the initial values of neural network weights before training begins. Proper initialization helps prevent vanishing/exploding gradients and accelerates convergence. Common methods include Xavier/Glorot initialization and He initialization.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture10.html">Lecture 10</a> (Backpropagation and Gradient Problems), <a href="../lectures/lecture13.html">Lecture 13</a> (Advanced CNN Architectures and Transfer Learning)</p>
</section>
</section>
<section id="z" class="level2">
<h2 class="anchored" data-anchor-id="z">Z</h2>
<section id="zero-shot-learning" class="level3">
<h3 class="anchored" data-anchor-id="zero-shot-learning">Zero-shot Learning</h3>
<p>A machine learning paradigm where a model can recognize objects or perform tasks it hasn’t encountered during training. The model learns to generalize from seen classes to unseen classes by leveraging additional information like semantic attributes or embeddings.</p>
<p><strong>Related lectures:</strong> <a href="../lectures/lecture19.html">Lecture 19</a> (Representation Learning, Vision Transformers, and Autoregressive Generation)</p>


<!-- -->

</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb1" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Machine Learning Glossary"</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co">    css: ../styles.css</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co">    toc: true</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">    toc-depth: 1</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="fu">## Introduction</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>This glossary provides definitions for key machine learning concepts covered in QTM 447. Each entry includes a concise, academically-focused explanation derived directly from course lectures, along with links to the specific lectures where the concept is discussed in detail. Use this resource as a reference guide throughout the course.</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="fu">## A</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="fu">### Accuracy</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>A performance metric quantifying the proportion of correct predictions (both true positives and true negatives) among all cases examined. While useful for balanced datasets, accuracy can be misleading when class distributions are skewed.</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 3</span><span class="co">](lecture3.qmd)</span> (Generalization Error), <span class="co">[</span><span class="ot">Lecture 4</span><span class="co">](lecture4.qmd)</span> (Linear Models and Likelihood)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="fu">### Activation Function</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>Mathematical functions applied to neural network layer outputs that introduce non-linearity, enabling networks to learn complex patterns. Key activation functions include:</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**ReLU (Rectified Linear Unit):** $f(x) = \max(0, x)$, widely used due to efficient gradient computation and helping mitigate vanishing gradients</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Sigmoid:** $f(x) = \frac{1}{1+e^{-x}}$, maps values to <span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>, useful for binary classification outputs</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Tanh:** $f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$, maps values to <span class="co">[</span><span class="ot">-1,1</span><span class="co">]</span>, often performs better than sigmoid in hidden layers</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 8</span><span class="co">](lecture8.qmd)</span> (Introduction to Neural Networks), <span class="co">[</span><span class="ot">Lecture 9</span><span class="co">](lecture9.qmd)</span> (Deep Neural Networks)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="fu">### Adam Optimizer</span></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>An adaptive learning rate optimization algorithm that combines ideas from momentum and RMSProp, maintaining per-parameter learning rates adapted based on first and second moments of gradients. Features include:</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Bias correction for moments</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Adaptive step sizes for each parameter</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Computational efficiency</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Good performance across a wide range of problems without extensive hyperparameter tuning</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 6</span><span class="co">](lecture6.qmd)</span> (Adaptive Methods for Minimization)</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a><span class="fu">### Area Under the Curve (AUC)</span></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>A performance metric for binary classification that measures the area under the ROC curve. AUC represents the probability that a randomly chosen positive instance ranks higher than a randomly chosen negative instance. An AUC of 1.0 indicates perfect ranking, while 0.5 represents random chance.</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 2</span><span class="co">](lecture2.qmd)</span> (Machine Learning Fundamentals)</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a><span class="fu">### Attention Mechanism</span></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>A neural network component that allows models to focus on different parts of the input when producing outputs. Attention computes weighted sums of input elements based on relevance scores, enabling models to selectively emphasize important information. This mechanism is fundamental to modern sequence processing architectures like Transformers.</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 18</span><span class="co">](lecture18.qmd)</span> (Transformers and the Attention Revolution)</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a><span class="fu">### Autoencoder</span></span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>Neural networks designed to learn efficient data representations (encodings) in an unsupervised manner by attempting to reconstruct their own inputs. The architecture consists of:</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Encoder:** Compresses input data into a lower-dimensional latent representation</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Bottleneck/latent space:** The compressed representation capturing essential features</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Decoder:** Reconstructs the original input from the latent representation</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>Applications include dimensionality reduction, denoising, anomaly detection, and as components in generative models.</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 20</span><span class="co">](lecture20.qmd)</span> (Generative Models: Autoregressives and Autoencoders)</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a><span class="fu">### Autoregressive Models</span></span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>Models that predict sequential outputs where each element depends on previously generated elements, modeling the probability distribution as:</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>$$p(x_1, x_2, ..., x_n) = \prod_{i=1}^n p(x_i | x_1, x_2, ..., x_{i-1})$$</span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>Used extensively in sequential data modeling such as language models, time series forecasting, and some generative models. Examples include traditional AR models, RNN-based language models, and GPT architectures.</span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 16</span><span class="co">](lecture16.qmd)</span> (Recurrent Neural Networks and Sequence Modeling), <span class="co">[</span><span class="ot">Lecture 20</span><span class="co">](lecture20.qmd)</span> (Generative Models: Autoregressives and Autoencoders)</span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a><span class="fu">## B</span></span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a><span class="fu">### Backpropagation</span></span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a>The primary algorithm for training neural networks by computing gradients of the loss function with respect to model parameters. Consists of two main phases:</span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Forward pass:** Computing all intermediate values through the network to calculate the output and loss</span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Backward pass:** Efficiently calculating gradients using the chain rule by propagating derivatives backward from the loss</span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a>The algorithm leverages computational graphs to track dependencies and facilitates efficient parameter updates through gradient-based optimization.</span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 10</span><span class="co">](lecture10.qmd)</span> (Backpropagation and Gradient Problems)</span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a><span class="fu">### Batch Normalization</span></span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a>A technique that normalizes layer inputs across mini-batches during training, maintaining internal representations with zero mean and unit variance:</span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a>$$\hat{x}^{(k)} = \frac{x^{(k)} - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}$$</span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a>After normalization, learnable parameters $\gamma$ and $\beta$ allow the network to undo the normalization if optimal:</span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a>$$y^{(k)} = \gamma^{(k)} \hat{x}^{(k)} + \beta^{(k)}$$</span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a>Advantages include:</span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Accelerated training by reducing internal covariate shift</span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Improved gradient flow</span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Regularizing effect that reduces overfitting</span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Enables higher learning rates</span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 11</span><span class="co">](lecture11.qmd)</span> (Vanishing Gradients and Generalization), <span class="co">[</span><span class="ot">Lecture 13</span><span class="co">](lecture13.qmd)</span> (Advanced CNN Architectures and Transfer Learning)</span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a><span class="fu">### Bayesian Machine Learning</span></span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a>A probabilistic approach to machine learning that applies Bayesian statistics to model uncertainty by:</span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Incorporating prior beliefs about parameters using probability distributions</span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Updating these beliefs with observed data using Bayes' theorem</span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Producing posterior distributions over parameters rather than point estimates</span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Enabling principled uncertainty quantification in predictions</span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a>Key methods include Bayesian linear regression, Bayesian neural networks, and Gaussian processes.</span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 22</span><span class="co">](lecture22.qmd)</span> (Bayesian Machine Learning)</span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a><span class="fu">### Bias-Variance Tradeoff</span></span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true" tabindex="-1"></a>A fundamental concept in statistical learning theory that decomposes prediction error into three components:</span>
<span id="cb1-114"><a href="#cb1-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-115"><a href="#cb1-115" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Bias:** Systematic error from incorrect model assumptions or insufficient complexity</span>
<span id="cb1-116"><a href="#cb1-116" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Variance:** Error from sensitivity to training data fluctuations</span>
<span id="cb1-117"><a href="#cb1-117" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Irreducible error:** Inherent noise that cannot be eliminated</span>
<span id="cb1-118"><a href="#cb1-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-119"><a href="#cb1-119" aria-hidden="true" tabindex="-1"></a>As model complexity increases, bias typically decreases while variance increases. Finding the optimal complexity minimizes the combined error (bias² + variance) to achieve better generalization.</span>
<span id="cb1-120"><a href="#cb1-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-121"><a href="#cb1-121" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 3</span><span class="co">](lecture3.qmd)</span> (Generalization Error)</span>
<span id="cb1-122"><a href="#cb1-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-123"><a href="#cb1-123" aria-hidden="true" tabindex="-1"></a><span class="fu">### Boosting</span></span>
<span id="cb1-124"><a href="#cb1-124" aria-hidden="true" tabindex="-1"></a>An ensemble learning approach where weak learners (typically shallow decision trees) are trained sequentially, with each new model focusing on the mistakes of previous models. Examples include:</span>
<span id="cb1-125"><a href="#cb1-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-126"><a href="#cb1-126" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**AdaBoost:** Adjusts instance weights to emphasize previously misclassified examples</span>
<span id="cb1-127"><a href="#cb1-127" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Gradient Boosting:** Iteratively fits models to the residual errors of previous models</span>
<span id="cb1-128"><a href="#cb1-128" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**XGBoost/LightGBM:** Optimized implementations with regularization and efficient training</span>
<span id="cb1-129"><a href="#cb1-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-130"><a href="#cb1-130" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 27</span><span class="co">](lecture27.qmd)</span> (Interpretable Machine Learning)</span>
<span id="cb1-131"><a href="#cb1-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-132"><a href="#cb1-132" aria-hidden="true" tabindex="-1"></a><span class="fu">## C</span></span>
<span id="cb1-133"><a href="#cb1-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-134"><a href="#cb1-134" aria-hidden="true" tabindex="-1"></a><span class="fu">### Categorical Cross-Entropy Loss</span></span>
<span id="cb1-135"><a href="#cb1-135" aria-hidden="true" tabindex="-1"></a>A loss function for multi-class classification problems measuring the difference between predicted probability distribution $\hat{p}$ and true distribution $p$ (usually one-hot encoded):</span>
<span id="cb1-136"><a href="#cb1-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-137"><a href="#cb1-137" aria-hidden="true" tabindex="-1"></a>$$L = -\sum_{i=1}^C p_i \log(\hat{p}_i)$$</span>
<span id="cb1-138"><a href="#cb1-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-139"><a href="#cb1-139" aria-hidden="true" tabindex="-1"></a>Where $C$ is the number of classes, $p_i$ is the true probability of class $i$ (typically 0 or 1), and $\hat{p}_i$ is the predicted probability. Minimizing this loss maximizes the log-likelihood of the correct classifications.</span>
<span id="cb1-140"><a href="#cb1-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-141"><a href="#cb1-141" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 2</span><span class="co">](lecture2.qmd)</span> (Machine Learning Fundamentals), <span class="co">[</span><span class="ot">Lecture 5</span><span class="co">](lecture5.qmd)</span> (Optimization Foundations and Stochastic Gradient Descent), <span class="co">[</span><span class="ot">Lecture 8</span><span class="co">](lecture8.qmd)</span> (Introduction to Neural Networks)</span>
<span id="cb1-142"><a href="#cb1-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-143"><a href="#cb1-143" aria-hidden="true" tabindex="-1"></a><span class="fu">### Computational Graph</span></span>
<span id="cb1-144"><a href="#cb1-144" aria-hidden="true" tabindex="-1"></a>A directed acyclic graph representation of mathematical operations where nodes represent operations (addition, multiplication, activation functions) and edges represent data flow. This structure enables:</span>
<span id="cb1-145"><a href="#cb1-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-146"><a href="#cb1-146" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Systematic computation of gradients using the chain rule</span>
<span id="cb1-147"><a href="#cb1-147" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Efficient implementation of backpropagation by storing intermediate values</span>
<span id="cb1-148"><a href="#cb1-148" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Automatic differentiation in deep learning frameworks</span>
<span id="cb1-149"><a href="#cb1-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-150"><a href="#cb1-150" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 10</span><span class="co">](lecture10.qmd)</span> (Backpropagation and Gradient Problems)</span>
<span id="cb1-151"><a href="#cb1-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-152"><a href="#cb1-152" aria-hidden="true" tabindex="-1"></a><span class="fu">### Confusion Matrix</span></span>
<span id="cb1-153"><a href="#cb1-153" aria-hidden="true" tabindex="-1"></a>A table used to evaluate classification model performance, showing the counts of true positives, true negatives, false positives, and false negatives. Helps calculate precision, recall, F1 score, and other metrics.</span>
<span id="cb1-154"><a href="#cb1-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-155"><a href="#cb1-155" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 2</span><span class="co">](lecture2.qmd)</span> (Machine Learning Fundamentals), <span class="co">[</span><span class="ot">Lecture 3</span><span class="co">](lecture3.qmd)</span> (Generalization Error)</span>
<span id="cb1-156"><a href="#cb1-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-157"><a href="#cb1-157" aria-hidden="true" tabindex="-1"></a><span class="fu">### Convolutional Neural Network (CNN)</span></span>
<span id="cb1-158"><a href="#cb1-158" aria-hidden="true" tabindex="-1"></a>Neural network architectures specialized for processing grid-structured data (especially images) through specialized layers:</span>
<span id="cb1-159"><a href="#cb1-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-160"><a href="#cb1-160" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Convolutional layers:** Apply learned filters across spatial dimensions, detecting local patterns while sharing parameters</span>
<span id="cb1-161"><a href="#cb1-161" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Pooling layers:** Reduce spatial dimensions by summarizing regions (max pooling, average pooling)</span>
<span id="cb1-162"><a href="#cb1-162" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Fully connected layers:** Typically used in final stages for classification or regression</span>
<span id="cb1-163"><a href="#cb1-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-164"><a href="#cb1-164" aria-hidden="true" tabindex="-1"></a>Key benefits include translation invariance, parameter efficiency through weight sharing, and hierarchical feature learning. Notable architectures include LeNet, AlexNet, VGG, and ResNet.</span>
<span id="cb1-165"><a href="#cb1-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-166"><a href="#cb1-166" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 12</span><span class="co">](lecture12.qmd)</span> (Image Classification with Convolutional Neural Networks), <span class="co">[</span><span class="ot">Lecture 13</span><span class="co">](lecture13.qmd)</span> (Advanced CNN Architectures and Transfer Learning)</span>
<span id="cb1-167"><a href="#cb1-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-168"><a href="#cb1-168" aria-hidden="true" tabindex="-1"></a><span class="fu">### Cross-Validation</span></span>
<span id="cb1-169"><a href="#cb1-169" aria-hidden="true" tabindex="-1"></a>A model evaluation technique that partitions data into training and validation sets multiple times to estimate performance. Common approaches include:</span>
<span id="cb1-170"><a href="#cb1-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-171"><a href="#cb1-171" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**K-fold cross-validation:** Divides data into k equal subsets (folds), trains on k-1 folds and tests on the remaining fold, rotating through all folds</span>
<span id="cb1-172"><a href="#cb1-172" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Leave-one-out cross-validation:** Uses a single observation for validation and all others for training, repeated for each observation</span>
<span id="cb1-173"><a href="#cb1-173" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Stratified cross-validation:** Maintains class distribution proportions in each fold</span>
<span id="cb1-174"><a href="#cb1-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-175"><a href="#cb1-175" aria-hidden="true" tabindex="-1"></a>Cross-validation provides more reliable performance estimates than single train-test splits, especially with limited data.</span>
<span id="cb1-176"><a href="#cb1-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-177"><a href="#cb1-177" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 3</span><span class="co">](lecture3.qmd)</span> (Generalization Error)</span>
<span id="cb1-178"><a href="#cb1-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-179"><a href="#cb1-179" aria-hidden="true" tabindex="-1"></a><span class="fu">### Curriculum Learning</span></span>
<span id="cb1-180"><a href="#cb1-180" aria-hidden="true" tabindex="-1"></a>Training strategy where models are taught easier concepts before harder ones, similar to how humans learn. This approach can lead to better performance and faster convergence by gradually increasing task complexity during training.</span>
<span id="cb1-181"><a href="#cb1-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-182"><a href="#cb1-182" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 13</span><span class="co">](lecture13.qmd)</span> (Advanced CNN Architectures and Transfer Learning)</span>
<span id="cb1-183"><a href="#cb1-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-184"><a href="#cb1-184" aria-hidden="true" tabindex="-1"></a><span class="fu">## D</span></span>
<span id="cb1-185"><a href="#cb1-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-186"><a href="#cb1-186" aria-hidden="true" tabindex="-1"></a><span class="fu">### Data Augmentation</span></span>
<span id="cb1-187"><a href="#cb1-187" aria-hidden="true" tabindex="-1"></a>A regularization technique that artificially expands training datasets by applying label-preserving transformations to existing samples. Common transformations include:</span>
<span id="cb1-188"><a href="#cb1-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-189"><a href="#cb1-189" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Image domain:** Rotations, flips, crops, color adjustments, elastic distortions</span>
<span id="cb1-190"><a href="#cb1-190" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Text domain:** Synonym substitution, back-translation, random deletion</span>
<span id="cb1-191"><a href="#cb1-191" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Audio domain:** Time stretching, pitch shifting, adding noise</span>
<span id="cb1-192"><a href="#cb1-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-193"><a href="#cb1-193" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 13</span><span class="co">](lecture13.qmd)</span> (Advanced CNN Architectures and Transfer Learning)</span>
<span id="cb1-194"><a href="#cb1-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-195"><a href="#cb1-195" aria-hidden="true" tabindex="-1"></a><span class="fu">### Decision Boundaries</span></span>
<span id="cb1-196"><a href="#cb1-196" aria-hidden="true" tabindex="-1"></a>The separating surfaces in feature space that demarcate different class regions in classification models. In linear models, these are hyperplanes; in nonlinear models, they can form complex shapes. Decision boundaries directly relate to model complexity - more flexible models can create more intricate decision boundaries.</span>
<span id="cb1-197"><a href="#cb1-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-198"><a href="#cb1-198" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 8</span><span class="co">](lecture8.qmd)</span> (Introduction to Neural Networks), <span class="co">[</span><span class="ot">Lecture 9</span><span class="co">](lecture9.qmd)</span> (Deep Neural Networks)</span>
<span id="cb1-199"><a href="#cb1-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-200"><a href="#cb1-200" aria-hidden="true" tabindex="-1"></a><span class="fu">### Decision Trees</span></span>
<span id="cb1-201"><a href="#cb1-201" aria-hidden="true" tabindex="-1"></a>Non-parametric supervised learning models that recursively partition the feature space based on feature values to create a tree-like model of decisions. Key components include:</span>
<span id="cb1-202"><a href="#cb1-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-203"><a href="#cb1-203" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Nodes:** Test a particular feature against a threshold</span>
<span id="cb1-204"><a href="#cb1-204" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Branches:** Outcomes of the tests</span>
<span id="cb1-205"><a href="#cb1-205" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Leaf nodes:** Final predictions or class assignments</span>
<span id="cb1-206"><a href="#cb1-206" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Splitting criteria:** Metrics like entropy, information gain, or Gini impurity that determine optimal splits</span>
<span id="cb1-207"><a href="#cb1-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-208"><a href="#cb1-208" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 7</span><span class="co">](lecture7.qmd)</span> (Nonlinearities and Expressive Learning Methods)</span>
<span id="cb1-209"><a href="#cb1-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-210"><a href="#cb1-210" aria-hidden="true" tabindex="-1"></a><span class="fu">### Deep Learning</span></span>
<span id="cb1-211"><a href="#cb1-211" aria-hidden="true" tabindex="-1"></a>A subset of machine learning using neural networks with multiple layers to progressively extract higher-level features from raw input. Characterized by:</span>
<span id="cb1-212"><a href="#cb1-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-213"><a href="#cb1-213" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Automatic feature extraction without manual engineering</span>
<span id="cb1-214"><a href="#cb1-214" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>End-to-end learning from raw data to final output</span>
<span id="cb1-215"><a href="#cb1-215" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Hierarchical representations increasing in abstraction with network depth</span>
<span id="cb1-216"><a href="#cb1-216" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Capacity to model extremely complex, non-linear relationships</span>
<span id="cb1-217"><a href="#cb1-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-218"><a href="#cb1-218" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 8</span><span class="co">](lecture8.qmd)</span> (Introduction to Neural Networks), <span class="co">[</span><span class="ot">Lecture 9</span><span class="co">](lecture9.qmd)</span> (Deep Neural Networks)</span>
<span id="cb1-219"><a href="#cb1-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-220"><a href="#cb1-220" aria-hidden="true" tabindex="-1"></a><span class="fu">### Diffusion Models</span></span>
<span id="cb1-221"><a href="#cb1-221" aria-hidden="true" tabindex="-1"></a>Generative models that learn to convert noise into structured data by reversing a gradual noising process. The approach involves:</span>
<span id="cb1-222"><a href="#cb1-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-223"><a href="#cb1-223" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Forward process:** Gradually adding Gaussian noise to training data according to a fixed schedule until it becomes pure noise</span>
<span id="cb1-224"><a href="#cb1-224" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Reverse process:** Training a neural network to denoise images step by step</span>
<span id="cb1-225"><a href="#cb1-225" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Sampling:** Generating new data by starting with random noise and iteratively denoising</span>
<span id="cb1-226"><a href="#cb1-226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-227"><a href="#cb1-227" aria-hidden="true" tabindex="-1"></a>Key advantages include high-quality generation, stable training, and good sample diversity. Examples include DDPM and Stable Diffusion.</span>
<span id="cb1-228"><a href="#cb1-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-229"><a href="#cb1-229" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 26</span><span class="co">](lecture26.qmd)</span> (Diffusion Models)</span>
<span id="cb1-230"><a href="#cb1-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-231"><a href="#cb1-231" aria-hidden="true" tabindex="-1"></a><span class="fu">### Dropout</span></span>
<span id="cb1-232"><a href="#cb1-232" aria-hidden="true" tabindex="-1"></a>A regularization technique where randomly selected neurons are temporarily removed during training iterations:</span>
<span id="cb1-233"><a href="#cb1-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-234"><a href="#cb1-234" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>For each training batch, each neuron has probability $p$ of being retained</span>
<span id="cb1-235"><a href="#cb1-235" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>During testing, all neurons are used but outputs are scaled by $p$ to maintain expected activation levels</span>
<span id="cb1-236"><a href="#cb1-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-237"><a href="#cb1-237" aria-hidden="true" tabindex="-1"></a>This prevents co-adaptation of neurons, effectively training an ensemble of subnetworks, which reduces overfitting and improves generalization.</span>
<span id="cb1-238"><a href="#cb1-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-239"><a href="#cb1-239" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 11</span><span class="co">](lecture11.qmd)</span> (Vanishing Gradients and Generalization)</span>
<span id="cb1-240"><a href="#cb1-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-241"><a href="#cb1-241" aria-hidden="true" tabindex="-1"></a><span class="fu">## E</span></span>
<span id="cb1-242"><a href="#cb1-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-243"><a href="#cb1-243" aria-hidden="true" tabindex="-1"></a><span class="fu">### Early Stopping</span></span>
<span id="cb1-244"><a href="#cb1-244" aria-hidden="true" tabindex="-1"></a>A regularization technique where training is halted when performance on a validation set stops improving. Implementation typically involves:</span>
<span id="cb1-245"><a href="#cb1-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-246"><a href="#cb1-246" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Monitoring validation loss/metric during training</span>
<span id="cb1-247"><a href="#cb1-247" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Stopping when validation performance has not improved for a specified number of iterations (patience)</span>
<span id="cb1-248"><a href="#cb1-248" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Reverting to the model weights that achieved best validation performance</span>
<span id="cb1-249"><a href="#cb1-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-250"><a href="#cb1-250" aria-hidden="true" tabindex="-1"></a>This prevents overfitting by not allowing the model to continue learning noise in the training data.</span>
<span id="cb1-251"><a href="#cb1-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-252"><a href="#cb1-252" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 11</span><span class="co">](lecture11.qmd)</span> (Vanishing Gradients and Generalization)</span>
<span id="cb1-253"><a href="#cb1-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-254"><a href="#cb1-254" aria-hidden="true" tabindex="-1"></a><span class="fu">### Embedding</span></span>
<span id="cb1-255"><a href="#cb1-255" aria-hidden="true" tabindex="-1"></a>A learned mapping from discrete entities (words, categories, user IDs) to continuous vector spaces of lower dimensionality. Embeddings:</span>
<span id="cb1-256"><a href="#cb1-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-257"><a href="#cb1-257" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Capture semantic relationships between entities in vector space</span>
<span id="cb1-258"><a href="#cb1-258" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Enable neural networks to process categorical inputs</span>
<span id="cb1-259"><a href="#cb1-259" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Place similar entities close together in the embedding space</span>
<span id="cb1-260"><a href="#cb1-260" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Serve as dense, information-rich features for downstream tasks</span>
<span id="cb1-261"><a href="#cb1-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-262"><a href="#cb1-262" aria-hidden="true" tabindex="-1"></a>Common applications include word embeddings (Word2Vec, GloVe), entity embeddings, and graph embeddings.</span>
<span id="cb1-263"><a href="#cb1-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-264"><a href="#cb1-264" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 17</span><span class="co">](lecture17.qmd)</span> (Sequence-to-Sequence Models and Attention Mechanisms), <span class="co">[</span><span class="ot">Lecture 19</span><span class="co">](lecture19.qmd)</span> (Representation Learning, Vision Transformers, and Autoregressive Generation)</span>
<span id="cb1-265"><a href="#cb1-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-266"><a href="#cb1-266" aria-hidden="true" tabindex="-1"></a><span class="fu">### Ensemble Methods</span></span>
<span id="cb1-267"><a href="#cb1-267" aria-hidden="true" tabindex="-1"></a>Techniques that combine multiple models to improve predictive performance and robustness. Major approaches include:</span>
<span id="cb1-268"><a href="#cb1-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-269"><a href="#cb1-269" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Bagging (Bootstrap Aggregating):** Trains models on random subsets of data with replacement and averages predictions</span>
<span id="cb1-270"><a href="#cb1-270" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Boosting:** Sequentially trains models with each focusing on errors from previous models</span>
<span id="cb1-271"><a href="#cb1-271" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Stacking:** Uses a meta-model that learns how to best combine predictions from base models</span>
<span id="cb1-272"><a href="#cb1-272" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Random Forests:** Combines bagging with random feature subset selection in decision trees</span>
<span id="cb1-273"><a href="#cb1-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-274"><a href="#cb1-274" aria-hidden="true" tabindex="-1"></a>Ensembles typically achieve better generalization by reducing variance or bias depending on the method used.</span>
<span id="cb1-275"><a href="#cb1-275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-276"><a href="#cb1-276" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 11</span><span class="co">](lecture11.qmd)</span> (Vanishing Gradients and Generalization), <span class="co">[</span><span class="ot">Lecture 27</span><span class="co">](lecture27.qmd)</span> (Interpretable Machine Learning)</span>
<span id="cb1-277"><a href="#cb1-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-278"><a href="#cb1-278" aria-hidden="true" tabindex="-1"></a><span class="fu">### Explainable AI (XAI)</span></span>
<span id="cb1-279"><a href="#cb1-279" aria-hidden="true" tabindex="-1"></a>Methods and techniques that make AI systems' decisions understandable and interpretable to humans. Approaches include:</span>
<span id="cb1-280"><a href="#cb1-280" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Post-hoc methods:** Feature importance, SHAP values, LIME, attention visualization</span>
<span id="cb1-281"><a href="#cb1-281" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Glass-box models:** Inherently interpretable models like decision trees, linear models</span>
<span id="cb1-282"><a href="#cb1-282" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Concept attribution:** Mapping activations to human-understandable concepts</span>
<span id="cb1-283"><a href="#cb1-283" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Counterfactual explanations:** Identifying minimal changes to inputs that would change outputs</span>
<span id="cb1-284"><a href="#cb1-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-285"><a href="#cb1-285" aria-hidden="true" tabindex="-1"></a>XAI is crucial for building trust, ensuring fairness, meeting regulatory requirements, and identifying potential biases in models.</span>
<span id="cb1-286"><a href="#cb1-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-287"><a href="#cb1-287" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 27</span><span class="co">](lecture27.qmd)</span> (Interpretable Machine Learning)</span>
<span id="cb1-288"><a href="#cb1-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-289"><a href="#cb1-289" aria-hidden="true" tabindex="-1"></a><span class="fu">### Evidence Lower Bound (ELBO)</span></span>
<span id="cb1-290"><a href="#cb1-290" aria-hidden="true" tabindex="-1"></a>A key concept in variational inference that provides a tractable lower bound on the marginal log-likelihood of observed data:</span>
<span id="cb1-291"><a href="#cb1-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-292"><a href="#cb1-292" aria-hidden="true" tabindex="-1"></a>$$\text{ELBO} = \mathbb{E}_Q[\log P(\mathbf{x}|\mathbf{z},\boldsymbol{\theta})] - D_{KL}(Q(\mathbf{z}|\mathbf{x},\boldsymbol{\phi}) || P(\mathbf{z}))$$</span>
<span id="cb1-293"><a href="#cb1-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-294"><a href="#cb1-294" aria-hidden="true" tabindex="-1"></a>The ELBO consists of two components:</span>
<span id="cb1-295"><a href="#cb1-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-296"><a href="#cb1-296" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Reconstruction term**: Expected log-likelihood of data given latent variables</span>
<span id="cb1-297"><a href="#cb1-297" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Regularization term**: KL divergence between the approximate posterior and the prior</span>
<span id="cb1-298"><a href="#cb1-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-299"><a href="#cb1-299" aria-hidden="true" tabindex="-1"></a>Maximizing the ELBO is equivalent to minimizing the KL divergence between the approximate and true posterior distributions. This objective function is central to training variational autoencoders and other approximate Bayesian methods.</span>
<span id="cb1-300"><a href="#cb1-300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-301"><a href="#cb1-301" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 23</span><span class="co">](lecture23.qmd)</span> (Variational Autoencoders)</span>
<span id="cb1-302"><a href="#cb1-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-303"><a href="#cb1-303" aria-hidden="true" tabindex="-1"></a><span class="fu">## F</span></span>
<span id="cb1-304"><a href="#cb1-304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-305"><a href="#cb1-305" aria-hidden="true" tabindex="-1"></a><span class="fu">### Feature Engineering</span></span>
<span id="cb1-306"><a href="#cb1-306" aria-hidden="true" tabindex="-1"></a>The process of using domain knowledge to extract or create features from raw data to improve machine learning model performance. Techniques include:</span>
<span id="cb1-307"><a href="#cb1-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-308"><a href="#cb1-308" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Transformation:** Applying mathematical functions to create new representations</span>
<span id="cb1-309"><a href="#cb1-309" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Encoding:** Converting categorical variables to numerical form</span>
<span id="cb1-310"><a href="#cb1-310" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Discretization:** Binning continuous variables into categories</span>
<span id="cb1-311"><a href="#cb1-311" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Interaction features:** Creating new features from combinations of existing ones</span>
<span id="cb1-312"><a href="#cb1-312" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Feature extraction:** Deriving informative features from raw data</span>
<span id="cb1-313"><a href="#cb1-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-314"><a href="#cb1-314" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 8</span><span class="co">](lecture8.qmd)</span> (Introduction to Neural Networks), <span class="co">[</span><span class="ot">Lecture 9</span><span class="co">](lecture9.qmd)</span> (Deep Neural Networks)</span>
<span id="cb1-315"><a href="#cb1-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-316"><a href="#cb1-316" aria-hidden="true" tabindex="-1"></a><span class="fu">### Feature Selection</span></span>
<span id="cb1-317"><a href="#cb1-317" aria-hidden="true" tabindex="-1"></a>Techniques for identifying and selecting the most relevant variables for model building. Common approaches include:</span>
<span id="cb1-318"><a href="#cb1-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-319"><a href="#cb1-319" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Filter methods:** Evaluate features independently using statistical measures (correlation, chi-square)</span>
<span id="cb1-320"><a href="#cb1-320" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Wrapper methods:** Test feature subsets using the model itself (recursive feature elimination)</span>
<span id="cb1-321"><a href="#cb1-321" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Embedded methods:** Perform selection during model training (L1 regularization)</span>
<span id="cb1-322"><a href="#cb1-322" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-323"><a href="#cb1-323" aria-hidden="true" tabindex="-1"></a>Effective feature selection improves model performance, reduces training time, and enhances model interpretability.</span>
<span id="cb1-324"><a href="#cb1-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-325"><a href="#cb1-325" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 3</span><span class="co">](lecture3.qmd)</span> (Generalization Error), <span class="co">[</span><span class="ot">Lecture 4</span><span class="co">](lecture4.qmd)</span> (Linear Models and Likelihood)</span>
<span id="cb1-326"><a href="#cb1-326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-327"><a href="#cb1-327" aria-hidden="true" tabindex="-1"></a><span class="fu">### Forward and Backward Passes</span></span>
<span id="cb1-328"><a href="#cb1-328" aria-hidden="true" tabindex="-1"></a>The two fundamental computational phases in neural network training:</span>
<span id="cb1-329"><a href="#cb1-329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-330"><a href="#cb1-330" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Forward Pass**: The computation flow from input to output where:</span>
<span id="cb1-331"><a href="#cb1-331" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Input data propagates through the network layer by layer</span>
<span id="cb1-332"><a href="#cb1-332" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Each layer applies weights, biases, and activation functions</span>
<span id="cb1-333"><a href="#cb1-333" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Intermediate values and final outputs are computed and stored</span>
<span id="cb1-334"><a href="#cb1-334" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>The loss function evaluates prediction quality</span>
<span id="cb1-335"><a href="#cb1-335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-336"><a href="#cb1-336" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Backward Pass**: The computation flow from output back to input where:</span>
<span id="cb1-337"><a href="#cb1-337" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Gradients of the loss with respect to parameters are computed</span>
<span id="cb1-338"><a href="#cb1-338" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>The chain rule is applied to propagate error signals backward</span>
<span id="cb1-339"><a href="#cb1-339" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Parameter updates are calculated for optimization algorithms</span>
<span id="cb1-340"><a href="#cb1-340" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Weights and biases are adjusted to minimize the loss</span>
<span id="cb1-341"><a href="#cb1-341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-342"><a href="#cb1-342" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 10</span><span class="co">](lecture10.qmd)</span> (Backpropagation and Gradient Problems)</span>
<span id="cb1-343"><a href="#cb1-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-344"><a href="#cb1-344" aria-hidden="true" tabindex="-1"></a><span class="fu">### Fine-tuning</span></span>
<span id="cb1-345"><a href="#cb1-345" aria-hidden="true" tabindex="-1"></a>The process of further training a pre-trained model on a specific target task with domain-specific data. The approach typically involves:</span>
<span id="cb1-346"><a href="#cb1-346" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-347"><a href="#cb1-347" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Starting with a model pre-trained on a large general dataset</span>
<span id="cb1-348"><a href="#cb1-348" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Replacing or modifying the output layers for the target task</span>
<span id="cb1-349"><a href="#cb1-349" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Training on the target dataset, typically with lower learning rates</span>
<span id="cb1-350"><a href="#cb1-350" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Potentially freezing early layers to preserve general features</span>
<span id="cb1-351"><a href="#cb1-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-352"><a href="#cb1-352" aria-hidden="true" tabindex="-1"></a>Fine-tuning leverages knowledge transfer to achieve better performance with less task-specific data.</span>
<span id="cb1-353"><a href="#cb1-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-354"><a href="#cb1-354" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 19</span><span class="co">](lecture19.qmd)</span> (Transfer Learning)</span>
<span id="cb1-355"><a href="#cb1-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-356"><a href="#cb1-356" aria-hidden="true" tabindex="-1"></a><span class="fu">### F1 Score</span></span>
<span id="cb1-357"><a href="#cb1-357" aria-hidden="true" tabindex="-1"></a>A performance metric that combines precision and recall into a single harmonized score:</span>
<span id="cb1-358"><a href="#cb1-358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-359"><a href="#cb1-359" aria-hidden="true" tabindex="-1"></a>$$F1 = 2 \cdot \frac{\text{precision} \cdot \text{recall}}{\text{precision} + \text{recall}}$$</span>
<span id="cb1-360"><a href="#cb1-360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-361"><a href="#cb1-361" aria-hidden="true" tabindex="-1"></a>F1 score ranges from 0 (worst) to 1 (best) and is particularly valuable for imbalanced classification problems where both false positives and false negatives are important considerations.</span>
<span id="cb1-362"><a href="#cb1-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-363"><a href="#cb1-363" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 6</span><span class="co">](lecture6.qmd)</span> (Classification Performance Metrics)</span>
<span id="cb1-364"><a href="#cb1-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-365"><a href="#cb1-365" aria-hidden="true" tabindex="-1"></a><span class="fu">## G</span></span>
<span id="cb1-366"><a href="#cb1-366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-367"><a href="#cb1-367" aria-hidden="true" tabindex="-1"></a><span class="fu">### Gated Recurrent Unit (GRU)</span></span>
<span id="cb1-368"><a href="#cb1-368" aria-hidden="true" tabindex="-1"></a>A type of recurrent neural network architecture that uses gating mechanisms to control information flow. Simpler than LSTMs with fewer parameters, GRUs use reset and update gates to handle the vanishing gradient problem while maintaining good performance.</span>
<span id="cb1-369"><a href="#cb1-369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-370"><a href="#cb1-370" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 16</span><span class="co">](lecture16.qmd)</span> (Recurrent Neural Networks and Sequence Modeling)</span>
<span id="cb1-371"><a href="#cb1-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-372"><a href="#cb1-372" aria-hidden="true" tabindex="-1"></a><span class="fu">### Generalization Error</span></span>
<span id="cb1-373"><a href="#cb1-373" aria-hidden="true" tabindex="-1"></a>The expected error when applying a model to new, unseen data. It measures how well a model performs beyond its training data and can be estimated using validation sets or cross-validation. Good generalization is the ultimate goal of machine learning.</span>
<span id="cb1-374"><a href="#cb1-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-375"><a href="#cb1-375" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 3</span><span class="co">](lecture3.qmd)</span> (Generalization Error)</span>
<span id="cb1-376"><a href="#cb1-376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-377"><a href="#cb1-377" aria-hidden="true" tabindex="-1"></a><span class="fu">### Generative Adversarial Networks (GANs)</span></span>
<span id="cb1-378"><a href="#cb1-378" aria-hidden="true" tabindex="-1"></a>Framework where two neural networks compete in a game-theoretic scenario:</span>
<span id="cb1-379"><a href="#cb1-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-380"><a href="#cb1-380" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Generator**: Creates synthetic data samples trying to fool the discriminator</span>
<span id="cb1-381"><a href="#cb1-381" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Discriminator**: Distinguishes between real and generated samples</span>
<span id="cb1-382"><a href="#cb1-382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-383"><a href="#cb1-383" aria-hidden="true" tabindex="-1"></a>Through this adversarial process, the generator learns to produce increasingly realistic data, while the discriminator becomes better at detecting fakes. This minimax game continues until equilibrium, resulting in a generator that creates high-quality synthetic data.</span>
<span id="cb1-384"><a href="#cb1-384" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-385"><a href="#cb1-385" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 24</span><span class="co">](lecture24.qmd)</span> (VAE Extensions and GANs Introduction), <span class="co">[</span><span class="ot">Lecture 25</span><span class="co">](lecture25.qmd)</span> (Generative Adversarial Networks)</span>
<span id="cb1-386"><a href="#cb1-386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-387"><a href="#cb1-387" aria-hidden="true" tabindex="-1"></a><span class="fu">### Generative Models</span></span>
<span id="cb1-388"><a href="#cb1-388" aria-hidden="true" tabindex="-1"></a>Models that learn to generate new data samples from the same distribution as the training data. Types include:</span>
<span id="cb1-389"><a href="#cb1-389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-390"><a href="#cb1-390" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Autoregressive models**: Model sequential dependencies (e.g., PixelCNN, GPT)</span>
<span id="cb1-391"><a href="#cb1-391" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Variational Autoencoders**: Learn probabilistic latent representations</span>
<span id="cb1-392"><a href="#cb1-392" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**GANs**: Use adversarial training to generate realistic samples</span>
<span id="cb1-393"><a href="#cb1-393" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Diffusion models**: Generate data by reversing a noising process</span>
<span id="cb1-394"><a href="#cb1-394" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Flow-based models**: Use invertible transformations to model probability distributions</span>
<span id="cb1-395"><a href="#cb1-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-396"><a href="#cb1-396" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 20</span><span class="co">](lecture20.qmd)</span> (Generative Models: Autoregressives and Autoencoders), <span class="co">[</span><span class="ot">Lecture 24</span><span class="co">](lecture24.qmd)</span> (VAE Extensions and GANs Introduction), <span class="co">[</span><span class="ot">Lecture 25</span><span class="co">](lecture25.qmd)</span> (Generative Adversarial Networks), <span class="co">[</span><span class="ot">Lecture 26</span><span class="co">](lecture26.qmd)</span> (Diffusion Models)</span>
<span id="cb1-397"><a href="#cb1-397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-398"><a href="#cb1-398" aria-hidden="true" tabindex="-1"></a><span class="fu">### Generalized Linear Models (GLMs)</span></span>
<span id="cb1-399"><a href="#cb1-399" aria-hidden="true" tabindex="-1"></a>Extension of linear regression that allows for response variables with non-normal distributions. GLMs consist of:</span>
<span id="cb1-400"><a href="#cb1-400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-401"><a href="#cb1-401" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Random component**: Specifies the distribution of the response variable (e.g., Gaussian, Binomial, Poisson)</span>
<span id="cb1-402"><a href="#cb1-402" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Systematic component**: Linear combination of predictors</span>
<span id="cb1-403"><a href="#cb1-403" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Link function**: Connects the expected value of the response to the linear predictor</span>
<span id="cb1-404"><a href="#cb1-404" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-405"><a href="#cb1-405" aria-hidden="true" tabindex="-1"></a>Common examples include logistic regression, Poisson regression, and log-linear models.</span>
<span id="cb1-406"><a href="#cb1-406" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-407"><a href="#cb1-407" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 4</span><span class="co">](lecture4.qmd)</span> (Linear Models and Likelihood)</span>
<span id="cb1-408"><a href="#cb1-408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-409"><a href="#cb1-409" aria-hidden="true" tabindex="-1"></a><span class="fu">### Gradient Descent</span></span>
<span id="cb1-410"><a href="#cb1-410" aria-hidden="true" tabindex="-1"></a>An optimization algorithm used to minimize loss functions by iteratively adjusting parameters in the direction of steepest descent of the gradient. Variants include:</span>
<span id="cb1-411"><a href="#cb1-411" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-412"><a href="#cb1-412" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Batch Gradient Descent**: Uses entire dataset for each update</span>
<span id="cb1-413"><a href="#cb1-413" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Stochastic Gradient Descent (SGD)**: Uses a single sample for each update</span>
<span id="cb1-414"><a href="#cb1-414" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Mini-batch Gradient Descent**: Uses a small random subset of data for each update</span>
<span id="cb1-415"><a href="#cb1-415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-416"><a href="#cb1-416" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 5</span><span class="co">](lecture5.qmd)</span> (Loss Minimization and Optimization), <span class="co">[</span><span class="ot">Lecture 10</span><span class="co">](lecture10.qmd)</span> (Backpropagation and Gradient Problems)</span>
<span id="cb1-417"><a href="#cb1-417" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-418"><a href="#cb1-418" aria-hidden="true" tabindex="-1"></a><span class="fu">## H</span></span>
<span id="cb1-419"><a href="#cb1-419" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-420"><a href="#cb1-420" aria-hidden="true" tabindex="-1"></a><span class="fu">### Hyperparameter</span></span>
<span id="cb1-421"><a href="#cb1-421" aria-hidden="true" tabindex="-1"></a>Parameters set before training begins (unlike model parameters that are learned during training). Examples include learning rate, number of layers, number of neurons per layer, batch size, and regularization strength. Tuning hyperparameters typically requires techniques like grid search, random search, or Bayesian optimization.</span>
<span id="cb1-422"><a href="#cb1-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-423"><a href="#cb1-423" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 8</span><span class="co">](lecture8.qmd)</span> (Introduction to Neural Networks), <span class="co">[</span><span class="ot">Lecture 11</span><span class="co">](lecture11.qmd)</span> (Vanishing Gradients and Generalization)</span>
<span id="cb1-424"><a href="#cb1-424" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-425"><a href="#cb1-425" aria-hidden="true" tabindex="-1"></a><span class="fu">## I</span></span>
<span id="cb1-426"><a href="#cb1-426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-427"><a href="#cb1-427" aria-hidden="true" tabindex="-1"></a><span class="fu">### Information Gain</span></span>
<span id="cb1-428"><a href="#cb1-428" aria-hidden="true" tabindex="-1"></a>A metric used in decision trees to determine the quality of a split, based on the reduction in entropy after a dataset is split. Higher information gain indicates a more useful split.</span>
<span id="cb1-429"><a href="#cb1-429" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-430"><a href="#cb1-430" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 7</span><span class="co">](lecture7.qmd)</span> (Nonlinearities and Expressive Learning Methods)</span>
<span id="cb1-431"><a href="#cb1-431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-432"><a href="#cb1-432" aria-hidden="true" tabindex="-1"></a><span class="fu">### Instance Segmentation</span></span>
<span id="cb1-433"><a href="#cb1-433" aria-hidden="true" tabindex="-1"></a>Computer vision task that involves identifying each instance of each object in an image at the pixel level. It combines elements of object detection (finding and classifying objects) with semantic segmentation (determining the class of each pixel).</span>
<span id="cb1-434"><a href="#cb1-434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-435"><a href="#cb1-435" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 14</span><span class="co">](lecture14.qmd)</span> (Object Detection in Computer Vision), <span class="co">[</span><span class="ot">Lecture 15</span><span class="co">](lecture15.qmd)</span> (Semantic Segmentation and Advanced CNN Applications)</span>
<span id="cb1-436"><a href="#cb1-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-437"><a href="#cb1-437" aria-hidden="true" tabindex="-1"></a><span class="fu">## K</span></span>
<span id="cb1-438"><a href="#cb1-438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-439"><a href="#cb1-439" aria-hidden="true" tabindex="-1"></a><span class="fu">### K-means Clustering</span></span>
<span id="cb1-440"><a href="#cb1-440" aria-hidden="true" tabindex="-1"></a>An unsupervised learning algorithm that partitions data into K clusters by minimizing the sum of squared distances between data points and their assigned cluster centroids. The algorithm works by:</span>
<span id="cb1-441"><a href="#cb1-441" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-442"><a href="#cb1-442" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Randomly initializing K cluster centroids</span>
<span id="cb1-443"><a href="#cb1-443" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Assigning each data point to the nearest centroid</span>
<span id="cb1-444"><a href="#cb1-444" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Updating centroids based on assigned points</span>
<span id="cb1-445"><a href="#cb1-445" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Repeating steps 2-3 until convergence</span>
<span id="cb1-446"><a href="#cb1-446" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-447"><a href="#cb1-447" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 2</span><span class="co">](lecture2.qmd)</span> (Machine Learning Fundamentals)</span>
<span id="cb1-448"><a href="#cb1-448" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-449"><a href="#cb1-449" aria-hidden="true" tabindex="-1"></a><span class="fu">### Kernel Methods</span></span>
<span id="cb1-450"><a href="#cb1-450" aria-hidden="true" tabindex="-1"></a>Techniques that implicitly transform data into higher-dimensional spaces without explicitly computing the coordinates in that space. The kernel trick allows for efficient computation of inner products in these spaces, making non-linear patterns linearly separable. Support Vector Machines often use kernel methods.</span>
<span id="cb1-451"><a href="#cb1-451" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-452"><a href="#cb1-452" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 7</span><span class="co">](lecture7.qmd)</span> (Nonlinearities and Expressive Learning Methods)</span>
<span id="cb1-453"><a href="#cb1-453" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-454"><a href="#cb1-454" aria-hidden="true" tabindex="-1"></a><span class="fu">### Kullback-Leibler (KL) Divergence</span></span>
<span id="cb1-455"><a href="#cb1-455" aria-hidden="true" tabindex="-1"></a>A statistical measure that quantifies the difference between two probability distributions P and Q:</span>
<span id="cb1-456"><a href="#cb1-456" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-457"><a href="#cb1-457" aria-hidden="true" tabindex="-1"></a>$$D_{KL}(P||Q) = \sum_x P(x) \log \frac{P(x)}{Q(x)} \text{ or } \int P(x) \log \frac{P(x)}{Q(x)} dx$$</span>
<span id="cb1-458"><a href="#cb1-458" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-459"><a href="#cb1-459" aria-hidden="true" tabindex="-1"></a>Key properties include:</span>
<span id="cb1-460"><a href="#cb1-460" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-461"><a href="#cb1-461" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Always non-negative (≥ 0)</span>
<span id="cb1-462"><a href="#cb1-462" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Equals zero only when the distributions are identical</span>
<span id="cb1-463"><a href="#cb1-463" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Asymmetric: $D_{KL}(P||Q) \neq D_{KL}(Q||P)$</span>
<span id="cb1-464"><a href="#cb1-464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-465"><a href="#cb1-465" aria-hidden="true" tabindex="-1"></a>In machine learning, KL divergence serves as:</span>
<span id="cb1-466"><a href="#cb1-466" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-467"><a href="#cb1-467" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>A regularization term in variational autoencoders</span>
<span id="cb1-468"><a href="#cb1-468" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>A component in the evidence lower bound (ELBO)</span>
<span id="cb1-469"><a href="#cb1-469" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>A loss function for matching probability distributions</span>
<span id="cb1-470"><a href="#cb1-470" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>A way to measure how much information is lost when approximating one distribution with another</span>
<span id="cb1-471"><a href="#cb1-471" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-472"><a href="#cb1-472" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 22</span><span class="co">](lecture22.qmd)</span> (Bayesian Machine Learning), <span class="co">[</span><span class="ot">Lecture 23</span><span class="co">](lecture23.qmd)</span> (Variational Autoencoders)</span>
<span id="cb1-473"><a href="#cb1-473" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-474"><a href="#cb1-474" aria-hidden="true" tabindex="-1"></a><span class="fu">## M</span></span>
<span id="cb1-475"><a href="#cb1-475" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-476"><a href="#cb1-476" aria-hidden="true" tabindex="-1"></a><span class="fu">### Machine Learning Fundamentals</span></span>
<span id="cb1-477"><a href="#cb1-477" aria-hidden="true" tabindex="-1"></a>The core components that define machine learning as articulated by Tom Mitchell (1997):</span>
<span id="cb1-478"><a href="#cb1-478" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-479"><a href="#cb1-479" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Tasks (T)**: The objectives the algorithm aims to achieve:</span>
<span id="cb1-480"><a href="#cb1-480" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>**Prediction**: Forecasting future or unknown values</span>
<span id="cb1-481"><a href="#cb1-481" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>**Description**: Uncovering patterns or structures in data</span>
<span id="cb1-482"><a href="#cb1-482" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>**Inference/Explanation**: Understanding causal relationships and variable impacts</span>
<span id="cb1-483"><a href="#cb1-483" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-484"><a href="#cb1-484" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Experience (E)**: The data that fuels learning:</span>
<span id="cb1-485"><a href="#cb1-485" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>**Supervised Learning**: Uses labeled data</span>
<span id="cb1-486"><a href="#cb1-486" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>**Unsupervised Learning**: Uses unlabeled data</span>
<span id="cb1-487"><a href="#cb1-487" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>**Semi-supervised Learning**: Uses both labeled and unlabeled data</span>
<span id="cb1-488"><a href="#cb1-488" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>**Reinforcement Learning**: Learning through environment interaction</span>
<span id="cb1-489"><a href="#cb1-489" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-490"><a href="#cb1-490" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Performance Measure (P)**: Metrics quantifying algorithm performance, typically through loss functions</span>
<span id="cb1-491"><a href="#cb1-491" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-492"><a href="#cb1-492" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 2</span><span class="co">](lecture2.qmd)</span> (Machine Learning Fundamentals)</span>
<span id="cb1-493"><a href="#cb1-493" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-494"><a href="#cb1-494" aria-hidden="true" tabindex="-1"></a><span class="fu">## L</span></span>
<span id="cb1-495"><a href="#cb1-495" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-496"><a href="#cb1-496" aria-hidden="true" tabindex="-1"></a><span class="fu">### L1 Regularization (LASSO)</span></span>
<span id="cb1-497"><a href="#cb1-497" aria-hidden="true" tabindex="-1"></a>Regularization technique that adds the sum of absolute values of coefficients to the loss function, promoting sparsity by shrinking less important feature coefficients to exactly zero. The regularization term is $\lambda\sum|w_i|$, where $\lambda$ controls the strength of the penalty.</span>
<span id="cb1-498"><a href="#cb1-498" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-499"><a href="#cb1-499" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 4</span><span class="co">](lecture4.qmd)</span> (Linear Models and Likelihood)</span>
<span id="cb1-500"><a href="#cb1-500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-501"><a href="#cb1-501" aria-hidden="true" tabindex="-1"></a><span class="fu">### L2 Regularization (Ridge)</span></span>
<span id="cb1-502"><a href="#cb1-502" aria-hidden="true" tabindex="-1"></a>Regularization technique that adds the sum of squared coefficients to the loss function, preventing overfitting by shrinking all coefficients toward zero but rarely setting them exactly to zero. The regularization term is $\lambda\sum w_i^2$, where $\lambda$ controls the strength of the penalty.</span>
<span id="cb1-503"><a href="#cb1-503" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-504"><a href="#cb1-504" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 4</span><span class="co">](lecture4.qmd)</span> (Linear Models and Likelihood), <span class="co">[</span><span class="ot">Lecture 11</span><span class="co">](lecture11.qmd)</span> (Vanishing Gradients and Generalization)</span>
<span id="cb1-505"><a href="#cb1-505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-506"><a href="#cb1-506" aria-hidden="true" tabindex="-1"></a><span class="fu">### Laplace Approximation</span></span>
<span id="cb1-507"><a href="#cb1-507" aria-hidden="true" tabindex="-1"></a>A method for approximating Bayesian posterior distributions with a multivariate Gaussian distribution centered at the maximum a posteriori (MAP) estimate:</span>
<span id="cb1-508"><a href="#cb1-508" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-509"><a href="#cb1-509" aria-hidden="true" tabindex="-1"></a>$$P(\boldsymbol{\theta}|\mathcal{D}) \approx \mathcal{N}(\boldsymbol{\theta}|\hat{\boldsymbol{\mu}}, \hat{\boldsymbol{\Sigma}})$$</span>
<span id="cb1-510"><a href="#cb1-510" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-511"><a href="#cb1-511" aria-hidden="true" tabindex="-1"></a>Where:</span>
<span id="cb1-512"><a href="#cb1-512" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-513"><a href="#cb1-513" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\hat{\boldsymbol{\mu}} = \arg\max_{\boldsymbol{\theta}} P(\boldsymbol{\theta}|\mathcal{D})$ is the MAP estimate</span>
<span id="cb1-514"><a href="#cb1-514" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\hat{\boldsymbol{\Sigma}} = -\left<span class="co">[</span><span class="ot">\frac{\partial^2 \log P(\hat{\boldsymbol{\mu}}|\mathcal{D})}{\partial \boldsymbol{\theta} \partial \boldsymbol{\theta}'}\right</span><span class="co">]</span>^{-1}$ is the inverse Hessian of the negative log posterior</span>
<span id="cb1-515"><a href="#cb1-515" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-516"><a href="#cb1-516" aria-hidden="true" tabindex="-1"></a>This approximation is particularly useful for models with intractable posterior distributions, providing both parameter estimates and uncertainty quantification. The Bernstein-von Mises theorem guarantees that under certain conditions, the posterior distribution converges to this normal approximation as sample size increases.</span>
<span id="cb1-517"><a href="#cb1-517" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-518"><a href="#cb1-518" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 22</span><span class="co">](lecture22.qmd)</span> (Bayesian Machine Learning)</span>
<span id="cb1-519"><a href="#cb1-519" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-520"><a href="#cb1-520" aria-hidden="true" tabindex="-1"></a><span class="fu">### Latent Space</span></span>
<span id="cb1-521"><a href="#cb1-521" aria-hidden="true" tabindex="-1"></a>A compressed representation in lower dimensions that captures the essential features of the input data. In autoencoders and VAEs, this is the bottleneck layer. Points that are close in latent space should represent semantically similar inputs.</span>
<span id="cb1-522"><a href="#cb1-522" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-523"><a href="#cb1-523" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 20</span><span class="co">](lecture20.qmd)</span> (Generative Models: Autoregressives and Autoencoders), <span class="co">[</span><span class="ot">Lecture 24</span><span class="co">](lecture24.qmd)</span> (VAE Extensions and GANs Introduction)</span>
<span id="cb1-524"><a href="#cb1-524" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-525"><a href="#cb1-525" aria-hidden="true" tabindex="-1"></a><span class="fu">### Learning Rate</span></span>
<span id="cb1-526"><a href="#cb1-526" aria-hidden="true" tabindex="-1"></a>A hyperparameter that controls how much to adjust model weights in response to the estimated error during training. Too high a learning rate can cause training to diverge; too low can cause it to converge too slowly or get stuck in suboptimal solutions.</span>
<span id="cb1-527"><a href="#cb1-527" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-528"><a href="#cb1-528" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 5</span><span class="co">](lecture5.qmd)</span> (Loss Minimization and Optimization), <span class="co">[</span><span class="ot">Lecture 10</span><span class="co">](lecture10.qmd)</span> (Backpropagation and Gradient Problems)</span>
<span id="cb1-529"><a href="#cb1-529" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-530"><a href="#cb1-530" aria-hidden="true" tabindex="-1"></a><span class="fu">### Learning Rate Scheduling</span></span>
<span id="cb1-531"><a href="#cb1-531" aria-hidden="true" tabindex="-1"></a>Techniques to adjust the learning rate during training, typically reducing it over time. Common approaches include step decay, exponential decay, cosine annealing, and warm restarts.</span>
<span id="cb1-532"><a href="#cb1-532" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-533"><a href="#cb1-533" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 5</span><span class="co">](lecture5.qmd)</span> (Loss Minimization and Optimization), <span class="co">[</span><span class="ot">Lecture 6</span><span class="co">](lecture6.qmd)</span> (Adaptive Methods for Minimization)</span>
<span id="cb1-534"><a href="#cb1-534" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-535"><a href="#cb1-535" aria-hidden="true" tabindex="-1"></a><span class="fu">### Likelihood</span></span>
<span id="cb1-536"><a href="#cb1-536" aria-hidden="true" tabindex="-1"></a>In statistical inference, the conditional probability of observing the data given specific parameter values, expressed as $P(\mathcal{D}|\boldsymbol{\theta})$. The likelihood function:</span>
<span id="cb1-537"><a href="#cb1-537" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-538"><a href="#cb1-538" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Measures how well different parameter values explain observed data</span>
<span id="cb1-539"><a href="#cb1-539" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Forms the basis for maximum likelihood estimation (MLE) when maximizing $P(\mathcal{D}|\boldsymbol{\theta})$</span>
<span id="cb1-540"><a href="#cb1-540" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Serves as a critical component in Bayes' theorem: $P(\boldsymbol{\theta}|\mathcal{D}) \propto P(\mathcal{D}|\boldsymbol{\theta})P(\boldsymbol{\theta})$</span>
<span id="cb1-541"><a href="#cb1-541" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Often expressed and optimized in logarithmic form (log-likelihood) for computational stability</span>
<span id="cb1-542"><a href="#cb1-542" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-543"><a href="#cb1-543" aria-hidden="true" tabindex="-1"></a>Common examples include Gaussian likelihood for regression and Bernoulli likelihood for binary classification.</span>
<span id="cb1-544"><a href="#cb1-544" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-545"><a href="#cb1-545" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 4</span><span class="co">](lecture4.qmd)</span> (Linear Models and Likelihood), <span class="co">[</span><span class="ot">Lecture 22</span><span class="co">](lecture22.qmd)</span> (Bayesian Machine Learning)</span>
<span id="cb1-546"><a href="#cb1-546" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-547"><a href="#cb1-547" aria-hidden="true" tabindex="-1"></a><span class="fu">### Long Short-Term Memory (LSTM)</span></span>
<span id="cb1-548"><a href="#cb1-548" aria-hidden="true" tabindex="-1"></a>A specialized recurrent neural network architecture designed to address the vanishing gradient problem in standard RNNs. LSTMs use gating mechanisms (input, forget, and output gates) to retain information over long sequences.</span>
<span id="cb1-549"><a href="#cb1-549" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-550"><a href="#cb1-550" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 16</span><span class="co">](lecture16.qmd)</span> (Recurrent Neural Networks and Sequence Modeling)</span>
<span id="cb1-551"><a href="#cb1-551" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-552"><a href="#cb1-552" aria-hidden="true" tabindex="-1"></a><span class="fu">### Logistic Regression</span></span>
<span id="cb1-553"><a href="#cb1-553" aria-hidden="true" tabindex="-1"></a>Statistical model used for binary classification that applies the logistic (sigmoid) function to a linear combination of features, transforming an unbounded output to a probability between 0 and 1. The formula is:</span>
<span id="cb1-554"><a href="#cb1-554" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-555"><a href="#cb1-555" aria-hidden="true" tabindex="-1"></a>$$P(y = 1 | \mathbf x) = \frac{1}{1 + \exp<span class="co">[</span><span class="ot">-\mathbf w^T \mathbf x - b</span><span class="co">]</span>}$$</span>
<span id="cb1-556"><a href="#cb1-556" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-557"><a href="#cb1-557" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-558"><a href="#cb1-558" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-559"><a href="#cb1-559" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 4</span><span class="co">](lecture4.qmd)</span> (Linear Models and Likelihood), <span class="co">[</span><span class="ot">Lecture 5</span><span class="co">](lecture5.qmd)</span> (Loss Minimization and Optimization)</span>
<span id="cb1-560"><a href="#cb1-560" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-561"><a href="#cb1-561" aria-hidden="true" tabindex="-1"></a><span class="fu">### Loss Function</span></span>
<span id="cb1-562"><a href="#cb1-562" aria-hidden="true" tabindex="-1"></a>A function that quantifies how well a model's predictions match the true values. Common loss functions include:</span>
<span id="cb1-563"><a href="#cb1-563" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-564"><a href="#cb1-564" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Mean Squared Error (MSE)**: For regression problems</span>
<span id="cb1-565"><a href="#cb1-565" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Cross-Entropy Loss**: For classification problems</span>
<span id="cb1-566"><a href="#cb1-566" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Hinge Loss**: Used in SVMs</span>
<span id="cb1-567"><a href="#cb1-567" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Kullback-Leibler Divergence**: Measures difference between probability distributions</span>
<span id="cb1-568"><a href="#cb1-568" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-569"><a href="#cb1-569" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 2</span><span class="co">](lecture2.qmd)</span> (Machine Learning Fundamentals), <span class="co">[</span><span class="ot">Lecture 3</span><span class="co">](lecture3.qmd)</span> (Generalization Error), <span class="co">[</span><span class="ot">Lecture 5</span><span class="co">](lecture5.qmd)</span> (Loss Minimization and Optimization)</span>
<span id="cb1-570"><a href="#cb1-570" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-571"><a href="#cb1-571" aria-hidden="true" tabindex="-1"></a><span class="fu">## M</span></span>
<span id="cb1-572"><a href="#cb1-572" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-573"><a href="#cb1-573" aria-hidden="true" tabindex="-1"></a><span class="fu">### Marginal Likelihood</span></span>
<span id="cb1-574"><a href="#cb1-574" aria-hidden="true" tabindex="-1"></a>Also known as model evidence, the marginal likelihood represents the probability of observing the data integrated over all possible parameter values:</span>
<span id="cb1-575"><a href="#cb1-575" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-576"><a href="#cb1-576" aria-hidden="true" tabindex="-1"></a>$$P(\mathcal{D}) = \int P(\mathcal{D}|\boldsymbol{\theta})P(\boldsymbol{\theta})d\boldsymbol{\theta}$$</span>
<span id="cb1-577"><a href="#cb1-577" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-578"><a href="#cb1-578" aria-hidden="true" tabindex="-1"></a>Key properties and applications include:</span>
<span id="cb1-579"><a href="#cb1-579" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-580"><a href="#cb1-580" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Serves as the normalizing constant in Bayes' theorem</span>
<span id="cb1-581"><a href="#cb1-581" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Automatically implements Occam's razor, balancing model fit and complexity</span>
<span id="cb1-582"><a href="#cb1-582" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Used for model comparison and hyperparameter selection </span>
<span id="cb1-583"><a href="#cb1-583" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Often computationally intractable, requiring approximation methods</span>
<span id="cb1-584"><a href="#cb1-584" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>For linear regression with normal prior: $P(\mathbf{y}|\mathbf{X}) = \mathcal{N}(\mathbf{y}|\mathbf{X}\boldsymbol{\mu}_0, \sigma^2\mathbf{I} + \mathbf{X}\boldsymbol{\Sigma}_0\mathbf{X}^T)$</span>
<span id="cb1-585"><a href="#cb1-585" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-586"><a href="#cb1-586" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 22</span><span class="co">](lecture22.qmd)</span> (Bayesian Machine Learning), <span class="co">[</span><span class="ot">Lecture 23</span><span class="co">](lecture23.qmd)</span> (Variational Autoencoders)</span>
<span id="cb1-587"><a href="#cb1-587" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-588"><a href="#cb1-588" aria-hidden="true" tabindex="-1"></a><span class="fu">### Maximum Likelihood Estimation (MLE)</span></span>
<span id="cb1-589"><a href="#cb1-589" aria-hidden="true" tabindex="-1"></a>A method for estimating the parameters of a statistical model by finding values that maximize the likelihood function, which measures how probable the observed data is given the parameter values.</span>
<span id="cb1-590"><a href="#cb1-590" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-591"><a href="#cb1-591" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 4</span><span class="co">](lecture4.qmd)</span> (Linear Models and Likelihood)</span>
<span id="cb1-592"><a href="#cb1-592" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-593"><a href="#cb1-593" aria-hidden="true" tabindex="-1"></a><span class="fu">### Model Calibration</span></span>
<span id="cb1-594"><a href="#cb1-594" aria-hidden="true" tabindex="-1"></a>The property that a model's predicted probabilities accurately reflect true probabilities. A well-calibrated model's confidence should match its accuracy. Techniques include Platt scaling and temperature scaling.</span>
<span id="cb1-595"><a href="#cb1-595" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-596"><a href="#cb1-596" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 4</span><span class="co">](lecture4.qmd)</span> (Linear Models and Likelihood), <span class="co">[</span><span class="ot">Lecture 27</span><span class="co">](lecture27.qmd)</span> (Interpretable Machine Learning)</span>
<span id="cb1-597"><a href="#cb1-597" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-598"><a href="#cb1-598" aria-hidden="true" tabindex="-1"></a><span class="fu">### Multi-Head Attention</span></span>
<span id="cb1-599"><a href="#cb1-599" aria-hidden="true" tabindex="-1"></a>An extension of self-attention that runs multiple attention mechanisms in parallel, allowing the model to focus on different aspects of the input simultaneously. Each "head" projects the input into different subspaces before computing attention, capturing diverse relationships in the data. The outputs from all heads are concatenated and linearly transformed to produce the final result. This approach enhances model expressivity by enabling attention to different representation subspaces and positions.</span>
<span id="cb1-600"><a href="#cb1-600" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-601"><a href="#cb1-601" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 18</span><span class="co">](lecture18.qmd)</span> (Transformers and the Attention Revolution)</span>
<span id="cb1-602"><a href="#cb1-602" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-603"><a href="#cb1-603" aria-hidden="true" tabindex="-1"></a><span class="fu">### Multi-Layer Perceptron (MLP)</span></span>
<span id="cb1-604"><a href="#cb1-604" aria-hidden="true" tabindex="-1"></a>A feedforward neural network with at least one hidden layer between input and output layers. Each neuron in one layer connects to every neuron in the next layer, using non-linear activation functions to learn complex patterns.</span>
<span id="cb1-605"><a href="#cb1-605" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-606"><a href="#cb1-606" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 8</span><span class="co">](lecture8.qmd)</span> (Introduction to Neural Networks), <span class="co">[</span><span class="ot">Lecture 9</span><span class="co">](lecture9.qmd)</span> (Deep Neural Networks)</span>
<span id="cb1-607"><a href="#cb1-607" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-608"><a href="#cb1-608" aria-hidden="true" tabindex="-1"></a><span class="fu">## N</span></span>
<span id="cb1-609"><a href="#cb1-609" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-610"><a href="#cb1-610" aria-hidden="true" tabindex="-1"></a><span class="fu">### Neural Network Architecture</span></span>
<span id="cb1-611"><a href="#cb1-611" aria-hidden="true" tabindex="-1"></a>The specific arrangement of layers, neurons, and connections in a neural network. Includes the number of layers (depth), neurons per layer (width), types of layers, activation functions, and connectivity patterns.</span>
<span id="cb1-612"><a href="#cb1-612" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-613"><a href="#cb1-613" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 9</span><span class="co">](lecture9.qmd)</span> (Deep Neural Networks), <span class="co">[</span><span class="ot">Lecture 13</span><span class="co">](lecture13.qmd)</span> (Advanced CNN Architectures and Transfer Learning)</span>
<span id="cb1-614"><a href="#cb1-614" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-615"><a href="#cb1-615" aria-hidden="true" tabindex="-1"></a><span class="fu">### Normalization</span></span>
<span id="cb1-616"><a href="#cb1-616" aria-hidden="true" tabindex="-1"></a>Techniques that adjust the scale of inputs or internal representations to improve neural network training. Examples include:</span>
<span id="cb1-617"><a href="#cb1-617" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-618"><a href="#cb1-618" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Batch Normalization**: Normalizes layer inputs within a mini-batch</span>
<span id="cb1-619"><a href="#cb1-619" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Layer Normalization**: Normalizes across features for each sample</span>
<span id="cb1-620"><a href="#cb1-620" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Instance Normalization**: Normalizes across spatial dimensions for each sample</span>
<span id="cb1-621"><a href="#cb1-621" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Group Normalization**: Normalizes across subsets of channels</span>
<span id="cb1-622"><a href="#cb1-622" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-623"><a href="#cb1-623" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 11</span><span class="co">](lecture11.qmd)</span> (Vanishing Gradients and Generalization), <span class="co">[</span><span class="ot">Lecture 13</span><span class="co">](lecture13.qmd)</span> (Advanced CNN Architectures and Transfer Learning)</span>
<span id="cb1-624"><a href="#cb1-624" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-625"><a href="#cb1-625" aria-hidden="true" tabindex="-1"></a><span class="fu">## O</span></span>
<span id="cb1-626"><a href="#cb1-626" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-627"><a href="#cb1-627" aria-hidden="true" tabindex="-1"></a><span class="fu">### Object Detection</span></span>
<span id="cb1-628"><a href="#cb1-628" aria-hidden="true" tabindex="-1"></a>Computer vision task that involves identifying and localizing multiple objects in images. Combines classification (what objects are present) with localization (where they are). Common architectures include R-CNN, Fast R-CNN, Faster R-CNN, YOLO, and SSD.</span>
<span id="cb1-629"><a href="#cb1-629" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-630"><a href="#cb1-630" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 14</span><span class="co">](lecture14.qmd)</span> (Object Detection in Computer Vision)</span>
<span id="cb1-631"><a href="#cb1-631" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-632"><a href="#cb1-632" aria-hidden="true" tabindex="-1"></a><span class="fu">### One-Hot Encoding</span></span>
<span id="cb1-633"><a href="#cb1-633" aria-hidden="true" tabindex="-1"></a>A representation of categorical variables as binary vectors where each position corresponds to a category, and only one position has a value of 1 (the category that applies), while all others are 0.</span>
<span id="cb1-634"><a href="#cb1-634" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-635"><a href="#cb1-635" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 2</span><span class="co">](lecture2.qmd)</span> (Machine Learning Fundamentals), <span class="co">[</span><span class="ot">Lecture 6</span><span class="co">](lecture6.qmd)</span> (Adaptive Methods for Minimization)</span>
<span id="cb1-636"><a href="#cb1-636" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-637"><a href="#cb1-637" aria-hidden="true" tabindex="-1"></a><span class="fu">### Optimization Algorithms</span></span>
<span id="cb1-638"><a href="#cb1-638" aria-hidden="true" tabindex="-1"></a>Methods used to minimize the loss function and update model parameters. Beyond basic gradient descent, advanced algorithms include:</span>
<span id="cb1-639"><a href="#cb1-639" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-640"><a href="#cb1-640" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Momentum**: Adds a fraction of the previous update to accelerate convergence</span>
<span id="cb1-641"><a href="#cb1-641" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**RMSProp**: Adapts learning rates using a moving average of squared gradients</span>
<span id="cb1-642"><a href="#cb1-642" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Adam**: Combines momentum and RMSProp approaches</span>
<span id="cb1-643"><a href="#cb1-643" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**AdamW**: A variant of Adam with improved weight decay regularization</span>
<span id="cb1-644"><a href="#cb1-644" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-645"><a href="#cb1-645" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 5</span><span class="co">](lecture5.qmd)</span> (Loss Minimization and Optimization), <span class="co">[</span><span class="ot">Lecture 6</span><span class="co">](lecture6.qmd)</span> (Adaptive Methods for Minimization)</span>
<span id="cb1-646"><a href="#cb1-646" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-647"><a href="#cb1-647" aria-hidden="true" tabindex="-1"></a><span class="fu">### Overfitting</span></span>
<span id="cb1-648"><a href="#cb1-648" aria-hidden="true" tabindex="-1"></a>When a model learns the training data too well, capturing noise instead of the underlying pattern, leading to poor generalization. Signs include:</span>
<span id="cb1-649"><a href="#cb1-649" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-650"><a href="#cb1-650" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>High accuracy on training data but low accuracy on validation/test data</span>
<span id="cb1-651"><a href="#cb1-651" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Complex model with many parameters relative to the amount of training data</span>
<span id="cb1-652"><a href="#cb1-652" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Model memorizes training examples rather than learning general patterns</span>
<span id="cb1-653"><a href="#cb1-653" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-654"><a href="#cb1-654" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 3</span><span class="co">](lecture3.qmd)</span> (Generalization Error), <span class="co">[</span><span class="ot">Lecture 4</span><span class="co">](lecture4.qmd)</span> (Linear Models and Likelihood)</span>
<span id="cb1-655"><a href="#cb1-655" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-656"><a href="#cb1-656" aria-hidden="true" tabindex="-1"></a><span class="fu">## P</span></span>
<span id="cb1-657"><a href="#cb1-657" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-658"><a href="#cb1-658" aria-hidden="true" tabindex="-1"></a><span class="fu">### Parameter Sharing</span></span>
<span id="cb1-659"><a href="#cb1-659" aria-hidden="true" tabindex="-1"></a>A technique in neural networks where the same parameters are used across different parts of the model. Most prominently used in CNNs, where the same convolutional filters are applied across the entire image, drastically reducing the number of parameters and making the network translation invariant.</span>
<span id="cb1-660"><a href="#cb1-660" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-661"><a href="#cb1-661" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 12</span><span class="co">](lecture12.qmd)</span> (Image Classification with Convolutional Neural Networks)</span>
<span id="cb1-662"><a href="#cb1-662" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-663"><a href="#cb1-663" aria-hidden="true" tabindex="-1"></a><span class="fu">### Pooling</span></span>
<span id="cb1-664"><a href="#cb1-664" aria-hidden="true" tabindex="-1"></a>Operations in convolutional neural networks that reduce spatial dimensions by summarizing values in local regions. Common pooling types include:</span>
<span id="cb1-665"><a href="#cb1-665" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-666"><a href="#cb1-666" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Max pooling**: Takes the maximum value in each window</span>
<span id="cb1-667"><a href="#cb1-667" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Average pooling**: Takes the average value in each window</span>
<span id="cb1-668"><a href="#cb1-668" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Global pooling**: Reduces each feature map to a single value</span>
<span id="cb1-669"><a href="#cb1-669" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-670"><a href="#cb1-670" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 12</span><span class="co">](lecture12.qmd)</span> (Image Classification with Convolutional Neural Networks)</span>
<span id="cb1-671"><a href="#cb1-671" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-672"><a href="#cb1-672" aria-hidden="true" tabindex="-1"></a><span class="fu">### Pre-training</span></span>
<span id="cb1-673"><a href="#cb1-673" aria-hidden="true" tabindex="-1"></a>Training a model on a large dataset for a general task before fine-tuning it on a specific task with less data. Enables transfer learning by learning general features that can be adapted to new tasks.</span>
<span id="cb1-674"><a href="#cb1-674" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-675"><a href="#cb1-675" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 13</span><span class="co">](lecture13.qmd)</span> (Advanced CNN Architectures and Transfer Learning), <span class="co">[</span><span class="ot">Lecture 19</span><span class="co">](lecture19.qmd)</span> (Representation Learning, Vision Transformers, and Autoregressive Generation)</span>
<span id="cb1-676"><a href="#cb1-676" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-677"><a href="#cb1-677" aria-hidden="true" tabindex="-1"></a><span class="fu">### Principal Component Analysis (PCA)</span></span>
<span id="cb1-678"><a href="#cb1-678" aria-hidden="true" tabindex="-1"></a>A dimensionality reduction technique that transforms data into a new coordinate system where the greatest variance lies along the first coordinate (first principal component), the second greatest variance along the second coordinate, and so on.</span>
<span id="cb1-679"><a href="#cb1-679" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-680"><a href="#cb1-680" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 7</span><span class="co">](lecture7.qmd)</span> (Nonlinearities and Expressive Learning Methods)</span>
<span id="cb1-681"><a href="#cb1-681" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-682"><a href="#cb1-682" aria-hidden="true" tabindex="-1"></a><span class="fu">### Prior</span></span>
<span id="cb1-683"><a href="#cb1-683" aria-hidden="true" tabindex="-1"></a>A probability distribution $P(\boldsymbol{\theta})$ that expresses beliefs about model parameters before observing data. Within Bayesian statistics, priors:</span>
<span id="cb1-684"><a href="#cb1-684" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-685"><a href="#cb1-685" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Encode domain knowledge or subjective beliefs</span>
<span id="cb1-686"><a href="#cb1-686" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Range from informative (strong beliefs) to diffuse/non-informative (minimal assumptions)</span>
<span id="cb1-687"><a href="#cb1-687" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Are updated to posterior distributions through Bayes' theorem</span>
<span id="cb1-688"><a href="#cb1-688" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Can have significant impact with small datasets but diminish in influence as data increases</span>
<span id="cb1-689"><a href="#cb1-689" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-690"><a href="#cb1-690" aria-hidden="true" tabindex="-1"></a>Common examples include:</span>
<span id="cb1-691"><a href="#cb1-691" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-692"><a href="#cb1-692" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Normal priors for regression coefficients</span>
<span id="cb1-693"><a href="#cb1-693" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Laplace priors leading to L1 regularization (LASSO)</span>
<span id="cb1-694"><a href="#cb1-694" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Normal priors leading to L2 regularization (Ridge)</span>
<span id="cb1-695"><a href="#cb1-695" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Dirichlet priors for multinomial parameters</span>
<span id="cb1-696"><a href="#cb1-696" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Conjugate priors that result in closed-form posteriors</span>
<span id="cb1-697"><a href="#cb1-697" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-698"><a href="#cb1-698" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 22</span><span class="co">](lecture22.qmd)</span> (Bayesian Machine Learning)</span>
<span id="cb1-699"><a href="#cb1-699" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-700"><a href="#cb1-700" aria-hidden="true" tabindex="-1"></a><span class="fu">### Precision</span></span>
<span id="cb1-701"><a href="#cb1-701" aria-hidden="true" tabindex="-1"></a>A classification metric measuring the proportion of true positive predictions among all positive predictions: TP/(TP+FP). High precision means a low false positive rate, which is important when the cost of a false positive is high.</span>
<span id="cb1-702"><a href="#cb1-702" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-703"><a href="#cb1-703" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 3</span><span class="co">](lecture3.qmd)</span> (Generalization Error)</span>
<span id="cb1-704"><a href="#cb1-704" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-705"><a href="#cb1-705" aria-hidden="true" tabindex="-1"></a><span class="fu">## Q</span></span>
<span id="cb1-706"><a href="#cb1-706" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-707"><a href="#cb1-707" aria-hidden="true" tabindex="-1"></a><span class="fu">### Q-Learning</span></span>
<span id="cb1-708"><a href="#cb1-708" aria-hidden="true" tabindex="-1"></a>A model-free reinforcement learning algorithm that learns the value of actions in states (Q-values) and selects actions based on these values. The goal is to find an optimal policy that maximizes the expected cumulative reward.</span>
<span id="cb1-709"><a href="#cb1-709" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-710"><a href="#cb1-710" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 27</span><span class="co">](lecture27.qmd)</span> (Interpretable Machine Learning)</span>
<span id="cb1-711"><a href="#cb1-711" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-712"><a href="#cb1-712" aria-hidden="true" tabindex="-1"></a><span class="fu">## R</span></span>
<span id="cb1-713"><a href="#cb1-713" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-714"><a href="#cb1-714" aria-hidden="true" tabindex="-1"></a><span class="fu">### Random Forest</span></span>
<span id="cb1-715"><a href="#cb1-715" aria-hidden="true" tabindex="-1"></a>An ensemble learning method that constructs multiple decision trees during training and outputs the mode of the classes (for classification) or mean prediction (for regression) of the individual trees. Reduces overfitting by combining many trees trained on different subsets of data and features.</span>
<span id="cb1-716"><a href="#cb1-716" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-717"><a href="#cb1-717" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 7</span><span class="co">](lecture7.qmd)</span> (Nonlinearities and Expressive Learning Methods)</span>
<span id="cb1-718"><a href="#cb1-718" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-719"><a href="#cb1-719" aria-hidden="true" tabindex="-1"></a><span class="fu">### Recall</span></span>
<span id="cb1-720"><a href="#cb1-720" aria-hidden="true" tabindex="-1"></a>A classification metric measuring the proportion of actual positive cases that were correctly identified: TP/(TP+FN). High recall means a low false negative rate, which is important when the cost of missing a positive case is high.</span>
<span id="cb1-721"><a href="#cb1-721" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-722"><a href="#cb1-722" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 3</span><span class="co">](lecture3.qmd)</span> (Generalization Error)</span>
<span id="cb1-723"><a href="#cb1-723" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-724"><a href="#cb1-724" aria-hidden="true" tabindex="-1"></a><span class="fu">### Recurrent Neural Networks (RNNs)</span></span>
<span id="cb1-725"><a href="#cb1-725" aria-hidden="true" tabindex="-1"></a>Neural network architectures designed to process sequential data by maintaining internal state through recurrent connections. The defining feature is the recurrence relation: h_t = f_w(h_{t-1}, x_t), where h_t is the current hidden state, h_{t-1} is the previous hidden state, and x_t is the current input.</span>
<span id="cb1-726"><a href="#cb1-726" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-727"><a href="#cb1-727" aria-hidden="true" tabindex="-1"></a>RNNs can handle various sequence relationships:</span>
<span id="cb1-728"><a href="#cb1-728" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-729"><a href="#cb1-729" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**One-to-Many**: Single input generates a sequence (e.g., image captioning)</span>
<span id="cb1-730"><a href="#cb1-730" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Many-to-One**: Sequence produces single output (e.g., sentiment analysis)</span>
<span id="cb1-731"><a href="#cb1-731" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Many-to-Many (Unaligned)**: Input sequence yields different-length output sequence (e.g., translation)</span>
<span id="cb1-732"><a href="#cb1-732" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Many-to-Many (Aligned)**: Each input element corresponds to an output element (e.g., part-of-speech tagging)</span>
<span id="cb1-733"><a href="#cb1-733" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-734"><a href="#cb1-734" aria-hidden="true" tabindex="-1"></a>Challenges include vanishing and exploding gradients when processing long sequences, which specialized architectures like LSTMs and GRUs address.</span>
<span id="cb1-735"><a href="#cb1-735" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-736"><a href="#cb1-736" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 16</span><span class="co">](lecture16.qmd)</span> (Recurrent Neural Networks and Sequence Modeling)</span>
<span id="cb1-737"><a href="#cb1-737" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-738"><a href="#cb1-738" aria-hidden="true" tabindex="-1"></a><span class="fu">### Regularization</span></span>
<span id="cb1-739"><a href="#cb1-739" aria-hidden="true" tabindex="-1"></a>Techniques to prevent overfitting by adding constraints or penalties to model complexity. Common methods include:</span>
<span id="cb1-740"><a href="#cb1-740" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-741"><a href="#cb1-741" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**L1 regularization**: Encourages sparse models by penalizing absolute weight values</span>
<span id="cb1-742"><a href="#cb1-742" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**L2 regularization**: Penalizes large weights using squared magnitude</span>
<span id="cb1-743"><a href="#cb1-743" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Dropout**: Randomly ignores neurons during training</span>
<span id="cb1-744"><a href="#cb1-744" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Early stopping**: Halts training when validation performance stops improving</span>
<span id="cb1-745"><a href="#cb1-745" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Data augmentation**: Artificially expands training data with transformations</span>
<span id="cb1-746"><a href="#cb1-746" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Weight decay**: Gradually reduces weights during training</span>
<span id="cb1-747"><a href="#cb1-747" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-748"><a href="#cb1-748" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 4</span><span class="co">](lecture4.qmd)</span> (Linear Models and Likelihood), <span class="co">[</span><span class="ot">Lecture 11</span><span class="co">](lecture11.qmd)</span> (Vanishing Gradients and Generalization)</span>
<span id="cb1-749"><a href="#cb1-749" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-750"><a href="#cb1-750" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-751"><a href="#cb1-751" aria-hidden="true" tabindex="-1"></a><span class="fu">### ResNet (Residual Network)</span></span>
<span id="cb1-752"><a href="#cb1-752" aria-hidden="true" tabindex="-1"></a>A deep neural network architecture that addresses the vanishing gradient problem by introducing skip connections (residual connections) that allow gradients to flow directly through the network. This innovation enables training of very deep networks with hundreds of layers.</span>
<span id="cb1-753"><a href="#cb1-753" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-754"><a href="#cb1-754" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 13</span><span class="co">](lecture13.qmd)</span> (Advanced CNN Architectures and Transfer Learning)</span>
<span id="cb1-755"><a href="#cb1-755" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-756"><a href="#cb1-756" aria-hidden="true" tabindex="-1"></a><span class="fu">## S</span></span>
<span id="cb1-757"><a href="#cb1-757" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-758"><a href="#cb1-758" aria-hidden="true" tabindex="-1"></a><span class="fu">### Sequence Modeling</span></span>
<span id="cb1-759"><a href="#cb1-759" aria-hidden="true" tabindex="-1"></a>A machine learning paradigm focusing on data where order and temporal dependencies matter. Sequence models can handle various input-output relationships:</span>
<span id="cb1-760"><a href="#cb1-760" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-761"><a href="#cb1-761" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**One-to-Many**: Single input generates a sequence (e.g., image captioning)</span>
<span id="cb1-762"><a href="#cb1-762" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Many-to-One**: Sequence produces single output (e.g., sentiment analysis)</span>
<span id="cb1-763"><a href="#cb1-763" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Many-to-Many (Unaligned)**: Input sequence yields different-length output sequence (e.g., machine translation)</span>
<span id="cb1-764"><a href="#cb1-764" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Many-to-Many (Aligned)**: Each input element corresponds to an output element (e.g., part-of-speech tagging)</span>
<span id="cb1-765"><a href="#cb1-765" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-766"><a href="#cb1-766" aria-hidden="true" tabindex="-1"></a>Architectures for sequence modeling include RNNs, LSTMs, GRUs, Transformers, and specialized convolutional architectures.</span>
<span id="cb1-767"><a href="#cb1-767" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-768"><a href="#cb1-768" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 16</span><span class="co">](lecture16.qmd)</span> (Recurrent Neural Networks and Sequence Modeling)</span>
<span id="cb1-769"><a href="#cb1-769" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-770"><a href="#cb1-770" aria-hidden="true" tabindex="-1"></a><span class="fu">### Semantic Segmentation</span></span>
<span id="cb1-771"><a href="#cb1-771" aria-hidden="true" tabindex="-1"></a>Computer vision task that classifies each pixel in an image into a predefined category, assigning a semantic label to every pixel. Unlike instance segmentation, it doesn't distinguish between different instances of the same class.</span>
<span id="cb1-772"><a href="#cb1-772" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-773"><a href="#cb1-773" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 15</span><span class="co">](lecture15.qmd)</span> (Semantic Segmentation and Advanced CNN Applications)</span>
<span id="cb1-774"><a href="#cb1-774" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-775"><a href="#cb1-775" aria-hidden="true" tabindex="-1"></a><span class="fu">### Self-Attention</span></span>
<span id="cb1-776"><a href="#cb1-776" aria-hidden="true" tabindex="-1"></a>A neural network mechanism that allows models to directly connect any position in a sequence to any other position, enabling the model to focus on different parts of the input when producing outputs. The self-attention operation works by:</span>
<span id="cb1-777"><a href="#cb1-777" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-778"><a href="#cb1-778" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Computing three vectors for each token through learned linear projections:</span>
<span id="cb1-779"><a href="#cb1-779" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb1-780"><a href="#cb1-780" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**Query vector (q_i)**: Represents what information the token is "looking for"</span>
<span id="cb1-781"><a href="#cb1-781" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**Key vector (k_i)**: Represents what information the token "contains" </span>
<span id="cb1-782"><a href="#cb1-782" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**Value vector (v_i)**: Represents the actual content of the token</span>
<span id="cb1-783"><a href="#cb1-783" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-784"><a href="#cb1-784" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Calculating attention weights by comparing queries to keys: e_{ij} = (q_i^T k_j)/√d_k</span>
<span id="cb1-785"><a href="#cb1-785" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-786"><a href="#cb1-786" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Normalizing weights using softmax function</span>
<span id="cb1-787"><a href="#cb1-787" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-788"><a href="#cb1-788" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Computing a weighted sum of all value vectors</span>
<span id="cb1-789"><a href="#cb1-789" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-790"><a href="#cb1-790" aria-hidden="true" tabindex="-1"></a>This mechanism forms the core of transformer architectures and enables constant computational complexity regardless of sequence distance.</span>
<span id="cb1-791"><a href="#cb1-791" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-792"><a href="#cb1-792" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 18</span><span class="co">](lecture18.qmd)</span> (Transformers and the Attention Revolution)</span>
<span id="cb1-793"><a href="#cb1-793" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-794"><a href="#cb1-794" aria-hidden="true" tabindex="-1"></a><span class="fu">### Self-Supervised Learning</span></span>
<span id="cb1-795"><a href="#cb1-795" aria-hidden="true" tabindex="-1"></a>Training paradigm where models generate supervisory signals from the data itself without explicit labels. The model creates pretext tasks from unlabeled data (like predicting masked tokens, image rotations, or contrastive objectives) to learn useful representations, which can then be fine-tuned for downstream tasks with limited labeled data.</span>
<span id="cb1-796"><a href="#cb1-796" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-797"><a href="#cb1-797" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 19</span><span class="co">](lecture19.qmd)</span> (Representation Learning, Vision Transformers, and Autoregressive Generation)</span>
<span id="cb1-798"><a href="#cb1-798" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-799"><a href="#cb1-799" aria-hidden="true" tabindex="-1"></a><span class="fu">### Softmax Function</span></span>
<span id="cb1-800"><a href="#cb1-800" aria-hidden="true" tabindex="-1"></a>A function that converts a vector of real numbers into a probability distribution. It exponentiates each input and normalizes by the sum of all exponentiated values, ensuring all outputs are between 0 and 1 and sum to 1. Commonly used for multi-class classification.</span>
<span id="cb1-801"><a href="#cb1-801" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-802"><a href="#cb1-802" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 4</span><span class="co">](lecture4.qmd)</span> (Linear Models and Likelihood), <span class="co">[</span><span class="ot">Lecture 8</span><span class="co">](lecture8.qmd)</span> (Introduction to Neural Networks)</span>
<span id="cb1-803"><a href="#cb1-803" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-804"><a href="#cb1-804" aria-hidden="true" tabindex="-1"></a><span class="fu">### Stochastic Gradient Descent (SGD)</span></span>
<span id="cb1-805"><a href="#cb1-805" aria-hidden="true" tabindex="-1"></a>An optimization algorithm that updates model parameters using the gradient of the loss function with respect to those parameters, computed on randomly selected subsets (mini-batches) of the training data rather than the entire dataset.</span>
<span id="cb1-806"><a href="#cb1-806" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-807"><a href="#cb1-807" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 5</span><span class="co">](lecture5.qmd)</span> (Loss Minimization and Optimization), <span class="co">[</span><span class="ot">Lecture 10</span><span class="co">](lecture10.qmd)</span> (Backpropagation and Gradient Problems)</span>
<span id="cb1-808"><a href="#cb1-808" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-809"><a href="#cb1-809" aria-hidden="true" tabindex="-1"></a><span class="fu">### Support Vector Machine (SVM)</span></span>
<span id="cb1-810"><a href="#cb1-810" aria-hidden="true" tabindex="-1"></a>A supervised learning algorithm that finds a hyperplane in an N-dimensional space that distinctly separates data points of different classes. SVMs maximize the margin between classes and can perform non-linear classification using the kernel trick.</span>
<span id="cb1-811"><a href="#cb1-811" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-812"><a href="#cb1-812" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 7</span><span class="co">](lecture7.qmd)</span> (Nonlinearities and Expressive Learning Methods)</span>
<span id="cb1-813"><a href="#cb1-813" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-814"><a href="#cb1-814" aria-hidden="true" tabindex="-1"></a><span class="fu">## T</span></span>
<span id="cb1-815"><a href="#cb1-815" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-816"><a href="#cb1-816" aria-hidden="true" tabindex="-1"></a><span class="fu">### Temperature Scaling</span></span>
<span id="cb1-817"><a href="#cb1-817" aria-hidden="true" tabindex="-1"></a>A post-processing calibration technique for deep neural networks that divides the logits (pre-softmax outputs) by a scalar parameter called temperature. Higher temperatures produce softer probability distributions, while lower temperatures make them sharper.</span>
<span id="cb1-818"><a href="#cb1-818" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-819"><a href="#cb1-819" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 4</span><span class="co">](lecture4.qmd)</span> (Linear Models and Likelihood), <span class="co">[</span><span class="ot">Lecture 27</span><span class="co">](lecture27.qmd)</span> (Interpretable Machine Learning)</span>
<span id="cb1-820"><a href="#cb1-820" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-821"><a href="#cb1-821" aria-hidden="true" tabindex="-1"></a><span class="fu">### Transfer Learning</span></span>
<span id="cb1-822"><a href="#cb1-822" aria-hidden="true" tabindex="-1"></a>A machine learning technique where knowledge gained from training a model on one task is applied to a different but related task. This approach is particularly effective when the target task has limited training data. Common strategies include:</span>
<span id="cb1-823"><a href="#cb1-823" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-824"><a href="#cb1-824" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Feature extraction**: Reuse learned features from pre-trained models</span>
<span id="cb1-825"><a href="#cb1-825" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Fine-tuning**: Adapt a pre-trained model by updating some or all of its parameters for a new task</span>
<span id="cb1-826"><a href="#cb1-826" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-827"><a href="#cb1-827" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 13</span><span class="co">](lecture13.qmd)</span> (Advanced CNN Architectures and Transfer Learning), <span class="co">[</span><span class="ot">Lecture 19</span><span class="co">](lecture19.qmd)</span> (Representation Learning, Vision Transformers, and Autoregressive Generation)</span>
<span id="cb1-828"><a href="#cb1-828" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-829"><a href="#cb1-829" aria-hidden="true" tabindex="-1"></a><span class="fu">### Transformers</span></span>
<span id="cb1-830"><a href="#cb1-830" aria-hidden="true" tabindex="-1"></a>Neural network architecture introduced in "Attention is All You Need" (Vaswani et al., 2017) that overcomes limitations of recurrent networks through self-attention mechanisms. Transformers offer several key advantages:</span>
<span id="cb1-831"><a href="#cb1-831" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-832"><a href="#cb1-832" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Parallel processing of sequences rather than sequential computation</span>
<span id="cb1-833"><a href="#cb1-833" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Constant path length between any positions in a sequence</span>
<span id="cb1-834"><a href="#cb1-834" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Direct modeling of long-range dependencies</span>
<span id="cb1-835"><a href="#cb1-835" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Highly parallelizable computation</span>
<span id="cb1-836"><a href="#cb1-836" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-837"><a href="#cb1-837" aria-hidden="true" tabindex="-1"></a>The architecture consists of:</span>
<span id="cb1-838"><a href="#cb1-838" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-839"><a href="#cb1-839" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Multi-head self-attention**: Allows the model to focus on different aspects of the input simultaneously</span>
<span id="cb1-840"><a href="#cb1-840" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Position encodings**: Incorporate sequence order information since attention has no inherent position awareness</span>
<span id="cb1-841"><a href="#cb1-841" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Feed-forward networks**: Process attention outputs with two linear transformations and a non-linearity</span>
<span id="cb1-842"><a href="#cb1-842" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Layer normalization**: Stabilizes training by normalizing activations</span>
<span id="cb1-843"><a href="#cb1-843" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Residual connections**: Help with gradient flow in deep networks</span>
<span id="cb1-844"><a href="#cb1-844" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-845"><a href="#cb1-845" aria-hidden="true" tabindex="-1"></a>Transformers have revolutionized NLP with models like BERT (bidirectional encoding), GPT (autoregressive generation), and T5 (sequence-to-sequence), and have been adapted for vision (ViT), audio, and multimodal tasks.</span>
<span id="cb1-846"><a href="#cb1-846" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-847"><a href="#cb1-847" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 18</span><span class="co">](lecture18.qmd)</span> (Transformers and the Attention Revolution), <span class="co">[</span><span class="ot">Lecture 19</span><span class="co">](lecture19.qmd)</span> (Representation Learning, Vision Transformers, and Autoregressive Generation)</span>
<span id="cb1-848"><a href="#cb1-848" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-849"><a href="#cb1-849" aria-hidden="true" tabindex="-1"></a><span class="fu">## U</span></span>
<span id="cb1-850"><a href="#cb1-850" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-851"><a href="#cb1-851" aria-hidden="true" tabindex="-1"></a><span class="fu">### Underfitting</span></span>
<span id="cb1-852"><a href="#cb1-852" aria-hidden="true" tabindex="-1"></a>When a model is too simple to capture the underlying pattern in the data, resulting in poor performance on both training and test data. Signs include high bias and high error on training data.</span>
<span id="cb1-853"><a href="#cb1-853" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-854"><a href="#cb1-854" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 3</span><span class="co">](lecture3.qmd)</span> (Generalization Error), <span class="co">[</span><span class="ot">Lecture 4</span><span class="co">](lecture4.qmd)</span> (Linear Models and Likelihood)</span>
<span id="cb1-855"><a href="#cb1-855" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-856"><a href="#cb1-856" aria-hidden="true" tabindex="-1"></a><span class="fu">### Universal Approximation Theorem</span></span>
<span id="cb1-857"><a href="#cb1-857" aria-hidden="true" tabindex="-1"></a>A mathematical result stating that a feedforward neural network with a single hidden layer containing a finite number of neurons can approximate any Borel measurable function from one finite-dimensional space to another with arbitrary precision, under mild assumptions about the activation function. Key aspects include:</span>
<span id="cb1-858"><a href="#cb1-858" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-859"><a href="#cb1-859" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Provides the theoretical foundation for neural networks' ability to model complex, nonlinear relationships</span>
<span id="cb1-860"><a href="#cb1-860" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Explains why neural networks are considered "universal approximators"</span>
<span id="cb1-861"><a href="#cb1-861" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Despite theoretical capability to represent any function, practical limitations arise from optimization challenges and overfitting</span>
<span id="cb1-862"><a href="#cb1-862" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Indicates neural networks' capacity to achieve zero training error given sufficient resources</span>
<span id="cb1-863"><a href="#cb1-863" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-864"><a href="#cb1-864" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 7</span><span class="co">](lecture7.qmd)</span> (Nonlinearities and Expressive Learning Methods)</span>
<span id="cb1-865"><a href="#cb1-865" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-866"><a href="#cb1-866" aria-hidden="true" tabindex="-1"></a><span class="fu">### Unsupervised Learning</span></span>
<span id="cb1-867"><a href="#cb1-867" aria-hidden="true" tabindex="-1"></a>A type of machine learning where the algorithm is given data without explicit instructions on what to do with it. The system tries to learn the underlying structure or distribution of the data on its own. Common applications include clustering, dimensionality reduction, and density estimation.</span>
<span id="cb1-868"><a href="#cb1-868" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-869"><a href="#cb1-869" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 2</span><span class="co">](lecture2.qmd)</span> (Machine Learning Fundamentals)</span>
<span id="cb1-870"><a href="#cb1-870" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-871"><a href="#cb1-871" aria-hidden="true" tabindex="-1"></a><span class="fu">## V</span></span>
<span id="cb1-872"><a href="#cb1-872" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-873"><a href="#cb1-873" aria-hidden="true" tabindex="-1"></a><span class="fu">### Validation Set</span></span>
<span id="cb1-874"><a href="#cb1-874" aria-hidden="true" tabindex="-1"></a>A portion of the data set aside from training data to tune hyperparameters and evaluate model performance during development. It helps detect overfitting and guides model selection without contaminating the test set.</span>
<span id="cb1-875"><a href="#cb1-875" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-876"><a href="#cb1-876" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 3</span><span class="co">](lecture3.qmd)</span> (Generalization Error)</span>
<span id="cb1-877"><a href="#cb1-877" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-878"><a href="#cb1-878" aria-hidden="true" tabindex="-1"></a><span class="fu">### Variational Autoencoders (VAEs)</span></span>
<span id="cb1-879"><a href="#cb1-879" aria-hidden="true" tabindex="-1"></a>Probabilistic generative models that learn a latent space representation of data using an encoder-decoder architecture with a variational inference approach. VAEs:</span>
<span id="cb1-880"><a href="#cb1-880" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-881"><a href="#cb1-881" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Encode inputs to parameters of probability distributions in latent space (typically mean and variance of Gaussian distributions)</span>
<span id="cb1-882"><a href="#cb1-882" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Regularize the latent space by forcing it towards a standard normal distribution using KL divergence</span>
<span id="cb1-883"><a href="#cb1-883" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Sample from the latent distribution using the reparameterization trick</span>
<span id="cb1-884"><a href="#cb1-884" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Decode the sampled point to reconstruct the input</span>
<span id="cb1-885"><a href="#cb1-885" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Enable both reconstruction and generation of new data points</span>
<span id="cb1-886"><a href="#cb1-886" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-887"><a href="#cb1-887" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 23</span><span class="co">](lecture23.qmd)</span> (Variational Autoencoders)</span>
<span id="cb1-888"><a href="#cb1-888" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-889"><a href="#cb1-889" aria-hidden="true" tabindex="-1"></a><span class="fu">### Variational Bayesian Inference</span></span>
<span id="cb1-890"><a href="#cb1-890" aria-hidden="true" tabindex="-1"></a>An approximate Bayesian inference method that transforms posterior computation into an optimization problem by:</span>
<span id="cb1-891"><a href="#cb1-891" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-892"><a href="#cb1-892" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Introducing a simpler parametric family of distributions $Q(\boldsymbol{\theta}|\boldsymbol{\phi})$ to approximate the true posterior $P(\boldsymbol{\theta}|\mathcal{D})$</span>
<span id="cb1-893"><a href="#cb1-893" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Finding the parameters $\boldsymbol{\phi}$ that minimize the KL divergence between the approximation and the true posterior</span>
<span id="cb1-894"><a href="#cb1-894" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Optimizing the Evidence Lower Bound (ELBO): $\mathcal{L}(\boldsymbol{\phi}) = \mathbb{E}_Q[\log P(\mathcal{D}|\boldsymbol{\theta})] - D_{KL}(Q(\boldsymbol{\theta}|\boldsymbol{\phi})||P(\boldsymbol{\theta}))$</span>
<span id="cb1-895"><a href="#cb1-895" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-896"><a href="#cb1-896" aria-hidden="true" tabindex="-1"></a>Key advantages include:</span>
<span id="cb1-897"><a href="#cb1-897" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-898"><a href="#cb1-898" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Scalability to large datasets through stochastic optimization</span>
<span id="cb1-899"><a href="#cb1-899" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Applicability to models with non-conjugate priors</span>
<span id="cb1-900"><a href="#cb1-900" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Ability to handle complex posterior distributions</span>
<span id="cb1-901"><a href="#cb1-901" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Amortized inference in the case of variational autoencoders</span>
<span id="cb1-902"><a href="#cb1-902" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-903"><a href="#cb1-903" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 22</span><span class="co">](lecture22.qmd)</span> (Bayesian Machine Learning), <span class="co">[</span><span class="ot">Lecture 23</span><span class="co">](lecture23.qmd)</span> (Variational Autoencoders)</span>
<span id="cb1-904"><a href="#cb1-904" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-905"><a href="#cb1-905" aria-hidden="true" tabindex="-1"></a><span class="fu">### Vanishing/Exploding Gradients</span></span>
<span id="cb1-906"><a href="#cb1-906" aria-hidden="true" tabindex="-1"></a>Challenges in training deep neural networks where gradients become extremely small (vanishing) or large (exploding) during backpropagation through many layers. These problems:</span>
<span id="cb1-907"><a href="#cb1-907" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-908"><a href="#cb1-908" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Make it difficult to learn long-range dependencies</span>
<span id="cb1-909"><a href="#cb1-909" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Cause unstable or stalled training</span>
<span id="cb1-910"><a href="#cb1-910" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Can be addressed through techniques like careful weight initialization, gradient clipping, residual connections, batch normalization, and specialized architectures like LSTMs and GRUs</span>
<span id="cb1-911"><a href="#cb1-911" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-912"><a href="#cb1-912" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 10</span><span class="co">](lecture10.qmd)</span> (Backpropagation and Gradient Problems), <span class="co">[</span><span class="ot">Lecture 11</span><span class="co">](lecture11.qmd)</span> (Vanishing Gradients and Generalization)</span>
<span id="cb1-913"><a href="#cb1-913" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-914"><a href="#cb1-914" aria-hidden="true" tabindex="-1"></a><span class="fu">### Vision Transformer (ViT)</span></span>
<span id="cb1-915"><a href="#cb1-915" aria-hidden="true" tabindex="-1"></a>An adaptation of the transformer architecture for computer vision tasks that treats an image as a sequence of patches, using self-attention to process relationships between all patches simultaneously. ViTs have achieved state-of-the-art performance on image classification and other vision tasks.</span>
<span id="cb1-916"><a href="#cb1-916" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-917"><a href="#cb1-917" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 19</span><span class="co">](lecture19.qmd)</span> (Representation Learning, Vision Transformers, and Autoregressive Generation)</span>
<span id="cb1-918"><a href="#cb1-918" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-919"><a href="#cb1-919" aria-hidden="true" tabindex="-1"></a><span class="fu">## W</span></span>
<span id="cb1-920"><a href="#cb1-920" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-921"><a href="#cb1-921" aria-hidden="true" tabindex="-1"></a><span class="fu">### Wasserstein GAN (WGAN)</span></span>
<span id="cb1-922"><a href="#cb1-922" aria-hidden="true" tabindex="-1"></a>A variant of Generative Adversarial Networks that uses the Wasserstein distance (Earth Mover's distance) instead of Jensen-Shannon divergence to measure the difference between the generated and real data distributions. WGANs offer more stable training and better quality gradients.</span>
<span id="cb1-923"><a href="#cb1-923" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-924"><a href="#cb1-924" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 25</span><span class="co">](lecture25.qmd)</span> (Generative Adversarial Networks)</span>
<span id="cb1-925"><a href="#cb1-925" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-926"><a href="#cb1-926" aria-hidden="true" tabindex="-1"></a><span class="fu">### Weight Initialization</span></span>
<span id="cb1-927"><a href="#cb1-927" aria-hidden="true" tabindex="-1"></a>Strategies for setting the initial values of neural network weights before training begins. Proper initialization helps prevent vanishing/exploding gradients and accelerates convergence. Common methods include Xavier/Glorot initialization and He initialization.</span>
<span id="cb1-928"><a href="#cb1-928" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-929"><a href="#cb1-929" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 10</span><span class="co">](lecture10.qmd)</span> (Backpropagation and Gradient Problems), <span class="co">[</span><span class="ot">Lecture 13</span><span class="co">](lecture13.qmd)</span> (Advanced CNN Architectures and Transfer Learning)</span>
<span id="cb1-930"><a href="#cb1-930" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-931"><a href="#cb1-931" aria-hidden="true" tabindex="-1"></a><span class="fu">## Z</span></span>
<span id="cb1-932"><a href="#cb1-932" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-933"><a href="#cb1-933" aria-hidden="true" tabindex="-1"></a><span class="fu">### Zero-shot Learning</span></span>
<span id="cb1-934"><a href="#cb1-934" aria-hidden="true" tabindex="-1"></a>A machine learning paradigm where a model can recognize objects or perform tasks it hasn't encountered during training. The model learns to generalize from seen classes to unseen classes by leveraging additional information like semantic attributes or embeddings.</span>
<span id="cb1-935"><a href="#cb1-935" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-936"><a href="#cb1-936" aria-hidden="true" tabindex="-1"></a>**Related lectures:** <span class="co">[</span><span class="ot">Lecture 19</span><span class="co">](lecture19.qmd)</span> (Representation Learning, Vision Transformers, and Autoregressive Generation)</span></code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>