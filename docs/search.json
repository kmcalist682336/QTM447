[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Course Syllabus",
    "section": "",
    "text": "OverviewSubmission GuidelinesLate Policy\n\n\n\n6-7 problem sets throughout the semester\nCan be completed in groups of up to 3 students\nEach problem set includes:\n\nTheoretical exercises (derivations and proofs)\nCoding exercises\nReal-world data applications\n\n\n\n\n\nUse Quarto or Jupyter notebooks for submissions\nSubmit both source file (.qmd or .ipynb) and rendered HTML/PDF\nInclude all required package dependencies\nWrite-ups should be clear and concise\nEach student must submit individually, even when working in groups\n\n\n\n\nOne “freebie” allowed for a 5-day extension on any problem set\nBeyond the freebie: 10% penalty per day late\nWeekends count as a single day\nNo assignments accepted more than 5 days late without prior approval"
  },
  {
    "objectID": "syllabus.html#problem-sets-50",
    "href": "syllabus.html#problem-sets-50",
    "title": "Course Syllabus",
    "section": "",
    "text": "OverviewSubmission GuidelinesLate Policy\n\n\n\n6-7 problem sets throughout the semester\nCan be completed in groups of up to 3 students\nEach problem set includes:\n\nTheoretical exercises (derivations and proofs)\nCoding exercises\nReal-world data applications\n\n\n\n\n\nUse Quarto or Jupyter notebooks for submissions\nSubmit both source file (.qmd or .ipynb) and rendered HTML/PDF\nInclude all required package dependencies\nWrite-ups should be clear and concise\nEach student must submit individually, even when working in groups\n\n\n\n\nOne “freebie” allowed for a 5-day extension on any problem set\nBeyond the freebie: 10% penalty per day late\nWeekends count as a single day\nNo assignments accepted more than 5 days late without prior approval"
  },
  {
    "objectID": "syllabus.html#final-project-50",
    "href": "syllabus.html#final-project-50",
    "title": "Course Syllabus",
    "section": "Final Project (50%)",
    "text": "Final Project (50%)\n\nOverviewProject Proposal (5%)Project Poster (20%)Final Deliverable (25%)Guidelines\n\n\nA significant application or methodological extension of course topics, divided into three deliverables:\n\n\n\nComponent\nWeight\nDue Date\n\n\n\n\nProject Proposal\n5%\nMarch 25, 2025\n\n\nProject Poster\n20%\nApril 25, 2025\n\n\nFinal Deliverable\n25%\nMay 7, 2025\n\n\n\n\n\n\nDue: March 25th, 12:00 PM\nOne slide presentation\n90-second pitch\nSubmit proposal slide to Canvas\n\n\n\n\nPresent at QTM DataBlitz on April 25th, 2025\nDemonstrate substantial progress\nPoster printing instructions will be provided in April\n\n\n\n\nDue: May 7th, 2025, 11:59 PM EST\nScientific paper format (max 15 pages)\nInclude figures and tables\nSubmit reproducible code repository\nCan be submitted as .zip or GitHub repository\n\n\n\n\nWork in groups of up to 3 students\nChoose a novel dataset or methodological extension\nAvoid basic comparisons on standard datasets (e.g., MNIST)\nConsult with instructor during office hours after spring break\nFor generative approaches, reading ahead may be necessary"
  },
  {
    "objectID": "syllabus.html#grading-scale",
    "href": "syllabus.html#grading-scale",
    "title": "Course Syllabus",
    "section": "Grading Scale",
    "text": "Grading Scale\n\nScaleNo Curve PolicyGrade Appeals\n\n\n\n\n\nGrade\nRange\n\n\n\n\nA\n93-100%\n\n\nA-\n90-92%\n\n\nB+\n87-89%\n\n\nB\n83-86%\n\n\nB-\n80-82%\n\n\nC+ and below\n&lt; 80%\n\n\n\n\n\nThere is no curve for final course grades. If everyone scores high enough for an A, everyone will receive an A. There is no competition for grades.\n\n\nIf you believe your grade on any assignment is incorrect:\n\nWait 48 hours before submitting an appeal (unless it’s a simple addition error)\nSubmit concerns in writing\nFully summarize the problems and justification for the appeal"
  },
  {
    "objectID": "syllabus.html#academic-resources",
    "href": "syllabus.html#academic-resources",
    "title": "Course Syllabus",
    "section": "Academic Resources",
    "text": "Academic Resources\n\n\n Required Texts\nAll main textbooks are available for free:\n\nMurphy, K. P. (2022). Probabilistic Machine Learning: An Introduction\nMurphy, K. P. (2023). Probabilistic Machine Learning: Advanced topics\nGoodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning\nFriedman, J., Hastie, T., & Tibshirani, R. (2017). The Elements of Statistical Learning, 2nd Edition\nPrince, S. J. D. (2023). Understanding Deep Learning\n\nAdditional resources will be posted on Canvas.\n\n\n Computing Resources\n\nGoogle Colab will be available for computationally intensive assignments\nInstructions for Colab usage will be provided when needed\nStudents are encouraged to read package documentation thoroughly\n\nGoogle Colab"
  },
  {
    "objectID": "syllabus.html#course-policies",
    "href": "syllabus.html#course-policies",
    "title": "Course Syllabus",
    "section": "Course Policies",
    "text": "Course Policies\n\nAttendanceLate WorkAcademic IntegrityCommunication\n\n\n\nAttendance is not mandatory but strongly encouraged\nAll lecture materials will be posted online\nStudents are responsible for all material covered in class\n\n\n\n\nSee specific policies for problem sets and project deadlines\nExtensions may be granted for documented emergencies\nPlan ahead for known conflicts\n\n\n\n\nAll work must be your own unless collaboration is explicitly allowed\nCite all sources and inspirations\nViolations will be reported to the Honor Council\nWhen in doubt, ask for clarification\n\n\n\n\nEmail is the preferred method for questions outside of class\nAllow 24-48 hours for email responses\nInclude “QTM 447” in the subject line\nFor code-related questions, please include relevant code snippets"
  },
  {
    "objectID": "syllabus.html#support-resources",
    "href": "syllabus.html#support-resources",
    "title": "Course Syllabus",
    "section": "Support Resources",
    "text": "Support Resources\n\nAcademic SupportTechnical SupportMental Health\n\n\n\nEPASS Tutoring offers peer tutoring services\nEmory Writing Center provides writing assistance\nEmory Office of Accessibility Services for accommodation needs\n\n\n\n\nCanvas support is available 24/7 through the Help button\nFor Google Colab issues, consult the documentation or course TAs\nFor computational resources, the library offers workstations and technical support\n\n\n\n\nEmory Counseling and Psychological Services (CAPS)\nStudent Health Services: 404-727-7551\nTimelyCare provides 24/7 mental health support"
  },
  {
    "objectID": "lectures/summaries/Lecture9Summary.html",
    "href": "lectures/summaries/Lecture9Summary.html",
    "title": "Lecture 9 Summary: Deep Neural Networks",
    "section": "",
    "text": "This summary explores deep neural networks, building on the foundation of single-layer networks to examine how increased depth enables more sophisticated feature learning, hierarchical representations, and improved model performance despite potential training challenges."
  },
  {
    "objectID": "lectures/summaries/Lecture9Summary.html#from-single-layer-to-deep-networks",
    "href": "lectures/summaries/Lecture9Summary.html#from-single-layer-to-deep-networks",
    "title": "Lecture 9 Summary: Deep Neural Networks",
    "section": "1. From Single-Layer to Deep Networks",
    "text": "1. From Single-Layer to Deep Networks\nSingle-layer neural networks can theoretically approximate any function with sufficient hidden units, but may require an impractically large number of parameters. A standard single-layer, fully-connected neural network is defined as:\n\\theta_i = g(\\boldsymbol{\\beta}^T \\varphi(\\mathbf{W}^T \\mathbf{x}_i + \\mathbf{c}) + b)\nWhere:\n\n\\mathbf{W} is a P \\times K weight matrix connecting inputs to hidden units\n\\mathbf{c} is a vector of bias terms for the hidden layer\n\\varphi() is a nonlinear activation function (typically ReLU)\n\\boldsymbol{\\beta} is a vector of weights connecting hidden units to the output\nb is an output bias term\n\nDeep neural networks extend this architecture by incorporating multiple hidden layers, transforming the model to:\n\\theta_i = g(\\boldsymbol{\\beta}^T \\varphi(\\mathbf{W}_2^T \\varphi(\\mathbf{W}_1^T \\mathbf{x}_i + \\mathbf{c}_1) + \\mathbf{c}_2) + b)\nThis nested structure allows for more complex transformations of the input data through successive nonlinear mappings."
  },
  {
    "objectID": "lectures/summaries/Lecture9Summary.html#the-power-of-hierarchical-representations",
    "href": "lectures/summaries/Lecture9Summary.html#the-power-of-hierarchical-representations",
    "title": "Lecture 9 Summary: Deep Neural Networks",
    "section": "2. The Power of Hierarchical Representations",
    "text": "2. The Power of Hierarchical Representations\nDeep neural networks derive their effectiveness from learning hierarchical representations of data, with each layer building upon the features extracted by the previous layer.\n2.1. Feature Learning in Practice\nUsing the MNIST dataset of handwritten digits (focusing on 3s and 8s) demonstrates how neural networks learn meaningful representations:\n\nFirst-layer units learn to identify primitive features like edges, curves, and regions of the digit\nSubsequent layers combine these primitive features into more complex patterns\nThe final layers create abstract representations that enable effective classification\n\nThis hierarchical approach follows a similar process to human visual perception, where complex recognition emerges from the combination of simpler features.\n2.2. Latent Space Transformations\nEach layer of a deep network creates a transformed representation of the data:\n\\mathbf{Z}^{(l)} = \\varphi(\\mathbf{Z}^{(l-1)}\\mathbf{W}^{(l)} + \\mathbf{c}^{(l)})\nWhere \\mathbf{Z}^{(l)} represents the activations at layer l. These transformations progressively map the data into spaces where:\n\nClass separation becomes more apparent\nRelevant features are emphasized\nIrrelevant variations are suppressed\n\nThis can be viewed as a nonlinear generalization of dimensionality reduction techniques like PCA, where each layer creates increasingly abstract representations of the data."
  },
  {
    "objectID": "lectures/summaries/Lecture9Summary.html#advantages-of-depth-over-width",
    "href": "lectures/summaries/Lecture9Summary.html#advantages-of-depth-over-width",
    "title": "Lecture 9 Summary: Deep Neural Networks",
    "section": "3. Advantages of Depth Over Width",
    "text": "3. Advantages of Depth Over Width\nDeep networks with multiple layers offer several advantages over equivalent-sized wide networks with a single hidden layer:\n3.1. Parameter Efficiency\nDeep networks can represent complex functions with fewer parameters than wide networks. For example, representing an XOR-like decision boundary may require:\n\nA single hidden layer with dozens of units\nA deep network with just a few units per layer\n\nThis efficiency stems from the ability to reuse and combine features across layers, allowing complex functions to be decomposed into simpler hierarchical components.\n3.2. Compositional Structure\nDeep networks naturally model compositional relationships, where complex concepts are built from simpler ones. This aligns with the hierarchical structure found in many real-world data types:\n\nIn images: edges → shapes → parts → objects\nIn language: characters → words → phrases → sentences\nIn audio: waveforms → phonemes → words → speech\n\nThis compositional representation provides an inductive bias that helps the network generalize better to unseen examples."
  },
  {
    "objectID": "lectures/summaries/Lecture9Summary.html#training-deep-neural-networks",
    "href": "lectures/summaries/Lecture9Summary.html#training-deep-neural-networks",
    "title": "Lecture 9 Summary: Deep Neural Networks",
    "section": "4. Training Deep Neural Networks",
    "text": "4. Training Deep Neural Networks\nWhile deep networks offer greater representational power, they also introduce training challenges that require specialized techniques.\n4.1. Loss Landscape Complexity\nThe loss landscapes of deep neural networks contain:\n\nMultiple local minima of varying quality\nFlat minima that may generalize better than sharp minima\nSaddle points that can slow optimization\nPlateaus where gradients provide little guidance\n\nStochastic gradient descent (SGD) with appropriate learning rate schedules often converges to flat minima, which can lead to better generalization performance compared to exact optimization methods.\n4.2. Gradient Challenges\nDeep networks face several gradient-related challenges:\n\nVanishing gradients: When gradients become extremely small in early layers\nExploding gradients: When gradients become extremely large\nDiscontinuities from ReLU activations\nHigh-dimensional Jacobian matrices that are computationally intensive\n\nThese issues are addressed through techniques like careful initialization, gradient clipping, batch normalization, and alternative activation functions."
  },
  {
    "objectID": "lectures/summaries/Lecture9Summary.html#geometric-interpretation-of-deep-networks",
    "href": "lectures/summaries/Lecture9Summary.html#geometric-interpretation-of-deep-networks",
    "title": "Lecture 9 Summary: Deep Neural Networks",
    "section": "5. Geometric Interpretation of Deep Networks",
    "text": "5. Geometric Interpretation of Deep Networks\nDeep networks create a sequence of nonlinear transformations that progressively reshape the input space:\n5.1. Visualization of Hidden Representations\nBy visualizing the activations of hidden layers, we can observe how the network:\n\nClusters similar examples together\nSeparates different classes\nCreates increasingly linear decision boundaries in the transformed spaces\n\nFor the MNIST digits 3 and 8, early layers learn to identify distinctive features such as the empty space on the left side of a 3 or the double circle structure of an 8.\n5.2. Decision Boundaries\nThrough successive nonlinear transformations, deep networks create complex decision boundaries that would be difficult to achieve with simpler models:\n\nEarly layers fold and warp the input space\nMiddle layers further transform the data to enhance separability\nLater layers perform the final classification in the transformed space\n\nThis process allows deep networks to learn complex patterns while maintaining computational efficiency."
  },
  {
    "objectID": "lectures/summaries/Lecture9Summary.html#the-role-of-optimization-in-generalization",
    "href": "lectures/summaries/Lecture9Summary.html#the-role-of-optimization-in-generalization",
    "title": "Lecture 9 Summary: Deep Neural Networks",
    "section": "6. The Role of Optimization in Generalization",
    "text": "6. The Role of Optimization in Generalization\nThe optimization process itself plays a critical role in how well deep networks generalize:\n6.1. Implicit Regularization of SGD\nStochastic gradient descent provides an implicit form of regularization:\n\nThe noise in gradient estimates helps escape sharp minima\nSGD tends to converge to flatter regions of the loss landscape\nThese flat minima often correspond to simpler models that generalize better\n\nThis phenomenon helps explain why deep networks can generalize well despite being highly overparameterized.\n6.2. The Backpropagation Algorithm\nTraining deep networks efficiently requires the backpropagation algorithm, which:\n\nApplies the chain rule to compute gradients across multiple layers\nReuses intermediate computations to avoid redundant calculations\nAllows for end-to-end training of all network parameters\n\nDespite its effectiveness, backpropagation still faces challenges with very deep networks, necessitating careful initialization and normalization techniques."
  },
  {
    "objectID": "lectures/summaries/Lecture7Summary.html",
    "href": "lectures/summaries/Lecture7Summary.html",
    "title": "Lecture 7 Summary: Nonlinearities and Expressive Learning Methods",
    "section": "",
    "text": "This summary explores the concept of universal approximators and various techniques for modeling complex, nonlinear relationships in machine learning, highlighting their theoretical capabilities, practical limitations, and computational efficiency."
  },
  {
    "objectID": "lectures/summaries/Lecture7Summary.html#the-need-for-expressive-models",
    "href": "lectures/summaries/Lecture7Summary.html#the-need-for-expressive-models",
    "title": "Lecture 7 Summary: Nonlinearities and Expressive Learning Methods",
    "section": "1. The Need for Expressive Models",
    "text": "1. The Need for Expressive Models\nIn the machine learning framework, the goal is to learn a function that maps inputs to outputs:\ny = f(\\mathbf{x}) + \\epsilon\nWhile we aim to minimize generalization error, this depends on three components:\n\nBias: How far the proposed function deviates from the truth on average\nIrreducible Error: Variation unexplainable by available features\nComplexity: Model capacity that can lead to overfitting\n\nAs training data grows, complexity concerns diminish, but bias persists. For complex problems with nonlinear decision boundaries, standard linear models cannot capture the true functional form, resulting in persistent bias regardless of data size."
  },
  {
    "objectID": "lectures/summaries/Lecture7Summary.html#universal-approximators",
    "href": "lectures/summaries/Lecture7Summary.html#universal-approximators",
    "title": "Lecture 7 Summary: Nonlinearities and Expressive Learning Methods",
    "section": "2. Universal Approximators",
    "text": "2. Universal Approximators\nUniversal approximators are learning methods that can theoretically approximate any Borel measurable function from one finite-dimensional space to another with arbitrary precision. In practical terms, these methods can learn any reasonable continuous function given sufficient resources.\n2.1. Theoretical vs. Practical Capability\nWhile universal approximators can theoretically represent any function, two factors limit practical success:\n\nOptimization challenges: Non-convex loss landscapes with local minima\nOverfitting: The training algorithm may learn noise rather than the true function\n\nWhen evaluating methods as universal approximators, a key indicator is their theoretical ability to achieve zero training error for any dataset—though this capability alone doesn’t guarantee good generalization."
  },
  {
    "objectID": "lectures/summaries/Lecture7Summary.html#k-nearest-neighbors-knn",
    "href": "lectures/summaries/Lecture7Summary.html#k-nearest-neighbors-knn",
    "title": "Lecture 7 Summary: Nonlinearities and Expressive Learning Methods",
    "section": "3. K-Nearest Neighbors (KNN)",
    "text": "3. K-Nearest Neighbors (KNN)\nKNN can represent arbitrarily complex functions as training data grows, but suffers severely from the curse of dimensionality.\n3.1. Characteristics and Limitations\n\nWith 1-NN, the method partitions the feature space into Voronoi cells\nTo effectively cover a space with P dimensions, approximately 10^P training points are needed\nThis exponential growth in parameter requirements creates generalization challenges\nThe generalization error grows according to: E_\\mathcal{T}[\\mathcal{L}(y - \\hat{y})] = \\frac{1}{N} \\sum_{i=1}^N \\mathcal{L}(y_i - \\hat{y}_i) + \\mathcal{O} \\left(\\frac{P}{N} \\right)\nAs dimensionality increases, an exponentially larger training set is needed to maintain generalization performance"
  },
  {
    "objectID": "lectures/summaries/Lecture7Summary.html#polynomial-expansions-and-feature-transformations",
    "href": "lectures/summaries/Lecture7Summary.html#polynomial-expansions-and-feature-transformations",
    "title": "Lecture 7 Summary: Nonlinearities and Expressive Learning Methods",
    "section": "4. Polynomial Expansions and Feature Transformations",
    "text": "4. Polynomial Expansions and Feature Transformations\nAdding polynomial terms to linear models creates more expressive models capable of capturing nonlinear relationships.\n4.1. Explicit Polynomial Features\n\nA full polynomial expansion of degree d with P features results in \\binom{P+d}{d} \\approx \\frac{P^d}{d!} terms\nThis approach suffers from:\n\nExponential parameter growth with increasing degree or dimensionality\nDegraded generalization with generalization gap scaling as \\mathcal{O} \\left( \\frac{P^d}{Nd!} \\right)\nComputational complexity for training scaling as \\mathcal{O} \\left(\\frac{NP^{2d}}{d!} \\right)\n\n\n4.2. Kernel Methods\nKernel methods implicitly map data to higher-dimensional spaces without explicitly computing the transformation:\n\nThe Radial Basis Function (RBF) kernel: K(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp(-\\gamma \\|\\mathbf{x}_i - \\mathbf{x}_j\\|^2)\nThe RBF kernel is a universal approximator where the \\gamma parameter controls flexibility\nWhile powerful for complex relationships, RBF kernels face challenges:\n\nPotential for poor generalization without proper regularization\nComputational scaling issues as training size grows (\\mathcal{O}(N^3) for matrix inversion)\nMemory requirements for storing the N \\times N kernel matrix"
  },
  {
    "objectID": "lectures/summaries/Lecture7Summary.html#local-approximation-methods",
    "href": "lectures/summaries/Lecture7Summary.html#local-approximation-methods",
    "title": "Lecture 7 Summary: Nonlinearities and Expressive Learning Methods",
    "section": "5. Local Approximation Methods",
    "text": "5. Local Approximation Methods\nThese methods divide the feature space into regions and fit local polynomial models within each region.\n5.1. Regression Splines\n\nSplit the feature space into K regions with division points \\boldsymbol{\\xi}\nFit local polynomials within each region\nEnsure continuity at region boundaries\nEffective for low-dimensional problems (P &lt; 5), but suffer from the curse of dimensionality as feature dimensions increase\n\n5.2. Tree-Based Methods\nDecision trees partition the feature space through recursive binary splitting:\n\nSingle decision trees can theoretically approximate any function when grown to full depth\nHowever, they suffer from poor generalization due to overfitting\nTraining complexity for a single tree to full depth is \\mathcal{O}(P N \\log N)\n\n5.3. Random Forests\nRandom Forests address the generalization issues of single trees:\n\nLimit feature consideration at each split (random feature subset of size P')\nGrow multiple trees to full depth\nAverage predictions through bagging\nComputational complexity scales as \\mathcal{O}(B P' N \\log N) where B is the number of trees\nEffectively combat overfitting while maintaining universal approximation properties\nParticularly effective for tabular data but less optimal for complex data types like images, speech, and text"
  },
  {
    "objectID": "lectures/summaries/Lecture7Summary.html#approaching-neural-networks",
    "href": "lectures/summaries/Lecture7Summary.html#approaching-neural-networks",
    "title": "Lecture 7 Summary: Nonlinearities and Expressive Learning Methods",
    "section": "6. Approaching Neural Networks",
    "text": "6. Approaching Neural Networks\nThe limitations of these methods point toward neural networks as another powerful universal approximator. Neural networks address the curse of dimensionality through:\n\nEngineering low-dimensional representations of high-dimensional spaces\nHierarchical feature extraction\nParameter sharing to reduce effective parameter count\nFlexible architectures adaptable to various data types\n\nThese characteristics make neural networks particularly effective for complex data types where other universal approximators struggle with either generalization or computational scaling."
  },
  {
    "objectID": "lectures/summaries/Lecture5Summary.html",
    "href": "lectures/summaries/Lecture5Summary.html",
    "title": "Lecture 5 Summary: Loss Minimization and Optimization",
    "section": "",
    "text": "This summary explores optimization techniques for finding parameters that minimize loss functions in machine learning models, bridging theoretical foundations with practical implementation challenges."
  },
  {
    "objectID": "lectures/summaries/Lecture5Summary.html#the-empirical-risk-minimization-framework",
    "href": "lectures/summaries/Lecture5Summary.html#the-empirical-risk-minimization-framework",
    "title": "Lecture 5 Summary: Loss Minimization and Optimization",
    "section": "1. The Empirical Risk Minimization Framework",
    "text": "1. The Empirical Risk Minimization Framework\nMachine learning problems with parameters can generally be expressed as minimizing empirical risk:\n\\hat{\\boldsymbol{\\beta}} = \\underset{\\boldsymbol{\\beta}}{\\text{argmin}} \\frac{1}{N} \\sum_{i=1}^{N} L(y_i, f(\\boldsymbol{\\beta}, \\mathbf{x}_i)) + \\lambda C(\\boldsymbol{\\beta})\nThis framework encompasses numerous methods including:\n\nLinear regression with MSE loss\nLogistic regression with cross-entropy loss\nSupport vector machines with hinge loss\n\nEach requires finding coefficients that minimize their respective loss functions."
  },
  {
    "objectID": "lectures/summaries/Lecture5Summary.html#mathematical-foundations-of-optimization",
    "href": "lectures/summaries/Lecture5Summary.html#mathematical-foundations-of-optimization",
    "title": "Lecture 5 Summary: Loss Minimization and Optimization",
    "section": "2. Mathematical Foundations of Optimization",
    "text": "2. Mathematical Foundations of Optimization\nFinding a minimum in the parameter space involves understanding:\n2.1. Gradients and Critical Points\nThe gradient vector represents the direction of steepest ascent at any point:\n\\mathbf{g}(\\boldsymbol{\\theta}) = \\nabla \\mathcal{L}(\\boldsymbol{\\theta}) = \\begin{bmatrix}\n\\frac{\\partial \\mathcal{L}(\\boldsymbol{\\theta})}{\\partial \\theta_1} \\\\\n\\frac{\\partial \\mathcal{L}(\\boldsymbol{\\theta})}{\\partial \\theta_2} \\\\\n\\vdots\n\\end{bmatrix}\nA critical point occurs when \\mathbf{g}(\\boldsymbol{\\theta}) = \\mathbf{0}.\n2.2. Hessians and Convexity\nThe Hessian matrix of second derivatives determines whether a critical point is a minimum:\n\\mathbf{H}(\\boldsymbol{\\theta}) = \\nabla^2 \\mathcal{L}(\\boldsymbol{\\theta})\nA function is strictly convex if its Hessian is positive definite everywhere, guaranteeing a unique global minimum.\nFor regularized linear regression, the Hessian: \\mathcal{H}(\\boldsymbol{\\beta}) = \\frac{2}{N}[\\mathbf{X}^T\\mathbf{X} + \\lambda\\mathcal{I}_P] is always positive definite, ensuring a single global minimum.\nFor logistic regression, the Hessian can be expressed as \\mathbf{X}^T\\mathbf{S}\\mathbf{X}, where \\mathbf{S} is a diagonal matrix with positive elements, also ensuring convexity."
  },
  {
    "objectID": "lectures/summaries/Lecture5Summary.html#gradient-descent-methods",
    "href": "lectures/summaries/Lecture5Summary.html#gradient-descent-methods",
    "title": "Lecture 5 Summary: Loss Minimization and Optimization",
    "section": "3. Gradient Descent Methods",
    "text": "3. Gradient Descent Methods\n3.1. Basic Gradient Descent\nThe simplest optimization approach follows the direction of steepest descent:\n\\boldsymbol{\\beta}_{t+1} = \\boldsymbol{\\beta}_t - \\eta \\mathbf{g}(\\boldsymbol{\\beta}_t)\nWhere:\n\n\\eta is the step size (learning rate)\n\\mathbf{g}(\\boldsymbol{\\beta}_t) is the gradient evaluated at the current position\n\nThis approach guarantees convergence to a minimum with a sufficiently small step size, but the trade-off between convergence speed and precision poses practical challenges.\n3.2. Stochastic Gradient Descent (SGD)\nRather than using all N training examples to compute the gradient, SGD uses a small batch B (often just one example):\n\\mathbf{g}(\\boldsymbol{\\beta}; \\mathcal{B}) = \\frac{1}{B} \\sum_{j \\in \\mathcal{B}} \\mathbf{g}(\\boldsymbol{\\beta}; \\mathbf{x}_j, y_j)\nKey advantages:\n\nReduced computational complexity from \\mathcal{O}(NP) to \\mathcal{O}(P) per step\nAvoids wasting time on redundant examples\nAllows quick initial movement away from poor starting points\n\nChallenges:\n\nNoisy gradient estimates lead to a “noise ball” around the minimum\nSensitivity to learning rate selection\nDifficulty determining convergence"
  },
  {
    "objectID": "lectures/summaries/Lecture5Summary.html#improving-sgd-performance",
    "href": "lectures/summaries/Lecture5Summary.html#improving-sgd-performance",
    "title": "Lecture 5 Summary: Loss Minimization and Optimization",
    "section": "4. Improving SGD Performance",
    "text": "4. Improving SGD Performance\n4.1. Learning Rate Schedules\nInstead of a fixed learning rate, polynomial decay schedules reduce step size over time:\n\\eta_t = \\eta_0(bt + 1)^{-a}\nThis approach allows:\n\nLarge initial steps to quickly move away from poor starting positions\nProgressively smaller steps as the algorithm approaches the minimum\nReduced final noise ball size\n\n4.2. Iterate Averaging\nComputing a running average of parameter values during SGD:\n\\bar{\\boldsymbol{\\beta}}_{t} = \\frac{1}{t} \\sum \\boldsymbol{\\beta}_t = \\frac{1}{t} \\boldsymbol{\\beta}_t + \\frac{t-1}{t} \\bar{\\boldsymbol{\\beta}}_{t-1}\nThis technique:\n\nProvides theoretical guarantees for optimal variance\nReduces the impact of random fluctuations around the minimum\nCan work effectively even without learning rate schedules"
  },
  {
    "objectID": "lectures/summaries/Lecture5Summary.html#practical-implementations",
    "href": "lectures/summaries/Lecture5Summary.html#practical-implementations",
    "title": "Lecture 5 Summary: Loss Minimization and Optimization",
    "section": "5. Practical Implementations",
    "text": "5. Practical Implementations\nEmpirical demonstrations with logistic regression reveal:\n\nThe critical impact of step size selection on convergence\nHow SGD with just one example per step can match full gradient descent with proper tuning\nThe effectiveness of combining SGD with learning rate schedules or iterate averaging\nThe challenge of balancing convergence speed with final accuracy\n\nConclusion:\nOptimization methods like SGD make machine learning feasible for large datasets by dramatically reducing computational complexity, particularly for models without closed-form solutions. The trade-offs between computational efficiency, convergence speed, and final accuracy can be managed through techniques like learning rate scheduling and iterate averaging. Modern improvements to SGD, including momentum and adaptive methods, further enhance efficiency by reducing the “random walk” behavior characteristic of basic SGD implementations."
  },
  {
    "objectID": "lectures/summaries/Lecture3Summary.html",
    "href": "lectures/summaries/Lecture3Summary.html",
    "title": "Summary: Lecture 3 - Generalization Error",
    "section": "",
    "text": "This lecture, “Generalization Error,” delves into the crucial concept of how well a model trained on a specific dataset will perform on new, unseen data. It establishes the fundamental principle that generalization error is always greater than or equal to training error."
  },
  {
    "objectID": "lectures/summaries/Lecture3Summary.html#understanding-generalization-error",
    "href": "lectures/summaries/Lecture3Summary.html#understanding-generalization-error",
    "title": "Summary: Lecture 3 - Generalization Error",
    "section": "1. Understanding Generalization Error",
    "text": "1. Understanding Generalization Error\nThe lecture initially breaks down generalization error with a foundational formula: Generalization Error = Training Error + \\frac{2}{N} \\sum Cov(y_i, \\hat{y}_i) This equation highlights that the discrepancy between generalization error and training error widens as the covariance between the actual outcomes (y_i) and the model’s predictions (\\hat{y}_i) increases.\nKey introductory concepts include:\n\nFixed X Assumption: A simplifying assumption in early analysis where training features (X) are considered constant, while outcomes vary due to different noise draws (e.g., y = f(x) + \\epsilon vs. y' = f(x) + \\epsilon').\nPolynomial Regression Example: This serves as an illustration of how increasing model complexity (e.g., the degree of a polynomial) can lead to the model becoming overly “wiggly” and fitting to noise, a phenomenon known as overfitting. The lecture uses Python code and visualizations to demonstrate how polynomials of varying degrees fit noisy data.\n\nA core idea presented is that the prediction at any given point is a weighted combination of the training outcomes: \\hat{y}_0 = \\mathbf{x}_0^T \\hat{\\beta} = \\mathbf{x}_0^T (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}\nFor linear models, the sum of self-covariances is shown to be P\\sigma^2, which results in a training error offset of \\frac{2P\\sigma^2}{N}. This implies that a model with more features (a higher P) will have increased covariance, thereby enlarging the gap between training and generalization error. This effect is mitigated by a larger sample size N.\n\nPrimary Factors Affecting Generalization:\n\nThe magnitude of the training error.\nThe covariance between training outcomes and the model’s predictions.\nThe size of the training sample."
  },
  {
    "objectID": "lectures/summaries/Lecture3Summary.html#bias-variance-decomposition-for-mse",
    "href": "lectures/summaries/Lecture3Summary.html#bias-variance-decomposition-for-mse",
    "title": "Summary: Lecture 3 - Generalization Error",
    "section": "2. Bias-Variance Decomposition (for MSE)",
    "text": "2. Bias-Variance Decomposition (for MSE)\nWhen the fixed X assumption is relaxed, the generalization error under Mean Squared Error (MSE) can be decomposed into three fundamental components: E_\\mathcal{T}[(y - \\hat{y})^2] = V_{y|x}[y] (Irreducible Error) + V_D[\\hat{y}] (Model Variance) + (E_D[\\hat{y}] - f(x))^2 (Bias^2) Let’s examine each term:\n\nBias (Bias^2): Defined as (E_D[\\hat{y}] - f(x))^2. This term quantifies the difference between the average prediction of the model (if trained on many different datasets D) and the true underlying function f(x). Bias tends to decrease as model flexibility increases, but it is bounded. Underfitting is often characterized by high bias.\nModel Variance (V_D[\\hat{y}]): This term measures the variability of the model’s predictions if it were trained on different datasets D. Model variance typically increases with model flexibility (complexity) and decreases with a larger sample size. Overfitting is often characterized by high model variance.\nIrreducible Error (V_{y|x}[y]): This component represents the inherent noise or randomness in the data generating process that no model can eliminate.\n\nPython examples are used to visualize how bias and model variance change for polynomial regression models of different degrees and with varying sample sizes, effectively illustrating the bias-variance tradeoff.\nAn approximation for Generalization Error encapsulates this tradeoff: \\sigma^2 + \\mathcal{O}(\\frac{1}{C}) + \\mathcal{O}(\\frac{C}{N}) Here, C represents model complexity. This formula underscores that if C is too high, model variance can dominate (overfitting), and if C is too low, bias can dominate (underfitting)."
  },
  {
    "objectID": "lectures/summaries/Lecture3Summary.html#generalization-in-classification-01-loss",
    "href": "lectures/summaries/Lecture3Summary.html#generalization-in-classification-01-loss",
    "title": "Summary: Lecture 3 - Generalization Error",
    "section": "3. Generalization in Classification (0/1 Loss)",
    "text": "3. Generalization in Classification (0/1 Loss)\nFor classification tasks, where 0/1 loss is common, the generalization error is the expected probability of misclassification: E_\\mathcal{T}[I(y \\neq \\hat{y})]\n\nVapnik-Chervonenkis (VC) Bound: This provides a theoretical upper bound on the generalization error for classification models: GenError \\le TrainError + \\sqrt{\\frac{1}{N} \\left[ d \\left(\\log \\frac{2N}{d} + 1\\right) - \\log \\frac{\\delta}{4} \\right]} In this bound, d is the VC dimension of the classifier.\nVC Dimension (d): This is a measure of a classifier’s complexity or capacity. It’s defined as the size of the largest set of points that the classifier can “shatter” (i.e., correctly classify for all 2^d possible binary labelings).\n\nFor instance, linear classifiers in P dimensions have a VC dimension of P+1.\nSome complex models, like 1-Nearest Neighbor (1NN) or Support Vector Machines (SVMs) with Radial Basis Function (RBF) kernels, can have an infinite VC dimension, although the VC bound represents a worst-case scenario.\n\nThe training offset term in the VC bound (the part added to Training Error) increases with the VC dimension d and decreases with the sample size N.\nAnalogous to the MSE case, the generalization error for 0/1 loss can also be thought of in terms of a complexity tradeoff: GenError \\approx \\mathcal{O}(\\frac{1}{C}) + \\mathcal{O}(\\frac{C}{N})"
  },
  {
    "objectID": "lectures/summaries/Lecture3Summary.html#practical-measurement-of-generalization-error",
    "href": "lectures/summaries/Lecture3Summary.html#practical-measurement-of-generalization-error",
    "title": "Summary: Lecture 3 - Generalization Error",
    "section": "4. Practical Measurement of Generalization Error",
    "text": "4. Practical Measurement of Generalization Error\nTheoretical bounds are insightful, but practical estimation of generalization error is crucial for model selection and assessment.\n\nHeldout Test Set: This is a portion of data that is never used during the training or tuning phases. It provides an unbiased estimate of how the model will perform on new, unseen data.\nValidation Set: This dataset is used to tune model hyperparameters (e.g., selecting the value of K in K-Nearest Neighbors, or determining the strength of a regularization penalty).\nData Splitting Strategies:\n\nFor large datasets, a common approach is a three-way split: training set, validation set, and test set.\nFor smaller datasets, K-fold cross-validation is often preferred for hyperparameter tuning to make more efficient use of limited data, followed by a final evaluation on a heldout test set.\n\n\nThe lecture strongly emphasizes that models should never be chosen based solely on their performance on the training error, as this is not indicative of real-world performance."
  },
  {
    "objectID": "lectures/summaries/Lecture3Summary.html#conclusion",
    "href": "lectures/summaries/Lecture3Summary.html#conclusion",
    "title": "Summary: Lecture 3 - Generalization Error",
    "section": "Conclusion",
    "text": "Conclusion\nThe lecture wraps up by highlighting that the next topics will focus on specific regression methodologies, including linear and logistic regression, the concept of likelihoods, and the use of regularization techniques to improve generalization. A detailed derivation of the MSE decomposition is also noted as being available at the end of the original lecture slides."
  },
  {
    "objectID": "lectures/summaries/Lecture28Summary.html",
    "href": "lectures/summaries/Lecture28Summary.html",
    "title": "Lecture 28: Ethics in AI and Course Conclusion",
    "section": "",
    "text": "Increasing accessibility of AI systems (e.g., ChatGPT) to general public\nGrowing capability to create realistic-looking fake information\nPotential for harmful applications:\n\nMisinformation and synthetic media (deepfakes)\nPersonalized cyberbullying\nIdentity theft and catfishing\nPrivacy violations\n\nNeed for ethical frameworks to guide AI development and deployment\n\n\n\n\n\n\n\nCore principle: Maximize happiness, minimize harm for the greatest number\nApplication to AI: Evaluate based on broad societal outcomes\nStrengths:\n\nPractical and outcome-focused\nConsiders widespread impacts\nClear criterion (maximize welfare)\n\nWeaknesses:\n\nRequires quantification of harm/benefit\nMay sacrifice minority interests\nCan justify harmful means for “positive” ends\n\n\n\n\n\n\nCore principle: Follow universal ethical rules and duties, regardless of outcomes\nApplication to AI: Focus on respecting rights and following ethical constraints\nStrengths:\n\nProvides clear ethical boundaries\nProtects individual rights\nPromotes consistency\n\nWeaknesses:\n\nMay ignore beneficial outcomes\nDifficult to resolve conflicting duties\nCan be inflexible in complex situations\n\n\n\n\n\n\nCore principle: Develop moral character and virtues; act as a morally virtuous person would\nApplication to AI: Design systems that embody positive moral values\nStrengths:\n\nIntegrates moral reasoning beyond rules\nAcknowledges complexity of ethical decisions\nFocuses on intention and character\n\nWeaknesses:\n\nLess concrete guidance\nMore questions than answers\nSubjective interpretations"
  },
  {
    "objectID": "lectures/summaries/Lecture28Summary.html#ethics-in-modern-ai",
    "href": "lectures/summaries/Lecture28Summary.html#ethics-in-modern-ai",
    "title": "Lecture 28: Ethics in AI and Course Conclusion",
    "section": "",
    "text": "Increasing accessibility of AI systems (e.g., ChatGPT) to general public\nGrowing capability to create realistic-looking fake information\nPotential for harmful applications:\n\nMisinformation and synthetic media (deepfakes)\nPersonalized cyberbullying\nIdentity theft and catfishing\nPrivacy violations\n\nNeed for ethical frameworks to guide AI development and deployment\n\n\n\n\n\n\n\nCore principle: Maximize happiness, minimize harm for the greatest number\nApplication to AI: Evaluate based on broad societal outcomes\nStrengths:\n\nPractical and outcome-focused\nConsiders widespread impacts\nClear criterion (maximize welfare)\n\nWeaknesses:\n\nRequires quantification of harm/benefit\nMay sacrifice minority interests\nCan justify harmful means for “positive” ends\n\n\n\n\n\n\nCore principle: Follow universal ethical rules and duties, regardless of outcomes\nApplication to AI: Focus on respecting rights and following ethical constraints\nStrengths:\n\nProvides clear ethical boundaries\nProtects individual rights\nPromotes consistency\n\nWeaknesses:\n\nMay ignore beneficial outcomes\nDifficult to resolve conflicting duties\nCan be inflexible in complex situations\n\n\n\n\n\n\nCore principle: Develop moral character and virtues; act as a morally virtuous person would\nApplication to AI: Design systems that embody positive moral values\nStrengths:\n\nIntegrates moral reasoning beyond rules\nAcknowledges complexity of ethical decisions\nFocuses on intention and character\n\nWeaknesses:\n\nLess concrete guidance\nMore questions than answers\nSubjective interpretations"
  },
  {
    "objectID": "lectures/summaries/Lecture28Summary.html#ethical-pillars-for-ai-development",
    "href": "lectures/summaries/Lecture28Summary.html#ethical-pillars-for-ai-development",
    "title": "Lecture 28: Ethics in AI and Course Conclusion",
    "section": "Ethical Pillars for AI Development",
    "text": "Ethical Pillars for AI Development\n\nFairness\n\nAI should promote equity and treat individuals/groups without discrimination\nChallenges:\n\nAI systems amplify existing societal biases if unchecked\nBiased outcomes erode trust in AI systems\n\nPotential solutions:\n\nDatasheets for datasets (transparency)\nActive bias detection and mitigation strategies\nDiverse training data and development teams\n\n\n\n\nTruthfulness\n\nAI should produce content that accurately represents reality\nSystems should transparently disclose artificial origins\nChallenges:\n\nIncreasing capability creates more convincing falsehoods\nTension between complexity/capability and potential for misuse\nDifficulty differentiating AI-generated from human content\n\n\n\n\nConsent\n\nAI should respect individual autonomy through explicit permission\nChallenges:\n\nDetermining appropriate consent mechanisms\nNavigating existing content (e.g., social media)\nIntellectual property concerns (e.g., training on copyrighted material)\n\nExamples:\n\nGitHub Copilot trained on public repositories containing proprietary code\nAI art generators replicating copyrighted artistic styles\n\n\n\n\nOpen Questions\n\nNo definitive answers to many ethical dilemmas in AI\nResponsibility falls to technically informed experts who understand the tools\nOngoing dialogue required as technology advances"
  },
  {
    "objectID": "lectures/summaries/Lecture28Summary.html#course-review-and-reflection",
    "href": "lectures/summaries/Lecture28Summary.html#course-review-and-reflection",
    "title": "Lecture 28: Ethics in AI and Course Conclusion",
    "section": "Course Review and Reflection",
    "text": "Course Review and Reflection\n\nTopics Covered\n\nMachine Learning Theory\n\nComplexity theory and VC dimensionality\nBias-variance tradeoffs\nBayesian statistics\nRegularization techniques\nUniversal approximation theory\nConvex optimization and stochastic minimization\n\n\n\nNeural Network Fundamentals\n\nMultilayer perceptrons\nDeep network architectures\nActivation functions\nGeneralization methods\n\n\n\nComputer Vision Models\n\nConvolutional neural networks\nResidual networks and batch normalization\nModern CNN architectures\nObject detection with bounding boxes\nSemantic segmentation and U-Nets\n\n\n\nSequence Models\n\nRecurrent neural networks\nLong short-term memory networks\nSequence-to-sequence models\nAttention mechanisms\nSelf-attention and transformers\nBERT and GPT architectures\n\n\n\nGenerative Models\n\nAutoregressive models\nVariational autoencoders\nGenerative adversarial networks\nDiffusion models\n\n\n\n\nCourse Reflections\n\nAmbitious pace covering cutting-edge topics\nFocus on neural network theory alongside applications\nEmphasis on understanding components of complex models\nInsight that advanced models (transformers, stable diffusion) are combinations of simpler building blocks\n\n\n\nFuture Directions\n\nContinued refinement of course materials and approach\nAdjustment to homework structure for better learning outcomes\nTransition from PyTorch Lightning to base PyTorch\nOngoing development based on student feedback"
  },
  {
    "objectID": "lectures/summaries/Lecture26Summary.html",
    "href": "lectures/summaries/Lecture26Summary.html",
    "title": "Lecture 26: Diffusion Models",
    "section": "",
    "text": "Autoregressive models: Direct \\(P(\\mathbf{x})\\) computation, high quality, but slow training/generation, no latent code\nVAEs: Fast generation, rich latent codes, but lower bound optimization, blurry images\nGANs: Fast generation, assumption-free, good image editing, but minimal control, difficult training\n\n\n\n\n\nCombination of VAE and autoregressive model concepts\nSequential process: add noise to images until purely random, then learn reverse mapping\nNew approach to generative modeling with high-quality results"
  },
  {
    "objectID": "lectures/summaries/Lecture26Summary.html#overview-of-generative-models",
    "href": "lectures/summaries/Lecture26Summary.html#overview-of-generative-models",
    "title": "Lecture 26: Diffusion Models",
    "section": "",
    "text": "Autoregressive models: Direct \\(P(\\mathbf{x})\\) computation, high quality, but slow training/generation, no latent code\nVAEs: Fast generation, rich latent codes, but lower bound optimization, blurry images\nGANs: Fast generation, assumption-free, good image editing, but minimal control, difficult training\n\n\n\n\n\nCombination of VAE and autoregressive model concepts\nSequential process: add noise to images until purely random, then learn reverse mapping\nNew approach to generative modeling with high-quality results"
  },
  {
    "objectID": "lectures/summaries/Lecture26Summary.html#diffusion-model-theory",
    "href": "lectures/summaries/Lecture26Summary.html#diffusion-model-theory",
    "title": "Lecture 26: Diffusion Models",
    "section": "Diffusion Model Theory",
    "text": "Diffusion Model Theory\n\nForward Process (Encoder)\n\nStart with image \\(\\mathbf{x} = \\mathbf{z}_0\\)\nDefine sequence of latent variables: \\(\\{\\mathbf{z}_0, \\mathbf{z}_1, ..., \\mathbf{z}_T\\}\\)\nForward diffusion step: \\(\\mathbf{z}_t = \\sqrt{1 - \\beta_t} \\mathbf{z}_{t-1} + \\sqrt{\\beta_t} \\epsilon_t\\)\nConditional distribution: \\(Q(\\mathbf{z}_t | \\mathbf{z}_{t-1}) = \\mathcal{N}(\\mathbf{z}_t | \\sqrt{1 - \\beta_t} \\mathbf{z}_{t-1}, \\beta_t \\mathcal{I})\\)\nMarkovian process: \\(P(\\mathbf{z}_1, \\mathbf{z}_2, ..., \\mathbf{z}_T | \\mathbf{z}_0) = \\prod_{t=1}^T P(\\mathbf{z}_t | \\mathbf{z}_{t-1})\\)\n\n\n\nDiffusion Kernel\n\nDirect computation from \\(\\mathbf{z}_0\\) to any time \\(t\\): \\(P(\\mathbf{z}_t | \\mathbf{z}_0) = \\mathcal{N}\\left(\\mathbf{z}_t | \\sqrt{\\tilde{\\beta}_t} \\mathbf{z}_0, (1 - \\tilde{\\beta}_t) \\mathcal{I}\\right)\\)\nWhere \\(\\tilde{\\beta}_t = \\prod_{s=1}^t (1 - \\beta_s)\\)\nMean approaches zero and variance approaches identity as \\(t \\to T\\)\nAll images converge to \\(\\mathbf{z}_T \\sim \\mathcal{N}(\\mathbf{0}, \\mathcal{I})\\)\n\n\n\nReverse Process (Decoder)\n\nGoal: Learn mapping from noise back to original image\nTarget: \\(P(\\mathbf{z}_{t-1} | \\mathbf{z}_t)\\) or \\(P(\\mathbf{z}_0 | \\mathbf{z}_t)\\)\nChallenge: Marginal distributions \\(P(\\mathbf{z}_t)\\) and \\(P(\\mathbf{z}_{t-1})\\) are intractable\nForward normal assumption doesn’t guarantee reverse normality\n\n\n\nTractable Reverse Conditional\n\nKnown conditional: \\(P(\\mathbf{z}_{t-1} | \\mathbf{z}_t, \\mathbf{z}_0)\\) (conditioned on original input)\nUsing Bayes’ rule and Markov property: \\(P(\\mathbf{z}_{t-1} | \\mathbf{z}_t, \\mathbf{z}_0) \\propto P(\\mathbf{z}_t | \\mathbf{z}_{t-1})P(\\mathbf{z}_{t-1} | \\mathbf{z}_0)\\)\nResult: \\(P(\\mathbf{z}_{t-1} | \\mathbf{z}_t, \\mathbf{z}_0) = \\mathcal{N}\\left(\\mathbf{z}_{t-1} | \\frac{1 - \\tilde{\\beta}_{t-1}}{1 - \\tilde{\\beta}_t} \\sqrt{1 - \\beta_t} \\mathbf{z}_t + \\frac{\\sqrt{\\tilde{\\beta}_{t-1}}\\beta_t}{1 - \\tilde{\\beta}_t} \\mathbf{z}_0, \\frac{\\beta_t (1 - \\tilde{\\beta}_{t-1})}{1 - \\tilde{\\beta}_t} \\mathcal{I}\\right)\\)"
  },
  {
    "objectID": "lectures/summaries/Lecture26Summary.html#training-diffusion-models",
    "href": "lectures/summaries/Lecture26Summary.html#training-diffusion-models",
    "title": "Lecture 26: Diffusion Models",
    "section": "Training Diffusion Models",
    "text": "Training Diffusion Models\n\nApproximation Strategy\n\nApproximate reverse mapping without \\(\\mathbf{z}_0\\): \\(Q(\\mathbf{z}_{t-1} | \\mathbf{z}_t, \\boldsymbol{\\theta}_t) = \\mathcal{N}(\\mathbf{z}_{t-1} | g(\\mathbf{z}_t, \\boldsymbol{\\theta}_t), \\sigma^2_t \\mathcal{I})\\)\nObjective: \\(\\hat{\\boldsymbol{\\theta}}_{1,2,...,T} = \\underset{\\boldsymbol{\\theta}}{\\text{argmax}} \\left[\\sum_{i=1}^N \\log P(\\mathbf{x}_i | \\boldsymbol{\\theta}_{1,2,...,T})\\right]\\)\nIntractable marginal: \\(P(\\mathbf{x} | \\boldsymbol{\\theta}_{1,2,...,T}) = \\int P(\\mathbf{x}, \\mathbf{z}_1, ..., \\mathbf{z}_T | \\boldsymbol{\\theta}_{1,2,...,T}) d\\mathbf{z}_1 ... d\\mathbf{z}_T\\)\n\n\n\nEvidence Lower Bound (ELBO)\n\nApproximate posterior: \\(Q(\\mathbf{z}_{1...T} | \\mathbf{x})\\)\nELBO form: \\(E_Q[\\log P(\\mathbf{x} | \\mathbf{z}_1, \\boldsymbol{\\theta}_1)] - \\sum_{t=2}^T E_Q[D_{KL}[P(\\mathbf{z}_{t-1} | \\mathbf{z}_t, \\mathbf{x}) || Q(\\mathbf{z}_{t-1} | \\mathbf{z}_t, \\boldsymbol{\\theta}_t)]]\\)\n\n\n\nLoss Function\n\nAnalytical loss for KL divergence between two normals: \\(\\sum_{i=1}^N -\\log \\mathcal{N}(\\mathbf{x}_i | g(\\mathbf{z}_{i1}, \\boldsymbol{\\theta}_1), \\sigma^2_1 \\mathcal{I}) + \\sum_{t=2}^T \\frac{1}{2\\sigma^2_t} \\left\\| \\frac{1 - \\tilde{\\beta}_{t-1}}{1 - \\tilde{\\beta}_t} \\sqrt{1 - \\beta_t} \\mathbf{z}_{it} + \\frac{\\sqrt{\\tilde{\\beta}_{t-1}} \\beta_t}{1 - \\tilde{\\beta}_t} \\mathbf{x}_i - g_t[\\mathbf{z}_{it}, \\boldsymbol{\\theta}_t] \\right\\|^2\\)\nFirst term: reconstruction error\nSecond term: distance between target and approximated means"
  },
  {
    "objectID": "lectures/summaries/Lecture26Summary.html#implementation-considerations",
    "href": "lectures/summaries/Lecture26Summary.html#implementation-considerations",
    "title": "Lecture 26: Diffusion Models",
    "section": "Implementation Considerations",
    "text": "Implementation Considerations\n\nArchitecture Challenges\n\nEach diffusion step requires separate parameters \\(\\boldsymbol{\\theta}_t\\)\nTraining different networks for each step is impractical\nNeed image-to-image prediction capability\n\n\n\nUNet with Time Embeddings\n\nSingle UNet architecture for all time steps\nPositional embeddings encode time information\nEach block represents a time step in diffusion process\nSelf-attention across all time points to relate diffusion steps\nTransformer-style architecture with encoder-side attention\n\n\n\nTraining Considerations\n\nLarge \\(T\\), small \\(\\beta\\): More steps with smaller noise increments improve quality\nComputational intensity: Requires significant GPU resources for training\nParallelization: Self-attention enables GPU parallelization\nPractical reparameterization: Often reformulated in terms of noise for easier training"
  },
  {
    "objectID": "lectures/summaries/Lecture26Summary.html#stable-diffusion",
    "href": "lectures/summaries/Lecture26Summary.html#stable-diffusion",
    "title": "Lecture 26: Diffusion Models",
    "section": "Stable Diffusion",
    "text": "Stable Diffusion\n\nOverview\n\nText-to-image generation using diffusion models\nCombines multiple advances in generative and discriminative modeling\nInput: \\(N\\) images with \\(N\\) associated text prompts\nHigh-quality, high-resolution image generation from text\n\n\n\nArchitecture Components\n\nStep 1: VAE Compression\n\nEncode high-resolution image to smaller latent space (e.g., 32×32)\nStructured latent space as smaller image rather than vector\nReduces computational requirements while preserving essential information\nAddresses VAE blurriness through subsequent diffusion refinement\n\n\n\nStep 2: Forward Diffusion\n\nApply diffusion process to compressed latent representation\nHundreds of steps with small noise variance for training\nComputationally less intensive than full-resolution diffusion\n\n\n\nStep 3: Prompt Embeddings\n\nText prompt processing using pre-trained BERT model\n512 or 1024 dimensional embeddings\nEnables conditioning on textual descriptions\n\n\n\nStep 4: Denoising UNet\n\nCross-attention with prompt embeddings at each step\nSelf-attention across all diffusion time steps\nPositional embeddings for time dependencies\nHigh-parameter models: separate UNet for each time transition (resource-intensive)\n\n\n\nStep 5: VAE Decoder\n\nTransform low-resolution output back to high-resolution\nComplete the generation pipeline from text to final image\n\n\n\n\nTraining and Properties\n\nEnd-to-end backpropagation: Entire pipeline trainable as single network\nSuperior image quality: Matches or exceeds GAN performance\nEnhanced control: Text conditioning more flexible than conditional GANs\nResource requirements: Requires substantial GPU resources for training"
  },
  {
    "objectID": "lectures/summaries/Lecture26Summary.html#advantages-and-limitations",
    "href": "lectures/summaries/Lecture26Summary.html#advantages-and-limitations",
    "title": "Lecture 26: Diffusion Models",
    "section": "Advantages and Limitations",
    "text": "Advantages and Limitations\n\nAdvantages\n\nHigh-quality generation: Superior image quality compared to many alternatives\nText conditioning: Natural integration of textual control\nTheoretical foundation: Well-grounded mathematical framework\nFlexibility: Fewer distributional assumptions than some alternatives\n\n\n\nLimitations\n\nComputational cost: Slower than GANs and VAEs\nResource requirements: Training requires extensive computational resources\nGeneration speed: Multiple denoising steps needed for generation\nComplexity: Multi-component architecture increases implementation complexity\n\n\n\nResearch Directions\n\nUnderstanding why transformer-style diffusion outperforms alternatives\nEfficiency improvements for computational requirements\nComparison with self-attention GANs\nApplications beyond image generation"
  },
  {
    "objectID": "lectures/summaries/Lecture24Summary.html",
    "href": "lectures/summaries/Lecture24Summary.html",
    "title": "Lecture 24: VAE Extensions and GANs Introduction",
    "section": "",
    "text": "Modified training objective: \\(\\mathcal{L} = E_Q[\\log P(\\mathbf{x}|\\mathbf{z})] - \\beta \\cdot D_{KL}(Q(\\mathbf{z}|\\mathbf{x}) || P(\\mathbf{z}))\\)\nβ parameter controls trade-off between reconstruction quality and regularization\nβ &gt; 1: Stronger regularization, encourages disentangled representations\nβ &lt; 1: Emphasizes reconstruction, may lead to entangled latent factors\nBenefits: Improved disentanglement of latent factors, more interpretable representations\n\n\n\n\n\nLatent space interpolation: Smooth transitions between encoded images\nAttribute manipulation: Edit specific features by moving in learned latent directions\nArithmetic operations: Combine latent representations to transfer attributes\nProcess:\n\nEncode images to latent space: \\(\\mathbf{z} = \\text{Encoder}(\\mathbf{x})\\)\nPerform operations in latent space\nDecode modified latent vectors: \\(\\mathbf{x}' = \\text{Decoder}(\\mathbf{z}')\\)\n\n\n\n\n\n\nExtend VAEs to incorporate conditioning information \\(\\mathbf{c}\\)\nModified architecture:\n\nEncoder: \\(Q(\\mathbf{z}|\\mathbf{x}, \\mathbf{c})\\)\nDecoder: \\(P(\\mathbf{x}|\\mathbf{z}, \\mathbf{c})\\)\nPrior: \\(P(\\mathbf{z}|\\mathbf{c})\\) or unchanged \\(P(\\mathbf{z})\\)\n\nTraining objective: \\(\\mathcal{L} = E_Q[\\log P(\\mathbf{x}|\\mathbf{z}, \\mathbf{c})] - D_{KL}(Q(\\mathbf{z}|\\mathbf{x}, \\mathbf{c}) || P(\\mathbf{z}|\\mathbf{c}))\\)\nApplications: Class-conditional generation, attribute-specific sampling"
  },
  {
    "objectID": "lectures/summaries/Lecture24Summary.html#vae-extensions",
    "href": "lectures/summaries/Lecture24Summary.html#vae-extensions",
    "title": "Lecture 24: VAE Extensions and GANs Introduction",
    "section": "",
    "text": "Modified training objective: \\(\\mathcal{L} = E_Q[\\log P(\\mathbf{x}|\\mathbf{z})] - \\beta \\cdot D_{KL}(Q(\\mathbf{z}|\\mathbf{x}) || P(\\mathbf{z}))\\)\nβ parameter controls trade-off between reconstruction quality and regularization\nβ &gt; 1: Stronger regularization, encourages disentangled representations\nβ &lt; 1: Emphasizes reconstruction, may lead to entangled latent factors\nBenefits: Improved disentanglement of latent factors, more interpretable representations\n\n\n\n\n\nLatent space interpolation: Smooth transitions between encoded images\nAttribute manipulation: Edit specific features by moving in learned latent directions\nArithmetic operations: Combine latent representations to transfer attributes\nProcess:\n\nEncode images to latent space: \\(\\mathbf{z} = \\text{Encoder}(\\mathbf{x})\\)\nPerform operations in latent space\nDecode modified latent vectors: \\(\\mathbf{x}' = \\text{Decoder}(\\mathbf{z}')\\)\n\n\n\n\n\n\nExtend VAEs to incorporate conditioning information \\(\\mathbf{c}\\)\nModified architecture:\n\nEncoder: \\(Q(\\mathbf{z}|\\mathbf{x}, \\mathbf{c})\\)\nDecoder: \\(P(\\mathbf{x}|\\mathbf{z}, \\mathbf{c})\\)\nPrior: \\(P(\\mathbf{z}|\\mathbf{c})\\) or unchanged \\(P(\\mathbf{z})\\)\n\nTraining objective: \\(\\mathcal{L} = E_Q[\\log P(\\mathbf{x}|\\mathbf{z}, \\mathbf{c})] - D_{KL}(Q(\\mathbf{z}|\\mathbf{x}, \\mathbf{c}) || P(\\mathbf{z}|\\mathbf{c}))\\)\nApplications: Class-conditional generation, attribute-specific sampling"
  },
  {
    "objectID": "lectures/summaries/Lecture24Summary.html#generative-adversarial-networks-gans",
    "href": "lectures/summaries/Lecture24Summary.html#generative-adversarial-networks-gans",
    "title": "Lecture 24: VAE Extensions and GANs Introduction",
    "section": "Generative Adversarial Networks (GANs)",
    "text": "Generative Adversarial Networks (GANs)\n\nMotivation and Overview\n\nAlternative approach to generative modeling that avoids explicit density estimation\nUses adversarial training between two neural networks\nDoes not require inference networks or variational approximations\nFocuses on generating samples that are indistinguishable from real data\n\n\n\nDensity Ratio Perspective\n\nConsider distinguishing between two distributions: \\(P_{data}(\\mathbf{x})\\) and \\(P_{model}(\\mathbf{x})\\)\nOptimal classifier for this task: \\(f^*(\\mathbf{x}) = \\frac{P_{data}(\\mathbf{x})}{P_{data}(\\mathbf{x}) + P_{model}(\\mathbf{x})}\\)\nWhen distributions are identical: \\(P_{data}(\\mathbf{x}) = P_{model}(\\mathbf{x})\\), then \\(f^*(\\mathbf{x}) = 0.5\\)\nKey insight: Train classifier to distinguish real from generated data\n\n\n\nGAN Architecture\n\nGenerator network \\(G(\\mathbf{z}; \\boldsymbol{\\theta}_g)\\): Maps noise \\(\\mathbf{z} \\sim P_z(\\mathbf{z})\\) to data space\nDiscriminator network \\(D(\\mathbf{x}; \\boldsymbol{\\theta}_d)\\): Outputs probability that input is real data\nGenerator objective: Fool discriminator by generating realistic samples\nDiscriminator objective: Correctly classify real vs. generated samples\n\n\n\nMinimax Game Formulation\n\nTwo-player minimax game with value function: \\(\\min_G \\max_D V(D,G) = E_{\\mathbf{x} \\sim P_{data}(\\mathbf{x})}[\\log D(\\mathbf{x})] + E_{\\mathbf{z} \\sim P_z(\\mathbf{z})}[\\log(1 - D(G(\\mathbf{z})))]\\)\nDiscriminator maximization: \\(\\max_D \\{E_{\\mathbf{x} \\sim P_{data}}[\\log D(\\mathbf{x})] + E_{\\mathbf{z} \\sim P_z}[\\log(1 - D(G(\\mathbf{z})))]\\}\\)\nGenerator minimization: \\(\\min_G E_{\\mathbf{z} \\sim P_z}[\\log(1 - D(G(\\mathbf{z})))]\\)\n\n\n\nTraining Algorithm\n\nDiscriminator update: Train to maximize ability to distinguish real from fake\n\nSample mini-batch of real data \\(\\{\\mathbf{x}^{(1)}, ..., \\mathbf{x}^{(m)}\\}\\)\nSample mini-batch of noise \\(\\{\\mathbf{z}^{(1)}, ..., \\mathbf{z}^{(m)}\\}\\)\nUpdate discriminator by ascending gradient: \\(\\nabla_{\\boldsymbol{\\theta}_d} \\frac{1}{m} \\sum_{i=1}^m [\\log D(\\mathbf{x}^{(i)}) + \\log(1 - D(G(\\mathbf{z}^{(i)})))]\\)\n\nGenerator update: Train to minimize discriminator’s ability to detect fakes\n\nSample mini-batch of noise \\(\\{\\mathbf{z}^{(1)}, ..., \\mathbf{z}^{(m)}\\}\\)\nUpdate generator by descending gradient: \\(\\nabla_{\\boldsymbol{\\theta}_g} \\frac{1}{m} \\sum_{i=1}^m \\log(1 - D(G(\\mathbf{z}^{(i)})))\\)\n\n\n\n\nTheoretical Properties\n\nGlobal optimum: When \\(P_g = P_{data}\\), the global minimum is achieved with \\(D^*(\\mathbf{x}) = \\frac{1}{2}\\)\nConvergence: Under ideal conditions, the minimax game converges to Nash equilibrium\nMode collapse: Generator may learn to produce limited variety of samples\nTraining instability: Adversarial training can be difficult to stabilize\n\n\n\nPractical Considerations\n\nAlternative generator objective: \\(\\max_G E_{\\mathbf{z} \\sim P_z}[\\log D(G(\\mathbf{z}))]\\) (instead of minimizing \\(\\log(1-D(G(\\mathbf{z})))\\))\nProvides stronger gradients early in training when discriminator is confident\nCareful balance required between generator and discriminator training\nVarious techniques developed to improve training stability"
  },
  {
    "objectID": "lectures/summaries/Lecture22Summary.html",
    "href": "lectures/summaries/Lecture22Summary.html",
    "title": "Lecture 22: Bayesian Machine Learning",
    "section": "",
    "text": "Treats model parameters \\(\\boldsymbol{\\theta}\\) as random variables with probability distributions\nPosterior distribution defined via Bayes’ theorem: \\(f(\\boldsymbol{\\theta} | \\mathcal{D}) = \\frac{f(\\mathcal{D} | \\boldsymbol{\\theta})f(\\boldsymbol{\\theta})}{\\int f(\\mathcal{D} | \\boldsymbol{\\theta})f(\\boldsymbol{\\theta}) d\\boldsymbol{\\theta}}\\)\nKey components:\n\nLikelihood: \\(f(\\mathcal{D} | \\boldsymbol{\\theta})\\) - probability of data given parameters\nPrior: \\(f(\\boldsymbol{\\theta})\\) - initial beliefs about parameters\nMarginal likelihood: \\(\\int f(\\mathcal{D} | \\boldsymbol{\\theta})f(\\boldsymbol{\\theta}) d\\boldsymbol{\\theta}\\) - model evidence\nPosterior: \\(f(\\boldsymbol{\\theta} | \\mathcal{D})\\) - updated parameter beliefs after observing data\n\n\n\n\n\n\nLikelihood: \\(f(\\mathbf{y} | \\mu, \\sigma^2) = \\prod_{i=1}^N \\mathcal{N}(y_i | \\mu, \\sigma^2)\\)\nPrior: \\(f(\\mu) \\sim \\mathcal{N}(\\mu | \\mu_0, \\sigma^2_0)\\)\nPosterior: \\(f(\\mu | \\mathbf{y}) \\sim \\mathcal{N}(\\mu | \\hat{\\mu}, \\hat{\\sigma}^2)\\)\n\n\\(\\hat{\\sigma}^2 = \\left(\\frac{N}{\\sigma^2} + \\frac{1}{\\sigma^2_0}\\right)^{-1}\\)\n\\(\\hat{\\mu} = \\hat{\\sigma}^2\\left(\\frac{1}{\\sigma^2}\\sum y_i + \\frac{1}{\\sigma^2_0}\\mu_0\\right)\\)\n\nProperties:\n\nWith diffuse prior (\\(\\sigma^2_0 \\to \\infty\\)), posterior mean converges to sample mean\nAs \\(N \\to \\infty\\), prior influence diminishes and posterior approaches MLE"
  },
  {
    "objectID": "lectures/summaries/Lecture22Summary.html#bayesian-framework",
    "href": "lectures/summaries/Lecture22Summary.html#bayesian-framework",
    "title": "Lecture 22: Bayesian Machine Learning",
    "section": "",
    "text": "Treats model parameters \\(\\boldsymbol{\\theta}\\) as random variables with probability distributions\nPosterior distribution defined via Bayes’ theorem: \\(f(\\boldsymbol{\\theta} | \\mathcal{D}) = \\frac{f(\\mathcal{D} | \\boldsymbol{\\theta})f(\\boldsymbol{\\theta})}{\\int f(\\mathcal{D} | \\boldsymbol{\\theta})f(\\boldsymbol{\\theta}) d\\boldsymbol{\\theta}}\\)\nKey components:\n\nLikelihood: \\(f(\\mathcal{D} | \\boldsymbol{\\theta})\\) - probability of data given parameters\nPrior: \\(f(\\boldsymbol{\\theta})\\) - initial beliefs about parameters\nMarginal likelihood: \\(\\int f(\\mathcal{D} | \\boldsymbol{\\theta})f(\\boldsymbol{\\theta}) d\\boldsymbol{\\theta}\\) - model evidence\nPosterior: \\(f(\\boldsymbol{\\theta} | \\mathcal{D})\\) - updated parameter beliefs after observing data\n\n\n\n\n\n\nLikelihood: \\(f(\\mathbf{y} | \\mu, \\sigma^2) = \\prod_{i=1}^N \\mathcal{N}(y_i | \\mu, \\sigma^2)\\)\nPrior: \\(f(\\mu) \\sim \\mathcal{N}(\\mu | \\mu_0, \\sigma^2_0)\\)\nPosterior: \\(f(\\mu | \\mathbf{y}) \\sim \\mathcal{N}(\\mu | \\hat{\\mu}, \\hat{\\sigma}^2)\\)\n\n\\(\\hat{\\sigma}^2 = \\left(\\frac{N}{\\sigma^2} + \\frac{1}{\\sigma^2_0}\\right)^{-1}\\)\n\\(\\hat{\\mu} = \\hat{\\sigma}^2\\left(\\frac{1}{\\sigma^2}\\sum y_i + \\frac{1}{\\sigma^2_0}\\mu_0\\right)\\)\n\nProperties:\n\nWith diffuse prior (\\(\\sigma^2_0 \\to \\infty\\)), posterior mean converges to sample mean\nAs \\(N \\to \\infty\\), prior influence diminishes and posterior approaches MLE"
  },
  {
    "objectID": "lectures/summaries/Lecture22Summary.html#bayesian-linear-regression",
    "href": "lectures/summaries/Lecture22Summary.html#bayesian-linear-regression",
    "title": "Lecture 22: Bayesian Machine Learning",
    "section": "Bayesian Linear Regression",
    "text": "Bayesian Linear Regression\n\nModel Formulation\n\nLikelihood: \\(f(\\mathbf{y} | \\mathbf{X}, \\boldsymbol{\\beta}, \\sigma^2) \\sim \\mathcal{N}_N(\\mathbf{y} | \\mathbf{X}\\boldsymbol{\\beta}, \\sigma^2\\mathcal{I}_N)\\)\nPrior: \\(f(\\boldsymbol{\\beta}) \\sim \\mathcal{N}_P(\\boldsymbol{\\beta} | \\boldsymbol{\\mu}_0, \\boldsymbol{\\Sigma}_0)\\)\nPosterior: \\(f(\\boldsymbol{\\beta} | \\mathbf{X}, \\mathbf{y}, \\sigma^2) \\sim \\mathcal{N}_P(\\boldsymbol{\\beta} | \\hat{\\boldsymbol{\\mu}}, \\hat{\\boldsymbol{\\Sigma}})\\)\n\n\\(\\hat{\\boldsymbol{\\Sigma}} = \\left(\\frac{1}{\\sigma^2}\\mathbf{X}^T\\mathbf{X} + \\boldsymbol{\\Sigma}_0^{-1}\\right)^{-1}\\)\n\\(\\hat{\\boldsymbol{\\mu}} = \\hat{\\boldsymbol{\\Sigma}}\\left(\\frac{1}{\\sigma^2}\\mathbf{X}^T\\mathbf{y} + \\boldsymbol{\\Sigma}^{-1}_0\\boldsymbol{\\mu}_0\\right)\\)\n\n\n\n\nConnections to Regularization\n\nWith \\(\\boldsymbol{\\mu}_0 = \\mathbf{0}\\) and \\(\\boldsymbol{\\Sigma}_0 = \\tau^2\\mathcal{I}_P\\): \\(\\hat{\\boldsymbol{\\mu}} = (\\mathbf{X}^T\\mathbf{X} + \\lambda\\mathcal{I}_P)^{-1}\\mathbf{X}^T\\mathbf{y}\\)\nThis is equivalent to ridge regression with regularization parameter \\(\\lambda = \\frac{\\sigma^2}{\\tau^2}\\)\nL2 regularization (weight decay) is a Bayesian solution to the generalization problem\nBayesian models are inherently regularized\nPrior variance controls model complexity similar to regularization strength\n\n\n\nPosterior Summarization\n\nMaximum a posteriori (MAP) estimate: \\(\\hat{\\boldsymbol{\\theta}} = \\underset{\\boldsymbol{\\theta}}{\\text{argmax}} f(\\boldsymbol{\\theta} | \\mathcal{D})\\)\nFor normal posteriors, the MAP estimate is the posterior mean\nBayesian 95% credible intervals for normal posterior: \\(\\hat{\\boldsymbol{\\mu}}_j \\pm 1.96 \\hat{\\boldsymbol{\\Sigma}}_{j,j}\\)"
  },
  {
    "objectID": "lectures/summaries/Lecture22Summary.html#map-approximations",
    "href": "lectures/summaries/Lecture22Summary.html#map-approximations",
    "title": "Lecture 22: Bayesian Machine Learning",
    "section": "MAP Approximations",
    "text": "MAP Approximations\n\nLaplace Approximation\n\nApproximates posterior with a multivariate normal distribution\nParameters of approximation:\n\nMean: \\(\\hat{\\boldsymbol{\\mu}} = \\underset{\\boldsymbol{\\Theta}}{\\text{argmax}}\\log f(\\boldsymbol{\\Theta} | \\mathbf{X})\\)\nCovariance: \\(\\hat{\\boldsymbol{\\Sigma}} = -\\left[\\frac{\\partial^2 \\log f(\\hat{\\boldsymbol{\\mu}} | \\mathbf{X})}{\\partial \\boldsymbol{\\Theta} \\partial \\boldsymbol{\\Theta}}\\right]^{-1}\\)\n\nBernstein-von Mises theorem: posterior converges to this normal approximation as \\(N \\to \\infty\\)\nMinimizes Kullback-Leibler divergence between true posterior and normal approximation\n\n\n\nNon-Conjugate Priors\n\nLaplace prior example: \\(f(\\boldsymbol{\\beta}) = \\prod_{j=1}^P \\frac{1}{2b_0}\\exp\\left[-\\frac{|\\beta_j|}{b_0}\\right]\\)\nMAP estimation with Laplace prior leads to LASSO regression: \\(\\log f(\\boldsymbol{\\beta}, \\alpha | \\mathbf{X}, \\mathbf{y}, \\sigma^2) \\propto -\\left[\\sum_{i=1}^N (y_i - \\mathbf{x}_i^T\\boldsymbol{\\beta} - \\alpha)^2 + \\frac{2\\sigma^2}{b_0}\\sum_{j=1}^P |\\beta_j|\\right]\\)\nNo closed-form solution for LASSO, requires numerical optimization"
  },
  {
    "objectID": "lectures/summaries/Lecture22Summary.html#marginal-likelihood",
    "href": "lectures/summaries/Lecture22Summary.html#marginal-likelihood",
    "title": "Lecture 22: Bayesian Machine Learning",
    "section": "Marginal Likelihood",
    "text": "Marginal Likelihood\n\nModel Evidence\n\nThe denominator in Bayes’ theorem: \\(f(\\mathcal{D}) = \\int f(\\mathcal{D} | \\boldsymbol{\\theta})f(\\boldsymbol{\\theta}) d\\boldsymbol{\\theta}\\)\nMeasures how well model explains data, integrating over all possible parameter values\nServes as a Bayesian measure of generalization error\nApproximately equivalent to leave-one-out cross-validation\n\n\n\nComputation and Applications\n\nFor linear regression with normal prior: \\(\\int \\mathcal{N}_N(\\mathbf{y} | \\mathbf{X}\\boldsymbol{\\beta}, \\sigma^2\\mathcal{I}_N)\\mathcal{N}_P(\\boldsymbol{\\beta} | \\boldsymbol{\\mu}_0, \\boldsymbol{\\Sigma}_0)d\\boldsymbol{\\beta} = \\mathcal{N}_N(\\mathbf{y} | \\mathbf{X}\\boldsymbol{\\mu}_0, \\sigma^2\\mathcal{I}_N + \\mathbf{X}\\boldsymbol{\\Sigma}_0\\mathbf{X}^T)\\)\nWith Laplace approximation: \\(\\int f(\\mathbf{X} | \\boldsymbol{\\theta})f(\\boldsymbol{\\theta})d\\boldsymbol{\\theta} \\approx f(\\mathbf{y} | \\mathbf{X}, \\hat{\\boldsymbol{\\mu}})f(\\hat{\\boldsymbol{\\mu}})(2\\pi)^{P/2}\\text{det}[\\hat{\\boldsymbol{\\Sigma}}]N^{-P/2}\\)\nUsed for hyperparameter selection, e.g., finding optimal \\(\\lambda\\) in ridge regression"
  },
  {
    "objectID": "lectures/summaries/Lecture22Summary.html#posterior-predictive-distribution",
    "href": "lectures/summaries/Lecture22Summary.html#posterior-predictive-distribution",
    "title": "Lecture 22: Bayesian Machine Learning",
    "section": "Posterior Predictive Distribution",
    "text": "Posterior Predictive Distribution\n\nDefinition and Properties\n\nDistribution of predictions at new points, accounting for parameter uncertainty: \\(f(y_0 | \\mathbf{x}_0, \\mathbf{X}, \\mathbf{y}, \\hat{\\boldsymbol{\\theta}}) = \\int f(\\mathbf{y}_0 | \\mathbf{x}_0, \\mathbf{y}, \\mathbf{X}, \\hat{\\boldsymbol{\\theta}})f(\\hat{\\boldsymbol{\\theta}} | \\mathcal{D})d\\hat{\\boldsymbol{\\theta}}\\)\nFor linear regression with normal prior: \\(f(y_0 | \\mathbf{x}_0, \\sigma^2, \\mathbf{X}, \\mathbf{y}) = \\mathcal{N}(y_0 | \\hat{\\boldsymbol{\\mu}}^T\\mathbf{x}_0, \\sigma^2 + \\mathbf{x}_0^T\\hat{\\boldsymbol{\\Sigma}}\\mathbf{x}_0)\\)\nPrediction variance includes both inherent noise (\\(\\sigma^2\\)) and parameter uncertainty (\\(\\mathbf{x}_0^T\\hat{\\boldsymbol{\\Sigma}}\\mathbf{x}_0\\))"
  },
  {
    "objectID": "lectures/summaries/Lecture20Summary.html",
    "href": "lectures/summaries/Lecture20Summary.html",
    "title": "Lecture 20: Generative Models: Autoregressives and Autoencoders",
    "section": "",
    "text": "Generative modeling represents a fundamental shift from the predictive focus of supervised learning to modeling the intrinsic structure of data. While supervised learning aims to model \\(P(y|\\mathbf{x})\\), generative approaches target the joint distribution \\(P(\\mathbf{x})\\).\n\n\nThe core distinction between these paradigms lies in their objectives:\n\nDiscriminative Models learn decision boundaries between classes\nGenerative Models learn the underlying data distribution\n\nThis distinction is crucial because modeling the full data distribution enables:\n\nSampling new instances that resemble training data\nEstimating the likelihood of observed data\nUncovering latent structure within data\nPerforming conditional generation given constraints\n\n\n\n\nGenerative modeling faces significant challenges, particularly for high-dimensional data:\n\nDimensionality: A 6000×4000 RGB image has approximately 72 million dimensions\nComplex Dependencies: Pixels/tokens exhibit intricate spatial and contextual relationships\nStructure Preservation: Generated outputs must maintain semantic coherence\nEvaluation: Success requires both perceptual quality and statistical fidelity\n\nDespite these challenges, the success of modern generative models demonstrates that capturing the statistical structure of complex data is possible, enabling the creation of realistic synthetic content across modalities."
  },
  {
    "objectID": "lectures/summaries/Lecture20Summary.html#foundations-of-generative-modeling",
    "href": "lectures/summaries/Lecture20Summary.html#foundations-of-generative-modeling",
    "title": "Lecture 20: Generative Models: Autoregressives and Autoencoders",
    "section": "",
    "text": "Generative modeling represents a fundamental shift from the predictive focus of supervised learning to modeling the intrinsic structure of data. While supervised learning aims to model \\(P(y|\\mathbf{x})\\), generative approaches target the joint distribution \\(P(\\mathbf{x})\\).\n\n\nThe core distinction between these paradigms lies in their objectives:\n\nDiscriminative Models learn decision boundaries between classes\nGenerative Models learn the underlying data distribution\n\nThis distinction is crucial because modeling the full data distribution enables:\n\nSampling new instances that resemble training data\nEstimating the likelihood of observed data\nUncovering latent structure within data\nPerforming conditional generation given constraints\n\n\n\n\nGenerative modeling faces significant challenges, particularly for high-dimensional data:\n\nDimensionality: A 6000×4000 RGB image has approximately 72 million dimensions\nComplex Dependencies: Pixels/tokens exhibit intricate spatial and contextual relationships\nStructure Preservation: Generated outputs must maintain semantic coherence\nEvaluation: Success requires both perceptual quality and statistical fidelity\n\nDespite these challenges, the success of modern generative models demonstrates that capturing the statistical structure of complex data is possible, enabling the creation of realistic synthetic content across modalities."
  },
  {
    "objectID": "lectures/summaries/Lecture20Summary.html#autoregressive-models",
    "href": "lectures/summaries/Lecture20Summary.html#autoregressive-models",
    "title": "Lecture 20: Generative Models: Autoregressives and Autoencoders",
    "section": "Autoregressive Models",
    "text": "Autoregressive Models\nAutoregressive models provide a tractable approach to generative modeling by factorizing the joint distribution using the chain rule of probability.\n\nChain Rule Factorization\nFor a sequence \\(\\mathbf{x} = [x_1, x_2, ..., x_P]\\), the joint distribution can be expressed as:\n\\[P(\\mathbf{x}) = P(x_1) \\cdot P(x_2|x_1) \\cdot P(x_3|x_1,x_2) \\cdot ... \\cdot P(x_P|x_1,...,x_{P-1})\\]\nThis factorization transforms the complex task of modeling a high-dimensional joint distribution into a sequence of more manageable conditional distributions.\n\n\nText Generation with GPT\nGenerative Pre-trained Transformers (GPT) implement autoregressive modeling for text:\n\nUse masked self-attention to enforce the autoregressive constraint\nProcess the input sequence with transformer decoder blocks\nOutput a probability distribution over the next token\nSample from this distribution and append to the sequence\nRepeat until generation is complete\n\nGPT’s success in language generation comes from its ability to capture complex patterns in linguistic data through this autoregressive approach.\n\n\nImage Generation with PixelRNN/PixelCNN\nAutoregressive image generation follows similar principles but must define a spatial ordering for pixels:\n\nTypically process pixels in raster scan order (row by row from top-left)\nModel RGB values at each position conditioned on all previously generated pixels\nUse recurrent architectures (PixelRNN) or masked convolutions (PixelCNN) to preserve the autoregressive property\n\nThe hidden state update for PixelRNN can be expressed as:\n\\[\\mathbf{h}_{x,y} = f(\\mathbf{h}_{x-1,y}, \\mathbf{h}_{x,y-1})\\]\nWhere the function \\(f\\) is implemented using LSTM units to capture spatial dependencies.\n\n\nLimitations of Autoregressive Generation\nDespite their theoretical elegance, autoregressive models face significant practical limitations:\n\nSequential Generation: Sampling must proceed one element at a time\nComputational Intensity: Particularly problematic for high-dimensional data like images\nResolution Constraints: Practical implementations often limited to small images (e.g., 64×64)\nTraining Time: Requires extensive computational resources\n\nThese limitations have motivated the development of alternative approaches that can generate high-dimensional data more efficiently."
  },
  {
    "objectID": "lectures/summaries/Lecture20Summary.html#dimensionality-reduction-and-autoencoders",
    "href": "lectures/summaries/Lecture20Summary.html#dimensionality-reduction-and-autoencoders",
    "title": "Lecture 20: Generative Models: Autoregressives and Autoencoders",
    "section": "Dimensionality Reduction and Autoencoders",
    "text": "Dimensionality Reduction and Autoencoders\nAn alternative approach to generative modeling begins with dimensionality reduction techniques that learn compact representations of data.\n\nPrincipal Component Analysis\nPCA represents a classical approach to dimensionality reduction that finds a linear projection minimizing reconstruction error:\n\\[\\min_{\\mathbf{W}} \\frac{1}{N} \\sum_{i=1}^N \\|\\mathbf{x}_i - \\mathbf{W}\\mathbf{W}^T\\mathbf{x}_i\\|^2_2\\]\nWhere:\n\n\\(\\mathbf{W}\\) is a \\(P \\times K\\) matrix of orthonormal basis vectors\n\\(K \\ll P\\) is the dimensionality of the latent space\n\\(\\mathbf{z}_i = \\mathbf{W}^T\\mathbf{x}_i\\) represents the latent encoding\n\nPCA provides the optimal linear reconstruction under squared error, but its linearity severely restricts its expressive power for complex data like images or text.\n\n\nDeterministic Autoencoders\nAutoencoders generalize the dimensionality reduction framework by replacing linear projections with neural networks:\n\nEncoder \\(e(\\mathbf{x})\\): Maps input data to a low-dimensional latent code \\(\\mathbf{z}\\)\nBottleneck: Forces the model to learn a compressed representation\nDecoder \\(d(\\mathbf{z})\\): Reconstructs the original input from the latent code\n\nThe model is trained to minimize reconstruction error:\n\\[\\min_{e,d} \\frac{1}{N} \\sum_{i=1}^N \\|\\mathbf{x}_i - d(e(\\mathbf{x}_i))\\|^2_2\\]\nAutoencoders can leverage arbitrary neural architectures, including convolutional networks for images, enabling them to capture complex non-linear relationships in data.\n\n\nLimitations as Generative Models\nWhile autoencoders excel at representation learning, they have significant limitations as generative models:\n\nNo Explicit Density Model: They don’t define a probability distribution over data\nDiscontinuous Latent Space: Random points in latent space may decode to unrealistic samples\nNo Sampling Mechanism: No principled way to generate novel samples\nOverfitting Risk: May memorize training examples rather than learning data distribution\n\nThese limitations arise because standard autoencoders focus solely on reconstruction rather than learning a proper statistical model of data."
  },
  {
    "objectID": "lectures/summaries/Lecture20Summary.html#toward-probabilistic-generative-models",
    "href": "lectures/summaries/Lecture20Summary.html#toward-probabilistic-generative-models",
    "title": "Lecture 20: Generative Models: Autoregressives and Autoencoders",
    "section": "Toward Probabilistic Generative Models",
    "text": "Toward Probabilistic Generative Models\nTo overcome the limitations of deterministic autoencoders, we need to incorporate probabilistic elements.\n\nFactor Analysis\nFactor analysis represents an early probabilistic approach to dimensionality reduction:\n\\[P(\\mathbf{z}) = \\mathcal{N}(\\mathbf{z}|0, \\mathbf{I})\\] \\[P(\\mathbf{x}|\\mathbf{z}) = \\mathcal{N}(\\mathbf{x}|\\mathbf{W}\\mathbf{z} + \\boldsymbol{\\alpha}, \\boldsymbol{\\Psi})\\]\nThis formulation explicitly models:\n\nA prior distribution over latent variables\nA conditional distribution generating observations from latent variables\nUncertainty in the reconstruction process\n\nThis probabilistic perspective lays the foundation for more sophisticated models like variational autoencoders, which extend these ideas to deep neural networks while maintaining a principled probabilistic interpretation.\n\n\nToward Variational Autoencoders\nThe framework of factor analysis points toward variational autoencoders, which:\n\nReplace linear mappings with neural networks\nMaintain probabilistic interpretations of encodings and decodings\nEnable principled sampling and density estimation\nSupport conditional generation\n\nThis combination of deep learning with probabilistic modeling delivers both the flexibility of neural networks and the statistical rigor of generative models."
  },
  {
    "objectID": "lectures/summaries/Lecture18Summary.html",
    "href": "lectures/summaries/Lecture18Summary.html",
    "title": "Lecture 18: Transformers and the Attention Revolution",
    "section": "",
    "text": "Sequence-to-sequence models with attention represented a significant advancement in handling sequential data, but they still relied on recurrent structures that limited their efficiency and capacity to handle long-range dependencies.\n\n\nRecurrent seq2seq models suffer from several fundamental constraints:\n\nSequential processing introduces computational bottlenecks\nInformation must flow through multiple time steps, leading to vanishing gradients\nTraining requires time-sequential operations that cannot be parallelized\nDeep networks (one layer per token) become increasingly difficult to train\n\nFor a sequence with hundreds of tokens, these limitations severely restrict model capacity and training efficiency, particularly when handling complex tasks like language translation or question answering."
  },
  {
    "objectID": "lectures/summaries/Lecture18Summary.html#from-seq2seq-to-transformers",
    "href": "lectures/summaries/Lecture18Summary.html#from-seq2seq-to-transformers",
    "title": "Lecture 18: Transformers and the Attention Revolution",
    "section": "",
    "text": "Sequence-to-sequence models with attention represented a significant advancement in handling sequential data, but they still relied on recurrent structures that limited their efficiency and capacity to handle long-range dependencies.\n\n\nRecurrent seq2seq models suffer from several fundamental constraints:\n\nSequential processing introduces computational bottlenecks\nInformation must flow through multiple time steps, leading to vanishing gradients\nTraining requires time-sequential operations that cannot be parallelized\nDeep networks (one layer per token) become increasingly difficult to train\n\nFor a sequence with hundreds of tokens, these limitations severely restrict model capacity and training efficiency, particularly when handling complex tasks like language translation or question answering."
  },
  {
    "objectID": "lectures/summaries/Lecture18Summary.html#self-attention-the-core-innovation",
    "href": "lectures/summaries/Lecture18Summary.html#self-attention-the-core-innovation",
    "title": "Lecture 18: Transformers and the Attention Revolution",
    "section": "Self-Attention: The Core Innovation",
    "text": "Self-Attention: The Core Innovation\nSelf-attention represents the key breakthrough that enabled the transformer architecture, allowing models to directly connect any position in a sequence to any other position with constant computational complexity.\n\nSelf-Attention Mechanism\nThe self-attention operation computes weighted representations of the entire input sequence for each position:\n\nFor each input token, compute three vectors through learned linear projections:\n\nQuery vector (\\(\\mathbf{q}_i\\)): Represents what information the token is “looking for”\nKey vector (\\(\\mathbf{k}_i\\)): Represents what information the token “contains”\nValue vector (\\(\\mathbf{v}_i\\)): Represents the actual content of the token\n\nFor each position \\(i\\), calculate attention weights by comparing its query to all keys: \\[e_{ij} = \\frac{\\mathbf{q}_i^T\\mathbf{k}_j}{\\sqrt{d_k}}\\]\nNormalize weights using softmax: \\[a_{ij} = \\frac{\\exp(e_{ij})}{\\sum_{m=1}^n \\exp(e_{im})}\\]\nCompute a weighted sum of all value vectors: \\[\\mathbf{o}_i = \\sum_{j=1}^n a_{ij}\\mathbf{v}_j\\]\n\nThis operation allows each token to attend to all other tokens, capturing contextual relationships regardless of their distance in the sequence.\n\n\nMulti-Head Attention\nTo capture different types of relationships simultaneously, the transformer employs multi-head attention:\n\nApply multiple sets of query/key/value projections in parallel\nEach “head” can attend to different aspects of the input (e.g., syntactic vs. semantic relationships)\nConcatenate the outputs from all heads and project to the original dimension\n\nMulti-head attention enables the model to jointly attend to information from different representational subspaces, enhancing the model’s ability to capture complex patterns."
  },
  {
    "objectID": "lectures/summaries/Lecture18Summary.html#positional-encodings",
    "href": "lectures/summaries/Lecture18Summary.html#positional-encodings",
    "title": "Lecture 18: Transformers and the Attention Revolution",
    "section": "Positional Encodings",
    "text": "Positional Encodings\nA critical insight in transformers is addressing the lack of inherent sequence ordering in self-attention. Unlike RNNs, self-attention has no built-in notion of token position.\n\nSinusoidal Position Embeddings\nThe transformer architecture incorporates position information through additive sinusoidal encodings:\n\\[PE_{(pos,2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)\\] \\[PE_{(pos,2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)\\]\nWhere:\n\n\\(pos\\) is the position in the sequence\n\\(i\\) is the dimension index\n\\(d_{model}\\) is the embedding dimension\n\nThese encodings have important properties:\n\nThey provide unique patterns for each position\nThey capture relative distances between positions\nThey allow the model to extrapolate to sequence lengths not seen in training\nThey don’t require additional parameters to be learned"
  },
  {
    "objectID": "lectures/summaries/Lecture18Summary.html#the-transformer-architecture",
    "href": "lectures/summaries/Lecture18Summary.html#the-transformer-architecture",
    "title": "Lecture 18: Transformers and the Attention Revolution",
    "section": "The Transformer Architecture",
    "text": "The Transformer Architecture\nThe complete transformer architecture combines multiple components into an encoder-decoder structure that entirely replaces recurrence with attention.\n\nEncoder Block\nEach encoder block consists of:\n\nMulti-head self-attention layer\nPosition-wise feed-forward network (FFN)\nLayer normalization and residual connections\n\nThe encoder processes the entire input sequence in parallel, generating contextual representations for each token that incorporate information from the entire sequence.\n\n\nDecoder Block\nEach decoder block contains:\n\nMasked multi-head self-attention (prevents attending to future positions)\nMulti-head cross-attention over encoder outputs\nPosition-wise feed-forward network\nLayer normalization and residual connections\n\nThe masking in the self-attention layer ensures that predictions for position \\(i\\) can only depend on known outputs at positions less than \\(i\\), preserving the autoregressive property needed for generation.\n\n\nParallelization Advantage\nThe transformer architecture offers significant computational advantages:\n\nSelf-attention operations can be parallelized across all sequence positions\nMultiple attention heads can be computed simultaneously\nFor a sequence of length \\(n\\), complexity reduces from \\(O(n)\\) sequential operations to \\(O(1)\\)\nGPU acceleration becomes dramatically more effective\n\nThis parallelization enables training on substantially larger datasets and with more parameters than was previously feasible with recurrent architectures."
  },
  {
    "objectID": "lectures/summaries/Lecture18Summary.html#the-generative-pre-trained-transformer-gpt",
    "href": "lectures/summaries/Lecture18Summary.html#the-generative-pre-trained-transformer-gpt",
    "title": "Lecture 18: Transformers and the Attention Revolution",
    "section": "The Generative Pre-trained Transformer (GPT)",
    "text": "The Generative Pre-trained Transformer (GPT)\nGPT represents a decoder-only variant of the transformer architecture, optimized for autoregressive language modeling.\n\nArchitecture and Training\nGPT consists of:\n\nToken and position embeddings\nMultiple layers of masked self-attention blocks\nA final linear layer with softmax activation\n\nThe model is trained to predict the next token given all previous tokens in the sequence, using a language modeling objective:\n\\[L(\\theta) = \\sum_i \\log P(x_i | x_{&lt;i}; \\theta)\\]\nDuring training, all tokens are processed in parallel with appropriate masking, while generation remains sequential.\n\n\nAutoregressive Generation\nAt inference time, GPT generates text one token at a time:\n\nStart with a prompt sequence\nFeed the sequence through the model to get a probability distribution over the next token\nSample from this distribution (using techniques like top-k or nucleus sampling)\nAppend the sampled token to the sequence\nRepeat until an end condition is met\n\nThis approach allows GPT to generate coherent, contextually relevant text that follows the statistical patterns observed in its training data.\n\n\nScaling Properties\nGPT models have demonstrated remarkable scaling properties:\n\nPerformance improves predictably with more parameters and training data\nLarger models exhibit emergent capabilities not present in smaller variants\nThe same architecture can be applied across diverse language tasks\n\nModern implementations like GPT-3 and GPT-4 have scaled to hundreds of billions of parameters, enabling unprecedented capabilities in language understanding and generation."
  },
  {
    "objectID": "lectures/summaries/Lecture18Summary.html#bidirectional-encoder-representations-from-transformers-bert",
    "href": "lectures/summaries/Lecture18Summary.html#bidirectional-encoder-representations-from-transformers-bert",
    "title": "Lecture 18: Transformers and the Attention Revolution",
    "section": "Bidirectional Encoder Representations from Transformers (BERT)",
    "text": "Bidirectional Encoder Representations from Transformers (BERT)\nWhile GPT focuses on generative capabilities, BERT represents an encoder-only approach optimized for representational learning.\n\nBidirectional Context\nUnlike the unidirectional context of GPT, BERT:\n\nProcesses the entire input sequence simultaneously\nAttends to both left and right context for each token\nDevelops rich bidirectional representations\n\nThis bidirectional approach is particularly suited for tasks that require deep understanding of context, such as question answering and natural language inference."
  },
  {
    "objectID": "lectures/summaries/Lecture18Summary.html#the-transformer-revolution",
    "href": "lectures/summaries/Lecture18Summary.html#the-transformer-revolution",
    "title": "Lecture 18: Transformers and the Attention Revolution",
    "section": "The Transformer Revolution",
    "text": "The Transformer Revolution\nThe introduction of transformer architectures marked a paradigm shift in machine learning for sequential data:\n\nEliminated the sequential bottleneck of recurrent models\nEnabled efficient training of much larger models\nEstablished a fundamental architecture that scales effectively with compute\nProvided a unified approach to diverse language and sequence modeling tasks\n\nThis architectural innovation directly enabled the development of today’s large language models and continues to drive advancements across machine learning domains from natural language processing to computer vision."
  },
  {
    "objectID": "lectures/summaries/Lecture16Summary.html",
    "href": "lectures/summaries/Lecture16Summary.html",
    "title": "Lecture 16: Recurrent Neural Networks and Sequence Modeling",
    "section": "",
    "text": "Sequence modeling represents a fundamental shift from previous neural network architectures by introducing temporal dependencies. While feedforward networks map individual inputs to individual outputs, sequence models handle data where order and context matter.\n\n\nSequence models can accommodate various input-output relationships:\n\nOne-to-Many: A single input generates a sequence of outputs\n\nExample: Image captioning (one image → sequence of words)\n\nMany-to-One: A sequence of inputs produces a single output\n\nExample: Sentiment analysis (sequence of words → sentiment class)\nExample: Video classification (sequence of frames → video category)\n\nMany-to-Many (Unaligned): Sequence of inputs yields a different-length sequence of outputs\n\nExample: Machine translation (English sentence → French sentence)\nExample: Time series forecasting (historical data → future predictions)\n\nMany-to-Many (Aligned): Each input element corresponds to an output element\n\nExample: Part-of-speech tagging (each word → grammatical tag)\nExample: Per-frame video annotation (each frame → frame label)"
  },
  {
    "objectID": "lectures/summaries/Lecture16Summary.html#sequence-modeling-paradigms",
    "href": "lectures/summaries/Lecture16Summary.html#sequence-modeling-paradigms",
    "title": "Lecture 16: Recurrent Neural Networks and Sequence Modeling",
    "section": "",
    "text": "Sequence modeling represents a fundamental shift from previous neural network architectures by introducing temporal dependencies. While feedforward networks map individual inputs to individual outputs, sequence models handle data where order and context matter.\n\n\nSequence models can accommodate various input-output relationships:\n\nOne-to-Many: A single input generates a sequence of outputs\n\nExample: Image captioning (one image → sequence of words)\n\nMany-to-One: A sequence of inputs produces a single output\n\nExample: Sentiment analysis (sequence of words → sentiment class)\nExample: Video classification (sequence of frames → video category)\n\nMany-to-Many (Unaligned): Sequence of inputs yields a different-length sequence of outputs\n\nExample: Machine translation (English sentence → French sentence)\nExample: Time series forecasting (historical data → future predictions)\n\nMany-to-Many (Aligned): Each input element corresponds to an output element\n\nExample: Part-of-speech tagging (each word → grammatical tag)\nExample: Per-frame video annotation (each frame → frame label)"
  },
  {
    "objectID": "lectures/summaries/Lecture16Summary.html#recurrent-neural-networks-rnns",
    "href": "lectures/summaries/Lecture16Summary.html#recurrent-neural-networks-rnns",
    "title": "Lecture 16: Recurrent Neural Networks and Sequence Modeling",
    "section": "Recurrent Neural Networks (RNNs)",
    "text": "Recurrent Neural Networks (RNNs)\nRecurrent Neural Networks extend feedforward architectures by introducing connections between hidden states across time steps.\n\nCore Concept: Hidden State Recurrence\nThe defining feature of RNNs is the recurrence relation:\n\\[h_t = f_w(h_{t-1}, x_t)\\]\nWhere:\n\n\\(h_{t-1}\\) is the previous hidden state vector\n\\(x_t\\) is the current input vector\n\\(f_w\\) is a parameterized function\n\\(h_t\\) is the new hidden state vector\n\n\n\nVanilla RNN Architecture\nThe standard RNN update function is:\n\\[\\mathbf{h}_t = \\tanh(\\mathbf{W}_{hh}\\mathbf{h}_{t-1} + \\mathbf{W}_{xh}\\mathbf{x}_t + \\mathbf{b}_h)\\]\nWhere:\n\n\\(\\mathbf{W}_{hh}\\) is the hidden-to-hidden weight matrix (dimension \\(m \\times m\\))\n\\(\\mathbf{W}_{xh}\\) is the input-to-hidden weight matrix (dimension \\(m \\times P\\))\n\\(\\mathbf{b}_h\\) is a bias vector\n\\(\\tanh\\) provides non-linearity while maintaining gradient flow\n\nThe output at each time step is computed as:\n\\[\\mathbf{y}_t = h(\\mathbf{W}_{hy}\\mathbf{h}_t + \\mathbf{b}_y)\\]\nWhere:\n\n\\(\\mathbf{W}_{hy}\\) is the hidden-to-output weight matrix\n\\(h()\\) is an appropriate activation function (e.g., softmax for classification)\n\n\n\nParameter Sharing\nA critical insight of RNNs is that the same weight matrices (\\(\\mathbf{W}_{hh}\\), \\(\\mathbf{W}_{xh}\\), \\(\\mathbf{W}_{hy}\\)) are used across all time steps, making the model:\n\nComputationally efficient\nAble to handle variable-length sequences\nCapable of learning patterns independent of their position in the sequence\n\n\n\nLanguage Modeling Example\nA character-level language model demonstrates the RNN’s ability to predict the next element in a sequence:\n\nInput: “h”, “e”, “l”, “l”\nTarget output: “e”, “l”, “l”, “o”\n\nAt inference time, the model can generate text by recursively feeding its own predictions back as inputs."
  },
  {
    "objectID": "lectures/summaries/Lecture16Summary.html#the-vanishing-gradient-problem",
    "href": "lectures/summaries/Lecture16Summary.html#the-vanishing-gradient-problem",
    "title": "Lecture 16: Recurrent Neural Networks and Sequence Modeling",
    "section": "The Vanishing Gradient Problem",
    "text": "The Vanishing Gradient Problem\nDespite their elegant design, vanilla RNNs suffer from a critical limitation when modeling long-range dependencies.\n\nBackpropagation Through Time\nTraining RNNs requires computing gradients through the entire sequence using backpropagation through time. For the weight matrix \\(\\mathbf{W}_{hh}\\), this gives:\n\\[\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}_{hh}} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_T} \\frac{\\partial \\mathbf{h}_{1}}{\\partial \\mathbf{W}_{hh}} \\prod_{t=2}^T \\frac{\\partial \\mathbf{h}_t}{\\partial \\mathbf{h}_{t-1}}\\]\nFor the tanh activation, each partial derivative includes:\n\\[\\frac{\\partial \\mathbf{h}_{t}}{\\partial \\mathbf{h}_{t-1}} = \\tanh'(\\mathbf{W}_{hh}\\mathbf{h}_{t-1} + \\mathbf{W}_{xh}\\mathbf{x}_t)\\mathbf{W}_{hh}\\]\n\n\nMathematical Analysis of Gradient Flow\nTwo factors affect the gradient magnitude across time steps:\n\nThe derivative of tanh is always between 0 and 1\nRepeated multiplication by \\(\\mathbf{W}_{hh}\\) has compounding effects:\n\nIf the largest singular value of \\(\\mathbf{W}_{hh}\\) is &gt; 1: exploding gradients\nIf the largest singular value of \\(\\mathbf{W}_{hh}\\) is &lt; 1: vanishing gradients\n\n\nThis means that for long sequences, earlier inputs have diminishing influence on later outputs, making it difficult to learn long-range dependencies."
  },
  {
    "objectID": "lectures/summaries/Lecture16Summary.html#long-short-term-memory-lstm",
    "href": "lectures/summaries/Lecture16Summary.html#long-short-term-memory-lstm",
    "title": "Lecture 16: Recurrent Neural Networks and Sequence Modeling",
    "section": "Long Short-Term Memory (LSTM)",
    "text": "Long Short-Term Memory (LSTM)\nLSTM networks address the vanishing gradient problem by introducing a more complex hidden state architecture with controlled information flow.\n\nCell State and Gating Mechanism\nLSTMs maintain two types of hidden state vectors:\n\nCell state (\\(\\mathbf{C}_t\\)): A memory vector that can maintain information across many time steps\nHidden state (\\(\\mathbf{h}_t\\)): The output vector used for predictions\n\nInformation flow between states is regulated by three gating mechanisms:\n\nForget Gate (\\(\\mathbf{f}_t\\)): Controls what information from the previous cell state to discard \\[\\mathbf{f}_t = \\sigma(\\mathbf{W}_{f}[\\mathbf{h}_{t-1}, \\mathbf{x}_t] + \\mathbf{b}_f)\\]\nInput Gate (\\(\\mathbf{i}_t\\)): Determines what new information will be stored in the cell state \\[\\mathbf{i}_t = \\sigma(\\mathbf{W}_{i}[\\mathbf{h}_{t-1}, \\mathbf{x}_t] + \\mathbf{b}_i)\\] \\[\\tilde{\\mathbf{C}}_t = \\tanh(\\mathbf{W}_{C}[\\mathbf{h}_{t-1}, \\mathbf{x}_t] + \\mathbf{b}_C)\\]\nOutput Gate (\\(\\mathbf{o}_t\\)): Determines what parts of the cell state to output \\[\\mathbf{o}_t = \\sigma(\\mathbf{W}_{o}[\\mathbf{h}_{t-1}, \\mathbf{x}_t] + \\mathbf{b}_o)\\]\n\n\n\nCell State Update\nThe cell state update follows a linear path with controlled modifications:\n\\[\\mathbf{C}_t = \\mathbf{f}_t \\odot \\mathbf{C}_{t-1} + \\mathbf{i}_t \\odot \\tilde{\\mathbf{C}}_t\\]\nWhere \\(\\odot\\) represents element-wise multiplication.\n\n\nHidden State Calculation\nThe hidden state is derived from the cell state:\n\\[\\mathbf{h}_t = \\mathbf{o}_t \\odot \\tanh(\\mathbf{C}_t)\\]\n\n\nGradient Flow in LSTMs\nThe key innovation of LSTMs is that gradients can flow through the cell state with minimal attenuation:\n\\[\\frac{\\partial \\mathbf{C}_t}{\\partial \\mathbf{C}_{t-1}} = \\mathbf{f}_t\\]\nSince the forget gate values are typically close to 1 for relevant information, the gradient doesn’t vanish exponentially across time steps, allowing LSTMs to learn long-range dependencies effectively."
  },
  {
    "objectID": "lectures/summaries/Lecture16Summary.html#practical-implications",
    "href": "lectures/summaries/Lecture16Summary.html#practical-implications",
    "title": "Lecture 16: Recurrent Neural Networks and Sequence Modeling",
    "section": "Practical Implications",
    "text": "Practical Implications\nLSTMs have become the standard building block for sequence modeling tasks due to their:\n\nAbility to learn long-range dependencies\nRobustness to vanishing gradients\nFlexibility in controlling information flow\n\nWhile LSTMs trade some expressiveness for stability in training, the architecture has proven remarkably effective for a wide range of sequence tasks including language modeling, machine translation, and time series forecasting.\nThe next evolution in sequence modeling—transformer architectures with multi-headed self-attention—builds on these foundations while addressing additional limitations of recurrent models."
  },
  {
    "objectID": "lectures/summaries/Lecture14Summary.html",
    "href": "lectures/summaries/Lecture14Summary.html",
    "title": "Lecture 14: Object Detection in Computer Vision",
    "section": "",
    "text": "Image classification—assigning a single label to an entire image—is just one of many computer vision tasks. More advanced applications require models that can:\n\nLocate multiple objects within images\nIdentify the class of each object\nPrecisely mark boundaries around detected objects\n\nObject detection addresses the first two tasks, providing both the classification (what) and localization (where) of objects in a scene."
  },
  {
    "objectID": "lectures/summaries/Lecture14Summary.html#computer-vision-tasks-beyond-classification",
    "href": "lectures/summaries/Lecture14Summary.html#computer-vision-tasks-beyond-classification",
    "title": "Lecture 14: Object Detection in Computer Vision",
    "section": "",
    "text": "Image classification—assigning a single label to an entire image—is just one of many computer vision tasks. More advanced applications require models that can:\n\nLocate multiple objects within images\nIdentify the class of each object\nPrecisely mark boundaries around detected objects\n\nObject detection addresses the first two tasks, providing both the classification (what) and localization (where) of objects in a scene."
  },
  {
    "objectID": "lectures/summaries/Lecture14Summary.html#transfer-learning-for-advanced-vision-tasks",
    "href": "lectures/summaries/Lecture14Summary.html#transfer-learning-for-advanced-vision-tasks",
    "title": "Lecture 14: Object Detection in Computer Vision",
    "section": "Transfer Learning for Advanced Vision Tasks",
    "text": "Transfer Learning for Advanced Vision Tasks\nWhen approaching complex vision tasks, transfer learning offers significant advantages:\n\nFeature Extraction Approach\n\nUse pre-trained CNN backbone (often from ImageNet) as a feature extractor\nFreeze the convolutional layers to maintain learned representations\nAdd task-specific heads for new objectives\nTrain only the new heads while keeping the feature extractor fixed\n\n\n\nFine-Tuning Approach\n\nInitialize model with pre-trained weights\nAdd task-specific layers for the desired task\nTrain in stages:\n\nTrain only the new layers with backbone frozen\nUnfreeze deeper layers gradually (“progressive unfreezing”)\nTrain the entire network with a very small learning rate\n\n\nBest practices for fine-tuning include:\n\nUse significantly lower learning rates (10-100× smaller)\nRequire substantial task-specific data\nOnly unfreeze deeper layers when necessary"
  },
  {
    "objectID": "lectures/summaries/Lecture14Summary.html#object-detection-fundamentals",
    "href": "lectures/summaries/Lecture14Summary.html#object-detection-fundamentals",
    "title": "Lecture 14: Object Detection in Computer Vision",
    "section": "Object Detection Fundamentals",
    "text": "Object Detection Fundamentals\nObject detection combines classification with localization to identify multiple objects in images.\n\nTask Definition\nFor each object in an image, predict:\n\nClass label (from a predefined set of categories)\nBounding box coordinates, typically represented as:\n\n\\((x_{min}, y_{min}, width, height)\\) or\n\\((x_{min}, y_{min}, x_{max}, y_{max})\\)\n\n\nThese bounding boxes are usually axis-aligned (parallel to image edges) for simplicity and computational efficiency.\n\n\nEvaluation Metrics\nThe primary metric for measuring bounding box accuracy is Intersection over Union (IoU):\n\\[IoU = \\frac{\\text{Area of Intersection}}{\\text{Area of Union}}\\]\nIoU ranges from 0 (no overlap) to 1 (perfect overlap) and provides a geometric measure of detection quality."
  },
  {
    "objectID": "lectures/summaries/Lecture14Summary.html#two-stage-detectors-r-cnn-family",
    "href": "lectures/summaries/Lecture14Summary.html#two-stage-detectors-r-cnn-family",
    "title": "Lecture 14: Object Detection in Computer Vision",
    "section": "Two-Stage Detectors: R-CNN Family",
    "text": "Two-Stage Detectors: R-CNN Family\nTwo-stage detectors divide the detection process into region proposal and classification/refinement stages.\n\nR-CNN (Regions with CNN)\nThe original R-CNN approach follows these steps:\n\nRegion Proposal: Use selective search to identify ~2,000 potential object regions\n\nSelective search merges similar regions based on color, texture, and other visual cues\nMore efficient than exhaustive sliding windows (which would require billions of evaluations)\n\nFeature Extraction: Pass each proposed region through a CNN\n\nResize each region to fit CNN input dimensions\nExtract fixed-length feature vectors\n\nClassification: Apply SVM classifiers to categorize each region\n\nOne classifier per object category\nIncludes a background class for non-objects\n\nBounding Box Refinement: Apply regression to improve bounding box precision\n\n\n\nTraining Process\nThe training procedure for R-CNN requires defining positive and negative examples:\n\nPositive examples: Region proposals with IoU &gt; 0.5 with any ground truth box\nNegative examples (hard negatives): Region proposals with IoU &lt; 0.3 with all ground truth boxes\nIgnored examples: Region proposals with IoU between 0.3 and 0.5\n\n\n\nPrediction and Post-Processing\nAfter obtaining raw detections, Non-Maximum Suppression (NMS) removes redundant predictions:\n\nSelect the highest-scoring detection\nEliminate overlapping detections with IoU &gt; threshold (typically 0.7)\nRepeat until no redundant detections remain\n\n\n\nLimitations of Two-Stage Detectors\nDespite their accuracy, R-CNN and its variants (Fast R-CNN, Faster R-CNN) suffer from:\n\nHigh computational cost (~0.5-5 frames per second)\nComplex multi-stage pipeline\nImpractical for real-time applications"
  },
  {
    "objectID": "lectures/summaries/Lecture14Summary.html#single-shot-detectors",
    "href": "lectures/summaries/Lecture14Summary.html#single-shot-detectors",
    "title": "Lecture 14: Object Detection in Computer Vision",
    "section": "Single-Shot Detectors",
    "text": "Single-Shot Detectors\nSingle-shot detectors perform object detection in a single forward pass through the network, dramatically improving speed.\n\nYOLO (You Only Look Once)\nYOLO revolutionized object detection by reformulating it as a single regression problem:\n\nGrid Division: Divide the image into an S×S grid\nPrediction Structure: For each grid cell, predict:\n\nObject presence probability\nB bounding boxes with confidence scores\nClass probabilities\n\nOutput Representation: The output is a tensor of shape: \\[S \\times S \\times (B \\times 5 + C)\\] Where B is the number of boxes per cell and C is the number of classes\nUnified Architecture: Uses a fully convolutional network without separate proposal and classification stages\nJoint Loss Function: Combines localization, confidence, and classification losses:\n\nBounding box coordinate error (weighted higher for boxes containing objects)\nObjectness score error (presence/absence of objects)\nClassification error (only for cells containing objects)\n\n\n\n\nAdvantages of Single-Shot Detectors\nYOLO and similar models (SSD, RetinaNet) offer:\n\nReal-time detection speeds (45-155 FPS for YOLOv3-v9)\nEnd-to-end training\nReasonable accuracy, though historically slightly lower than two-stage detectors\nPractical deployment in resource-constrained environments\n\n\n\nEvolution of Detectors\nModern object detection has seen a convergence in performance:\n\nTwo-stage detectors offer higher accuracy but lower speed\nSingle-shot detectors offer higher speed but lower accuracy\nRecent advances (YOLOv4-v9, EfficientDet) have narrowed this gap\n\nThe choice between architectures depends on application requirements:\n\nSafety-critical applications may prioritize accuracy (two-stage)\nReal-time applications may prioritize speed (single-shot)\nMobile applications may prioritize efficiency (specialized single-shot)\n\nThis progression reflects a fundamental engineering tradeoff between speed and accuracy that continues to drive innovation in the field."
  },
  {
    "objectID": "lectures/summaries/Lecture12Summary.html",
    "href": "lectures/summaries/Lecture12Summary.html",
    "title": "Lecture 12: Image Classification with Convolutional Neural Networks",
    "section": "",
    "text": "Fully connected neural networks (FCNNs) encounter several challenges when applied to image data:\n\nLoss of Spatial Information: When images are flattened into vectors, the spatial relationships between neighboring pixels are lost.\nLack of Translational Invariance: FCNNs are sensitive to the exact position of features within an image, making them inefficient for pattern recognition tasks where the position of a feature might vary.\nParameter Explosion: For even modest-sized images, FCNNs require an enormous number of parameters. A 28×28 grayscale image with 128 hidden units requires 100,352 parameters in just the first layer.\nInefficient Feature Learning: FCNNs struggle to identify localized patterns that are critical for image recognition."
  },
  {
    "objectID": "lectures/summaries/Lecture12Summary.html#limitations-of-fully-connected-networks-for-images",
    "href": "lectures/summaries/Lecture12Summary.html#limitations-of-fully-connected-networks-for-images",
    "title": "Lecture 12: Image Classification with Convolutional Neural Networks",
    "section": "",
    "text": "Fully connected neural networks (FCNNs) encounter several challenges when applied to image data:\n\nLoss of Spatial Information: When images are flattened into vectors, the spatial relationships between neighboring pixels are lost.\nLack of Translational Invariance: FCNNs are sensitive to the exact position of features within an image, making them inefficient for pattern recognition tasks where the position of a feature might vary.\nParameter Explosion: For even modest-sized images, FCNNs require an enormous number of parameters. A 28×28 grayscale image with 128 hidden units requires 100,352 parameters in just the first layer.\nInefficient Feature Learning: FCNNs struggle to identify localized patterns that are critical for image recognition."
  },
  {
    "objectID": "lectures/summaries/Lecture12Summary.html#convolutional-neural-networks",
    "href": "lectures/summaries/Lecture12Summary.html#convolutional-neural-networks",
    "title": "Lecture 12: Image Classification with Convolutional Neural Networks",
    "section": "Convolutional Neural Networks",
    "text": "Convolutional Neural Networks\nConvolutional Neural Networks (CNNs) address these limitations through specialized layers designed for processing grid-like data, particularly images.\n\nConvolution Operations\nThe core operation in CNNs is the convolution, which slides a small weight matrix (kernel or filter) over the input image, computing the dot product at each position:\n\\[(\\mathbf{A} \\circledast \\mathbf{W})_{ij} = \\sum_{m} \\sum_{n} \\mathbf{A}_{i+m,j+n} \\mathbf{W}_{m,n}\\]\nFor a 2D input with a 2×2 filter example:\n\\[c_{11} = a_{11}w_{11} + a_{12}w_{12} + a_{21}w_{21} + a_{22}w_{22}\\]\nThis operation produces a feature map that highlights regions where patterns match the filter.\n\n\nOutput Dimensions\nFor an input of size \\(n_h \\times n_w\\) and a filter of size \\(f_h \\times f_w\\), the output dimensions are:\n\\[\\text{Output Size} = (n_h - f_h + 1) \\times (n_w - f_w + 1)\\]\nWith zero-padding of \\(p\\) and stride \\(s\\), the output dimensions become:\n\\[\\text{Output Size} = \\left(\\frac{n_h + 2p - f_h + s}{s}\\right) \\times \\left(\\frac{n_w + 2p - f_w + s}{s}\\right)\\]\n\n\nKey Properties\nConvolution operations provide several advantages:\n\nParameter Sharing: The same filter is applied across the entire image, dramatically reducing the number of parameters. A 5×5 filter requires only 25 parameters compared to thousands in FCNNs.\nTranslational Invariance: Patterns are detected regardless of their position in the image.\nHierarchical Feature Learning: Early layers detect simple features (edges, corners), while deeper layers combine these to recognize complex patterns."
  },
  {
    "objectID": "lectures/summaries/Lecture12Summary.html#cnn-architecture-components",
    "href": "lectures/summaries/Lecture12Summary.html#cnn-architecture-components",
    "title": "Lecture 12: Image Classification with Convolutional Neural Networks",
    "section": "CNN Architecture Components",
    "text": "CNN Architecture Components\n\nConvolutional Layers\nEach convolutional layer applies a set of learnable filters to produce multiple feature maps:\n\\[\\text{Feature Map}_k = \\text{ReLU}(\\mathbf{W}_k \\circledast \\mathbf{X} + b_k)\\]\nWhere:\n\n\\(\\mathbf{W}_k\\) is the \\(k\\)-th filter\n\\(b_k\\) is the bias term\nReLU is the activation function\n\n\n\nPooling Layers\nPooling reduces the spatial dimensions of feature maps while preserving important information:\n\nMax Pooling: Takes the maximum value in each window\n\\[\\text{MaxPool}(A)_{ij} = \\max_{m,n \\in \\text{window}} A_{i+m,j+n}\\]\nAverage Pooling: Takes the average of values in each window\n\nFor a pooling operation with filter size \\(K \\times K\\) and stride \\(s\\), the output size is:\n\\[\\text{Output Size} = \\left(\\frac{H - K}{s} + 1\\right) \\times \\left(\\frac{W - K}{s} + 1\\right)\\]\nPooling provides:\n\nFurther translational invariance\nReduced computational complexity\nResistance to small distortions\n\n\n\nMulti-Channel Convolutions\nFor RGB images or multi-channel feature maps, filters extend through all input channels:\n\nInput: \\(C_{in} \\times H \\times W\\)\nFilter: \\(C_{in} \\times K_h \\times K_w\\)\nOutput: Single channel feature map\n\nWith \\(C_{out}\\) filters, the output becomes a \\(C_{out} \\times H_{out} \\times W_{out}\\) feature map."
  },
  {
    "objectID": "lectures/summaries/Lecture12Summary.html#standard-cnn-architecture",
    "href": "lectures/summaries/Lecture12Summary.html#standard-cnn-architecture",
    "title": "Lecture 12: Image Classification with Convolutional Neural Networks",
    "section": "Standard CNN Architecture",
    "text": "Standard CNN Architecture\nThe LeNet-5 architecture (1998) exemplifies the standard CNN structure:\n\nInput Layer: Raw image (e.g., 1×28×28 for MNIST)\nConvolutional Layer: Apply filters to detect features\nActivation Layer: Apply nonlinearity (typically ReLU)\nPooling Layer: Reduce spatial dimensions\nRepeat Steps 2-4: Build hierarchical representations\nFully Connected Layers: Combine features for classification\nOutput Layer: Final classification (e.g., softmax for multi-class)\n\nThis architecture progressively transforms the input from pixel values to high-level features:\n\nFirst-layer filters typically detect edges and simple textures\nMiddle layers combine these to form parts of objects\nDeeper layers represent complete objects or scenes"
  },
  {
    "objectID": "lectures/summaries/Lecture12Summary.html#advantages-of-cnns",
    "href": "lectures/summaries/Lecture12Summary.html#advantages-of-cnns",
    "title": "Lecture 12: Image Classification with Convolutional Neural Networks",
    "section": "Advantages of CNNs",
    "text": "Advantages of CNNs\nCNNs offer significant benefits for image processing:\n\nEfficiency: Despite performing many operations, convolutions are computationally efficient, especially on GPUs.\nReduced Parameters: A CNN with 6 filters of size 5×5 requires only 150 parameters, compared to 4,704 for an equivalent FCNN.\nPerformance: CNNs consistently outperform FCNNs on image tasks in both accuracy and training speed.\nFeature Interpretability: Convolutional filters often correspond to human-interpretable image features, unlike the abstract representations in FCNNs.\n\nThese advantages have made CNNs the dominant architecture for computer vision tasks, including image classification, object detection, and segmentation."
  },
  {
    "objectID": "lectures/summaries/Lecture10Summary.html",
    "href": "lectures/summaries/Lecture10Summary.html",
    "title": "Lecture 10: Backpropagation and Gradient Problems",
    "section": "",
    "text": "Backpropagation is an algorithmic approach to compute gradients efficiently in deep neural networks. The algorithm consists of two key steps:\n\nForward Pass: Starting from input, compute all intermediate values up to the loss function\nBackward Pass: Propagate gradients from the loss function back to all parameters\n\nFor a deep neural network, the loss can be expressed as:\n\\[\\mathcal{L}(\\boldsymbol{\\theta} | \\mathbf{X}, \\mathbf{y}) = \\frac{1}{N} \\sum_{i=1}^N \\mathcal{L}(\\theta_i | \\mathbf{x}_i, y_i)\\]\nWhere the output \\(\\theta_i\\) is defined as:\n\\[\\theta_i = g(\\boldsymbol{\\beta}^T \\varphi(\\mathbf{W}_D^T \\varphi(\\mathbf{W}_{D-1}^T \\varphi(\\mathbf{W}_{D-2}^T ... \\varphi(\\mathbf{W}_1^T \\mathbf{x}_i)))))\\]\n\n\nBackpropagation operates on computational graphs, where:\n\nEach operation in the network is represented as a node in a directed acyclic graph\nThe graph consists of primitive operations (multiplication, addition) and activation functions\nParameters to be learned are represented as special nodes\n\nDuring the backward pass, gradients are computed using the chain rule:\n\\[\\text{Downstream Gradient} = \\text{Local Gradient} \\times \\text{Upstream Gradient}\\]\nOr mathematically:\n\\[\\frac{\\partial \\mathcal{L}}{\\partial x} = \\frac{\\partial \\mathcal{L}}{\\partial q} \\frac{\\partial q}{\\partial x}\\]\nIn a simple logistic regression, the gradient for the coefficients can be derived as:\n\\[\\frac{\\partial \\mathcal{L}_i}{\\partial \\mathbf{W}} = (\\sigma(z_i) - y_i)\\mathbf{x}_i\\]\n\\[\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}} = \\frac{1}{N} \\sum_{i=1}^N (\\sigma(z_i) - y_i)\\mathbf{x}_i\\]\n\n\n\nFor multi-layer neural networks, gradients become more complex. In a two-layer network with ReLU activations:\n\\[\\mathcal{L}(\\mathbf{z} | \\mathbf{X}, \\mathbf{y}) = -\\frac{1}{N} \\sum_{i=1}^N y_i \\log \\sigma(z_i) + (1 - y_i)(1 - \\log\\sigma(z_i))\\]\n\\[z_i = \\boldsymbol{\\beta}^T \\mathbf{h}_{i2} + a\\]\n\\[\\mathbf{h}_{i2} = \\varphi(\\mathbf{q}_{i2})\\]\n\\[\\mathbf{q}_{i2} = \\mathbf{W}_2^T\\mathbf{h}_{i1} + \\mathbf{b}_2\\]\n\\[\\mathbf{h}_{i1} = \\varphi(\\mathbf{q}_{i1})\\]\n\\[\\mathbf{q}_{i1} = \\mathbf{W}_1^T\\mathbf{x}_i + \\mathbf{b}_1\\]\nThe gradient flows backward through the network, with each parameter receiving gradients that are products of all upstream derivatives:\n\\[\\frac{\\partial \\mathcal{L}_i}{\\partial \\mathbf{W}_1} = \\mathbf{u}^T \\times \\mathbf{x}_i^T\\]\nWhere \\(\\mathbf{u}\\) represents the accumulated upstream gradient:\n\\[\\mathbf{u} = [\\sigma(z_i) - y_i] \\otimes \\boldsymbol{\\beta}^T \\odot I(\\mathbf{q}_{i2} &gt; 0) \\otimes \\mathbf{W}_{2} \\odot I(\\mathbf{q}_{i1} &gt; 0)\\]"
  },
  {
    "objectID": "lectures/summaries/Lecture10Summary.html#backpropagation",
    "href": "lectures/summaries/Lecture10Summary.html#backpropagation",
    "title": "Lecture 10: Backpropagation and Gradient Problems",
    "section": "",
    "text": "Backpropagation is an algorithmic approach to compute gradients efficiently in deep neural networks. The algorithm consists of two key steps:\n\nForward Pass: Starting from input, compute all intermediate values up to the loss function\nBackward Pass: Propagate gradients from the loss function back to all parameters\n\nFor a deep neural network, the loss can be expressed as:\n\\[\\mathcal{L}(\\boldsymbol{\\theta} | \\mathbf{X}, \\mathbf{y}) = \\frac{1}{N} \\sum_{i=1}^N \\mathcal{L}(\\theta_i | \\mathbf{x}_i, y_i)\\]\nWhere the output \\(\\theta_i\\) is defined as:\n\\[\\theta_i = g(\\boldsymbol{\\beta}^T \\varphi(\\mathbf{W}_D^T \\varphi(\\mathbf{W}_{D-1}^T \\varphi(\\mathbf{W}_{D-2}^T ... \\varphi(\\mathbf{W}_1^T \\mathbf{x}_i)))))\\]\n\n\nBackpropagation operates on computational graphs, where:\n\nEach operation in the network is represented as a node in a directed acyclic graph\nThe graph consists of primitive operations (multiplication, addition) and activation functions\nParameters to be learned are represented as special nodes\n\nDuring the backward pass, gradients are computed using the chain rule:\n\\[\\text{Downstream Gradient} = \\text{Local Gradient} \\times \\text{Upstream Gradient}\\]\nOr mathematically:\n\\[\\frac{\\partial \\mathcal{L}}{\\partial x} = \\frac{\\partial \\mathcal{L}}{\\partial q} \\frac{\\partial q}{\\partial x}\\]\nIn a simple logistic regression, the gradient for the coefficients can be derived as:\n\\[\\frac{\\partial \\mathcal{L}_i}{\\partial \\mathbf{W}} = (\\sigma(z_i) - y_i)\\mathbf{x}_i\\]\n\\[\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}} = \\frac{1}{N} \\sum_{i=1}^N (\\sigma(z_i) - y_i)\\mathbf{x}_i\\]\n\n\n\nFor multi-layer neural networks, gradients become more complex. In a two-layer network with ReLU activations:\n\\[\\mathcal{L}(\\mathbf{z} | \\mathbf{X}, \\mathbf{y}) = -\\frac{1}{N} \\sum_{i=1}^N y_i \\log \\sigma(z_i) + (1 - y_i)(1 - \\log\\sigma(z_i))\\]\n\\[z_i = \\boldsymbol{\\beta}^T \\mathbf{h}_{i2} + a\\]\n\\[\\mathbf{h}_{i2} = \\varphi(\\mathbf{q}_{i2})\\]\n\\[\\mathbf{q}_{i2} = \\mathbf{W}_2^T\\mathbf{h}_{i1} + \\mathbf{b}_2\\]\n\\[\\mathbf{h}_{i1} = \\varphi(\\mathbf{q}_{i1})\\]\n\\[\\mathbf{q}_{i1} = \\mathbf{W}_1^T\\mathbf{x}_i + \\mathbf{b}_1\\]\nThe gradient flows backward through the network, with each parameter receiving gradients that are products of all upstream derivatives:\n\\[\\frac{\\partial \\mathcal{L}_i}{\\partial \\mathbf{W}_1} = \\mathbf{u}^T \\times \\mathbf{x}_i^T\\]\nWhere \\(\\mathbf{u}\\) represents the accumulated upstream gradient:\n\\[\\mathbf{u} = [\\sigma(z_i) - y_i] \\otimes \\boldsymbol{\\beta}^T \\odot I(\\mathbf{q}_{i2} &gt; 0) \\otimes \\mathbf{W}_{2} \\odot I(\\mathbf{q}_{i1} &gt; 0)\\]"
  },
  {
    "objectID": "lectures/summaries/Lecture10Summary.html#gradient-problems",
    "href": "lectures/summaries/Lecture10Summary.html#gradient-problems",
    "title": "Lecture 10: Backpropagation and Gradient Problems",
    "section": "Gradient Problems",
    "text": "Gradient Problems\nAs neural networks become deeper, two major gradient-related problems can arise:\n\nExploding Gradients\nGradient explosion occurs when the chain of multiplication in backpropagation causes gradients to become extremely large:\n\\[(1 + \\epsilon)^D \\rightarrow \\infty\\]\nWhere \\(D\\) is the depth of the network and \\(\\epsilon\\) is a small positive value. This causes:\n\nUnstable training with large, erratic parameter updates\nInability to converge to a solution\nPotential numerical overflow issues\n\nApproaches to address exploding gradients include:\n\nGradient Clipping: Limiting the magnitude of gradients\n\\[\\mathbf{g}'(\\theta_{t-1}) = \\text{min}\\left(1, \\frac{c}{||\\mathbf{g}(\\theta_{t-1})||}\\right)\\mathbf{g}(\\theta_{t-1})\\]\nWhere \\(c\\) is a positive threshold value (often around 3).\nFeature Normalization: Ensuring input features are standardized or scaled to [0,1]\nWeight Initialization: Using appropriate initialization methods to control gradient scale\n\n\n\nVanishing Gradients\nVanishing gradients occur when the chain of multiplication causes gradients to become extremely small:\n\\[(1 - \\epsilon)^D \\rightarrow 0\\]\nThis leads to:\n\nParameters in early layers receiving minimal updates\nDifficulty in learning long-range dependencies\nTraining stagnation\n\nSpecific instances include “dead ReLUs,” where ReLU units output zero for all inputs, causing gradients to be zero.\n\n\nProper Initialization\nWeight initialization plays a crucial role in preventing gradient problems:\n\nRandom initialization from a normal distribution can lead to variance explosion as network depth increases\nThe variance of hidden units scales with the number of incoming connections (\\(N_{in}\\))\nThe variance of gradients scales with the number of outgoing connections (\\(N_{out}\\))\n\nGlorot Initialization addresses this by sampling weights from a distribution with:\n\\[\\sigma^2_{W_k} = \\frac{2}{N_{in} + N_{out}}\\]\nThis keeps the variance of activations and gradients stable across layers, enabling more effective training of deep networks."
  },
  {
    "objectID": "lectures/resources.html",
    "href": "lectures/resources.html",
    "title": "Additional Learning Resources",
    "section": "",
    "text": "This page provides additional learning resources to supplement the course materials.\n\n\n\n\n\nElements of Statistical Learning by Hastie, Tibshirani, and Friedman\nPattern Recognition and Machine Learning by Christopher Bishop\nIntroduction to Statistical Learning by James, Witten, Hastie, and Tibshirani\n\n\n\n\n\nDeep Learning by Goodfellow, Bengio, and Courville\nNeural Networks and Deep Learning by Michael Nielsen\nDive into Deep Learning - Interactive book with code examples\n\n\n\n\n\nProbabilistic Machine Learning: An Introduction by Kevin Murphy\nBayesian Reasoning and Machine Learning by David Barber\n\n\n\n\n\n\n\n\nStanford CS229: Machine Learning - Andrew Ng’s course with lecture notes and materials\nCornell CS4780: Machine Learning for Intelligent Systems - Kilian Weinberger’s lectures\nMIT 6.867: Machine Learning - MIT OpenCourseWare\n\n\n\n\n\nStanford CS231n: Convolutional Neural Networks for Visual Recognition - Excellent resource for computer vision\nStanford CS224n: Natural Language Processing with Deep Learning - Deep dive into NLP\nFast.ai - Practical deep learning courses\n\n\n\n\n\n\n\n\nscikit-learn - Comprehensive machine learning in Python\nTensorFlow Tutorials - Official TensorFlow documentation\nPyTorch Tutorials - Official PyTorch documentation\nNumPy - Foundation for scientific computing in Python\nPandas - Data manipulation and analysis\n\n\n\n\n\nMatplotlib - Comprehensive visualization library\nSeaborn - Statistical data visualization\nPlotly - Interactive visualizations\n\n\n\n\n\nHugging Face - State-of-the-art NLP models\nHands-on Reinforcement Learning - RL course by Hugging Face\nGeometric Deep Learning - Advanced ML on non-Euclidean domains"
  },
  {
    "objectID": "lectures/resources.html#reference-materials",
    "href": "lectures/resources.html#reference-materials",
    "title": "Additional Learning Resources",
    "section": "",
    "text": "This page provides additional learning resources to supplement the course materials.\n\n\n\n\n\nElements of Statistical Learning by Hastie, Tibshirani, and Friedman\nPattern Recognition and Machine Learning by Christopher Bishop\nIntroduction to Statistical Learning by James, Witten, Hastie, and Tibshirani\n\n\n\n\n\nDeep Learning by Goodfellow, Bengio, and Courville\nNeural Networks and Deep Learning by Michael Nielsen\nDive into Deep Learning - Interactive book with code examples\n\n\n\n\n\nProbabilistic Machine Learning: An Introduction by Kevin Murphy\nBayesian Reasoning and Machine Learning by David Barber\n\n\n\n\n\n\n\n\nStanford CS229: Machine Learning - Andrew Ng’s course with lecture notes and materials\nCornell CS4780: Machine Learning for Intelligent Systems - Kilian Weinberger’s lectures\nMIT 6.867: Machine Learning - MIT OpenCourseWare\n\n\n\n\n\nStanford CS231n: Convolutional Neural Networks for Visual Recognition - Excellent resource for computer vision\nStanford CS224n: Natural Language Processing with Deep Learning - Deep dive into NLP\nFast.ai - Practical deep learning courses\n\n\n\n\n\n\n\n\nscikit-learn - Comprehensive machine learning in Python\nTensorFlow Tutorials - Official TensorFlow documentation\nPyTorch Tutorials - Official PyTorch documentation\nNumPy - Foundation for scientific computing in Python\nPandas - Data manipulation and analysis\n\n\n\n\n\nMatplotlib - Comprehensive visualization library\nSeaborn - Statistical data visualization\nPlotly - Interactive visualizations\n\n\n\n\n\nHugging Face - State-of-the-art NLP models\nHands-on Reinforcement Learning - RL course by Hugging Face\nGeometric Deep Learning - Advanced ML on non-Euclidean domains"
  },
  {
    "objectID": "lectures/resources.html#research-paper-collections",
    "href": "lectures/resources.html#research-paper-collections",
    "title": "Additional Learning Resources",
    "section": "Research Paper Collections",
    "text": "Research Paper Collections\n\nFoundational Papers\n\nGradient-Based Learning Applied to Document Recognition - LeNet by Yann LeCun\nImageNet Classification with Deep Convolutional Neural Networks - AlexNet paper\nAttention Is All You Need - Transformer architecture\n\n\n\nRecent Impactful Papers\n\nLanguage Models are Few-Shot Learners - GPT-3 paper\nHigh-Resolution Image Synthesis with Latent Diffusion Models - Stable Diffusion\nAn Image is Worth 16x16 Words: Transformers for Image Recognition at Scale - Vision Transformer (ViT)\n\n\n\nPaper Discussion Groups\n\nPapers with Code - Papers with implementations\nTwo Minute Papers - Quick explanations of recent papers\nML Explained - In-depth explanations of ML papers"
  },
  {
    "objectID": "lectures/resources.html#interactive-learning-resources",
    "href": "lectures/resources.html#interactive-learning-resources",
    "title": "Additional Learning Resources",
    "section": "Interactive Learning Resources",
    "text": "Interactive Learning Resources\n\nNotebooks and Interactive Demos\n\nDistill.pub - Clear explanations of machine learning concepts\nTensorflow Playground - Neural network visualization\nGAN Lab - Interactive GAN experiments\n\n\n\nChallenges and Competitions\n\nKaggle Competitions - Data science competitions\nDrivenData - ML competitions for social impact\nAIcrowd - ML challenges and benchmarks"
  },
  {
    "objectID": "lectures/resources.html#career-development",
    "href": "lectures/resources.html#career-development",
    "title": "Additional Learning Resources",
    "section": "Career Development",
    "text": "Career Development\n\nJob Search Resources\n\nMachine Learning Jobs - Indeed job listings\nAI Jobs - Specialized AI/ML job board\nML/AI Graduate Programs - Guide to graduate programs\n\n\n\nML in Industry\n\nPapers from Industry - Collection of ML applications\nProduction ML Systems - Google’s guide to ML in production\nAI Ethics Guidelines - Ethical considerations in AI"
  },
  {
    "objectID": "lectures/lecture8.html",
    "href": "lectures/lecture8.html",
    "title": "Lecture 8: Introduction to Neural Networks for Tabular Data",
    "section": "",
    "text": "← Previous: Lecture 7\n\n\nNext: Lecture 9 →"
  },
  {
    "objectID": "lectures/lecture8.html#the-limitations-of-linear-models",
    "href": "lectures/lecture8.html#the-limitations-of-linear-models",
    "title": "Lecture 8: Introduction to Neural Networks for Tabular Data",
    "section": "1. The Limitations of Linear Models",
    "text": "1. The Limitations of Linear Models\nLinear models fail to capture complex decision boundaries in classification problems, as exemplified by the XOR problem:\n\\[\\mathbf{X} =\n\\begin{bmatrix}\n0 & 0 \\\\\n0 & 1 \\\\\n1 & 0 \\\\\n1 & 1\n\\end{bmatrix},\n\\mathbf{y} =\n\\begin{bmatrix}\n0 \\\\\n1 \\\\\n1 \\\\\n0\n\\end{bmatrix}\\]\nWhile polynomial expansions can solve this problem (e.g., a 5th-degree polynomial), they quickly become unwieldy as dimensionality increases, requiring \\(\\binom{P+d}{d} \\approx \\frac{P^d}{d!}\\) parameters. For high-dimensional data, this approach suffers from:\n\nExponential parameter growth\nPoor generalization\nHigh computational complexity\nLimited flexibility"
  },
  {
    "objectID": "lectures/lecture8.html#neural-network-architecture",
    "href": "lectures/lecture8.html#neural-network-architecture",
    "title": "Lecture 8: Introduction to Neural Networks for Tabular Data",
    "section": "2. Neural Network Architecture",
    "text": "2. Neural Network Architecture\nNeural networks overcome these limitations through a hierarchical architecture that transforms the input features through multiple processing layers.\n2.1. The Single Hidden Layer Network\nThe simplest neural network consists of:\n\nAn input layer representing the features\nA hidden layer that performs feature transformations\nAn output layer that produces the final prediction\n\nMathematically, this is represented as:\n\\[\\theta_i = \\boldsymbol{\\beta}^T \\phi(\\mathbf{x}_i; \\mathbf{W}, \\mathbf{c}) + b\\]\nWhere:\n\n\\(\\mathbf{W}\\) is a weight matrix with dimensions \\(P \\times K\\) (features × hidden units)\n\\(\\mathbf{c}\\) is a vector of bias terms for the hidden layer\n\\(\\phi()\\) is a nonlinear activation function\n\\(\\boldsymbol{\\beta}\\) is a vector of weights connecting the hidden layer to the output\n\\(b\\) is a bias term for the output layer\n\n2.2. The Critical Role of Activation Functions\nThe key innovation in neural networks is the introduction of nonlinearity through activation functions. Without nonlinear activation, multi-layer networks would reduce to simple linear models:\n\\[\\theta_i = \\boldsymbol{\\beta}^T(\\mathbf{W}^T \\mathbf{x}_i + \\mathbf{c}) + b = \\boldsymbol{\\beta}^T\\mathbf{W}^T \\mathbf{x}_i + (\\boldsymbol{\\beta}^T\\mathbf{c} + b)\\]\nActivation functions transform linear combinations of inputs into nonlinear representations. The Rectified Linear Unit (ReLU) is a common activation function:\n\\[\\varphi(x) = \\max(0, x)\\]\nApplied element-wise, ReLU introduces “kinks” in the function space, allowing neural networks to approximate complex, nonlinear decision boundaries."
  },
  {
    "objectID": "lectures/lecture8.html#learning-representations",
    "href": "lectures/lecture8.html#learning-representations",
    "title": "Lecture 8: Introduction to Neural Networks for Tabular Data",
    "section": "3. Learning Representations",
    "text": "3. Learning Representations\nNeural networks learn useful feature transformations from data, engineering new feature spaces where classification or regression becomes more tractable.\n3.1. Hidden Representations\nFor each input \\(\\mathbf{x}_i\\), the network computes a hidden representation:\n\\[\\mathbf{h}_i = \\varphi(\\mathbf{W}^T \\mathbf{x}_i + \\mathbf{c})\\]\nThis transformation maps the original features to a new space where:\n\nPoints that were not linearly separable might become separable\nComplex relationships might be simplified\nRelevant patterns are emphasized while noise is suppressed\n\n3.2. Geometric Interpretation\nEach hidden unit in a ReLU network creates a partition in the feature space:\n\nOn one side of the partition, the unit outputs zero\nOn the other side, it outputs a linear function of the input\nThe combination of multiple hidden units creates complex, piecewise linear decision boundaries\n\nWith sufficient hidden units, a single-layer neural network can approximate any continuous function on a bounded domain."
  },
  {
    "objectID": "lectures/lecture8.html#deep-neural-networks",
    "href": "lectures/lecture8.html#deep-neural-networks",
    "title": "Lecture 8: Introduction to Neural Networks for Tabular Data",
    "section": "4. Deep Neural Networks",
    "text": "4. Deep Neural Networks\nWhile single-layer networks are universal approximators, they may require an impractically large number of hidden units for complex functions. Deep neural networks (with multiple hidden layers) offer a more efficient alternative.\n4.1. Architecture of Deep Networks\nA two-layer network is represented as:\n\\[\\theta_i = g(\\boldsymbol{\\beta}^T \\varphi(\\mathbf{W}_2^T \\varphi(\\mathbf{W}_1^T \\mathbf{x}_i + \\mathbf{c}_1) + \\mathbf{c}_2) + b)\\]\nWhere:\n\n\\(\\mathbf{W}_1\\) is the weight matrix for the first hidden layer\n\\(\\mathbf{c}_1\\) is the bias vector for the first hidden layer\n\\(\\mathbf{W}_2\\) is the weight matrix for the second hidden layer\n\\(\\mathbf{c}_2\\) is the bias vector for the second hidden layer\n\n4.2. Benefits of Depth Over Width\nDeep networks offer several advantages over equivalently-sized wide networks:\n\nHierarchical Feature Learning: Each layer builds upon representations from previous layers\nParameter Efficiency: Deep networks can represent complex functions with fewer parameters\nCompositional Structure: The nested structure allows for representing compositional relationships\nInductive Bias: The hierarchical structure aligns well with many real-world data types"
  },
  {
    "objectID": "lectures/lecture8.html#training-neural-networks",
    "href": "lectures/lecture8.html#training-neural-networks",
    "title": "Lecture 8: Introduction to Neural Networks for Tabular Data",
    "section": "5. Training Neural Networks",
    "text": "5. Training Neural Networks\nNeural networks are trained through empirical risk minimization, typically using variants of gradient descent.\n5.1. Loss Functions\nCommon loss functions include:\n\nMean Squared Error for regression tasks\nCross-Entropy Loss for classification tasks: \\[\\mathcal{L}(\\boldsymbol{\\theta} | \\mathbf{X}, \\mathbf{y}) = -\\frac{1}{N} \\sum_{i=1}^N [y_i \\log(\\theta_i) + (1-y_i)\\log(1-\\theta_i)]\\]\n\n5.2. Optimization Challenges\nNeural networks pose unique optimization challenges:\n\nNon-convex loss landscapes with multiple local minima\nGradient vanishing or explosion in deep networks\nHigh sensitivity to initialization and hyperparameters\nLarge numbers of parameters requiring efficient optimization techniques\n\nModern neural network training relies on:\n\nStochastic gradient descent and adaptive optimization methods\nProper initialization techniques\nRegularization strategies\nBatch normalization and other stabilization techniques"
  },
  {
    "objectID": "lectures/lecture8.html#universal-approximation-properties",
    "href": "lectures/lecture8.html#universal-approximation-properties",
    "title": "Lecture 8: Introduction to Neural Networks for Tabular Data",
    "section": "6. Universal Approximation Properties",
    "text": "6. Universal Approximation Properties\nNeural networks with just a single hidden layer of sufficient width can approximate any continuous function on a bounded domain with arbitrary precision. This theoretical result, known as the Universal Approximation Theorem, explains why neural networks can model highly complex relationships.\nThe ability to learn representations directly from data, combined with their universal approximation properties, makes neural networks particularly powerful for complex tasks where feature engineering is difficult, such as image recognition, speech processing, and natural language understanding."
  },
  {
    "objectID": "lectures/lecture6.html",
    "href": "lectures/lecture6.html",
    "title": "Lecture 6: Adaptive Minimization Methods",
    "section": "",
    "text": "← Previous: Lecture 5\n\n\nNext: Lecture 7 →"
  },
  {
    "objectID": "lectures/lecture6.html#gradient-descent-the-foundation",
    "href": "lectures/lecture6.html#gradient-descent-the-foundation",
    "title": "Lecture 6: Adaptive Minimization Methods",
    "section": "1. Gradient Descent: The Foundation",
    "text": "1. Gradient Descent: The Foundation\nWhen analytical solutions for minimizing empirical risk are unavailable, gradient-based optimization methods become essential. The standard gradient descent algorithm iteratively updates parameters:\n\\[\\boldsymbol{\\beta}_{t+1} = \\boldsymbol{\\beta}_t - \\eta_t \\mathbf{g}(\\boldsymbol{\\beta}_t)\\]\nWhere:\n\n\\(\\mathbf{g}(\\boldsymbol{\\beta}_t)\\) is the gradient of the loss function evaluated at the current parameter values\n\\(\\eta_t\\) is the step size (learning rate) at iteration \\(t\\)\n\nFor most machine learning problems, the loss takes the form of an average across observations:\n\\[\\mathcal{L}(\\boldsymbol{\\beta}) = \\frac{1}{N} \\sum_{i=1}^N \\mathcal{L}_i(\\boldsymbol{\\beta})\\]\nConsequently, the gradient calculation requires evaluating gradients for all \\(N\\) observations, becoming computationally expensive as datasets grow."
  },
  {
    "objectID": "lectures/lecture6.html#stochastic-gradient-descent-sgd",
    "href": "lectures/lecture6.html#stochastic-gradient-descent-sgd",
    "title": "Lecture 6: Adaptive Minimization Methods",
    "section": "2. Stochastic Gradient Descent (SGD)",
    "text": "2. Stochastic Gradient Descent (SGD)\nStochastic Gradient Descent addresses the computational burden by approximating the full gradient using small batches of observations:\n\\[\\mathbf{g}(\\boldsymbol{\\beta}) \\approx \\frac{1}{B} \\sum_{i \\in \\mathcal{B}} \\mathbf{g}_i(\\boldsymbol{\\beta})\\]\nWhere \\(\\mathcal{B}\\) represents a randomly sampled batch of \\(B\\) observations.\nEven with batch size as small as 1, SGD converges to the minimum with appropriate learning rate schedules, drastically reducing computational requirements at the cost of introducing noise into the optimization path.\n2.1. Challenges with SGD\nWhile computationally efficient, SGD faces several challenges:\n\nHigh variance around the true minimum\nSensitivity to learning rate selection\nSlow convergence in flat regions of the loss landscape\nPoor performance in ravines and saddle points"
  },
  {
    "objectID": "lectures/lecture6.html#second-order-methods-leveraging-curvature",
    "href": "lectures/lecture6.html#second-order-methods-leveraging-curvature",
    "title": "Lecture 6: Adaptive Minimization Methods",
    "section": "3. Second-Order Methods: Leveraging Curvature",
    "text": "3. Second-Order Methods: Leveraging Curvature\nSecond-order methods incorporate information about the curvature of the loss function through the Hessian matrix. Using a multivariate Taylor series expansion, the update rule becomes:\n\\[\\boldsymbol{\\beta}_{t+1} = \\boldsymbol{\\beta}_t - \\eta \\mathbf{H}(\\boldsymbol{\\beta}_t)^{-1}\\mathbf{g}(\\boldsymbol{\\beta}_t)\\]\nWhere \\(\\mathbf{H}(\\boldsymbol{\\beta}_t)\\) is the Hessian matrix containing second derivatives of the loss function.\nThis approach “deskews” the loss space, accounting for curvature rather than following straight-line paths from each position. For logistic regression, the Hessian can be expressed as:\n\\[\\mathbf{H}(\\boldsymbol{\\beta}_t) = \\frac{1}{N} \\sum_{i=1}^N \\mathbf{x}_i \\mathbf{x}_i^T \\sigma(\\mathbf{x}_i^T \\boldsymbol{\\beta}_t)(1 - \\sigma(\\mathbf{x}_i^T \\boldsymbol{\\beta}_t))\\]\nWhile theoretically powerful, computing and inverting the full Hessian becomes prohibitively expensive in high dimensions."
  },
  {
    "objectID": "lectures/lecture6.html#momentum-the-heavy-ball-approach",
    "href": "lectures/lecture6.html#momentum-the-heavy-ball-approach",
    "title": "Lecture 6: Adaptive Minimization Methods",
    "section": "4. Momentum: The Heavy Ball Approach",
    "text": "4. Momentum: The Heavy Ball Approach\nMomentum methods accelerate optimization by accumulating a running average of past gradients, analogous to a heavy ball rolling down a hill:\n\\[\\mathbf{m}_{t+1} = b \\mathbf{m}_t + \\mathbf{g}(\\boldsymbol{\\beta}_t)\\] \\[\\boldsymbol{\\beta}_{t+1} = \\boldsymbol{\\beta}_t - \\eta \\mathbf{m}_{t+1}\\]\nWhere \\(b \\in [0,1)\\) is the momentum coefficient, typically set around 0.9.\nMomentum provides two key advantages:\n\nAccelerated progress in consistent gradient directions (flat regions)\nDampened oscillations in regions with rapidly changing gradients\n\nWhen gradient directions remain consistent, momentum effectively multiplies the learning rate by a factor approaching \\(\\frac{1}{1-b}\\). With \\(b=0.9\\), this can yield a 10x speedup in consistent gradient regions."
  },
  {
    "objectID": "lectures/lecture6.html#adaptive-gradient-methods",
    "href": "lectures/lecture6.html#adaptive-gradient-methods",
    "title": "Lecture 6: Adaptive Minimization Methods",
    "section": "5. Adaptive Gradient Methods",
    "text": "5. Adaptive Gradient Methods\nAdaptive methods dynamically adjust learning rates for each parameter based on their historical gradient information.\n5.1. AdaGrad: Parameter-Specific Learning Rates\nAdaGrad scales the learning rate inversely proportional to the accumulated squared gradients:\n\\[\\boldsymbol{\\beta}_{t+1} = \\boldsymbol{\\beta}_t - \\eta \\frac{\\mathbf{g}(\\boldsymbol{\\beta}_t)}{\\sqrt{\\mathbf{s}_t + \\epsilon}}\\]\nWhere:\n\n\\(\\mathbf{s}_t = \\sum_{h=1}^t \\mathbf{g}(\\boldsymbol{\\beta}_h)^2\\) (elementwise squaring)\n\\(\\epsilon\\) is a small constant to prevent division by zero\n\nThis approximates the diagonal of the Hessian, allowing larger updates for parameters with small gradients and smaller updates for parameters with large gradients. However, AdaGrad’s accumulation of squared gradients can cause learning rates to diminish too quickly, stalling progress.\n5.2. RMSprop: Exponential Moving Average\nRMSprop addresses AdaGrad’s diminishing learning rates by replacing the sum with an exponential moving average:\n\\[\\mathbf{s}_t = a \\mathbf{s}_{t-1} + (1-a) \\mathbf{g}(\\boldsymbol{\\beta}_t)^2\\]\nWhere \\(a \\in [0,1]\\) controls the decay rate of historical information.\n5.3. Adam: Combining Momentum and Adaptive Learning Rates\nAdam (Adaptive Moment Estimation) combines the benefits of momentum and adaptive learning rates:\n\\[\\mathbf{m}_t = b_1 \\mathbf{m}_{t-1} + (1-b_1) \\mathbf{g}(\\boldsymbol{\\beta}_t)\\] \\[\\mathbf{v}_t = b_2 \\mathbf{v}_{t-1} + (1-b_2) \\mathbf{g}(\\boldsymbol{\\beta}_t)^2\\] \\[\\boldsymbol{\\beta}_{t+1} = \\boldsymbol{\\beta}_t - \\eta \\frac{\\mathbf{m}_t}{\\sqrt{\\mathbf{v}_t + \\epsilon}}\\]\nCommon hyperparameter values are:\n\n\\(\\eta = 0.001\\) (initial learning rate)\n\\(b_1 = 0.9\\) (momentum decay)\n\\(b_2 = 0.999\\) (squared gradient decay)\n\\(\\epsilon = 10^{-8}\\) (numerical stability constant)"
  },
  {
    "objectID": "lectures/lecture6.html#practical-considerations",
    "href": "lectures/lecture6.html#practical-considerations",
    "title": "Lecture 6: Adaptive Minimization Methods",
    "section": "6. Practical Considerations",
    "text": "6. Practical Considerations\nThe choice of optimization method significantly impacts model performance:\n\nStandard SGD: Simple but requires careful learning rate tuning and may converge slowly\nMomentum: Faster convergence in flat regions but still sensitive to learning rate\nAdaGrad/RMSprop: Parameter-specific adaptation but may require tuning of decay rates\nAdam: Generally robust performance across a variety of problems, particularly in high-dimensional spaces\n\nThese adaptive methods form the foundation of modern deep learning optimization, enabling effective training across diverse architectures and problem domains, especially for models with numerous parameters and complex loss landscapes."
  },
  {
    "objectID": "lectures/lecture4.html",
    "href": "lectures/lecture4.html",
    "title": "Lecture 4: Linear Models: The Starting Point",
    "section": "",
    "text": "← Previous: Lecture 3\n\n\nNext: Lecture 5 →"
  },
  {
    "objectID": "lectures/lecture4.html#from-predictive-models-to-hypothesis-spaces",
    "href": "lectures/lecture4.html#from-predictive-models-to-hypothesis-spaces",
    "title": "Lecture 4: Linear Models: The Starting Point",
    "section": "1. From Predictive Models to Hypothesis Spaces",
    "text": "1. From Predictive Models to Hypothesis Spaces\nIn supervised learning, we seek to approximate an unknown function \\(f(\\mathbf{x})\\) that relates features to outcomes:\n\\[y = f(\\mathbf{x}) + \\epsilon\\]\nWhile the ideal goal is to minimize the true expected risk, this is generally impossible without knowing the true data-generating distribution. Instead, we work within a constrained hypothesis space \\(\\mathcal{H}\\) - the set of candidate functions we consider.\n1.1. Linear Models as a Hypothesis Space\nThe most common hypothesis space is the set of linear models, characterized as:\n\\[\\hat{y}_0 = g(\\phi(\\mathbf{x}_0)^T \\hat{\\boldsymbol{\\beta}})\\]\nWhere:\n\n\\(\\phi(\\mathbf{x})\\) is a feature extractor that can transform input features\n\\(g(\\cdot)\\) is a function mapping inputs to outputs\n\\(\\hat{\\boldsymbol{\\beta}}\\) are the model parameters to be learned\n\nAn important insight: linear models don’t necessarily produce linear decision boundaries. Through clever feature extraction, they can represent highly complex functions. Examples include polynomial expansions and kernel functions (polynomial and RBF kernels) that can capture non-linear patterns in data."
  },
  {
    "objectID": "lectures/lecture4.html#empirical-risk-minimization-framework",
    "href": "lectures/lecture4.html#empirical-risk-minimization-framework",
    "title": "Lecture 4: Linear Models: The Starting Point",
    "section": "2. Empirical Risk Minimization Framework",
    "text": "2. Empirical Risk Minimization Framework\nGiven that we can’t minimize the true risk directly, we resort to empirical risk minimization (ERM) using our observed training data:\n\\[\\hat{\\boldsymbol{\\beta}} = \\underset{\\boldsymbol{\\beta}^*}{\\text{argmin}} \\frac{1}{N} \\sum_{i=1}^{N} L(y_i, g(\\phi(\\mathbf{x}_i)^T \\boldsymbol{\\beta}^*))\\]\nSince empirical risk typically underestimates generalization error, regularization serves as a correction:\n\\[\\hat{\\boldsymbol{\\beta}} = \\underset{\\boldsymbol{\\beta}^*}{\\text{argmin}} \\frac{1}{N} \\sum_{i=1}^{N} L(y_i, g(\\phi(\\mathbf{x}_i)^T \\boldsymbol{\\beta}^*)) + \\lambda C(\\boldsymbol{\\beta}^*)\\]\nWhere \\(C(\\cdot)\\) measures model complexity and \\(\\lambda\\) is a tuning parameter that controls the strength of the penalty."
  },
  {
    "objectID": "lectures/lecture4.html#common-linear-models-and-their-losses",
    "href": "lectures/lecture4.html#common-linear-models-and-their-losses",
    "title": "Lecture 4: Linear Models: The Starting Point",
    "section": "3. Common Linear Models and Their Losses",
    "text": "3. Common Linear Models and Their Losses\nLinear models can be categorized based on outcome types and their corresponding loss functions:\n\nLinear Regression (Continuous Outcomes):\nUses mean squared error loss: \\[\\hat{\\boldsymbol{\\beta}} = \\underset{\\boldsymbol{\\beta}^*}{\\text{argmin}} \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\phi(\\mathbf{x}_i)^T \\boldsymbol{\\beta}^*)^2\\]\nBinary Logistic Regression (Binary Outcomes):\nUses probability loss with the logistic function: \\[\\sigma(z) = \\frac{\\exp[z]}{1 + \\exp[z]}\\]\nMultinomial Logistic Regression (Categorical Outcomes):\nUses probability loss with the softmax function: \\[\\sigma_k(\\mathbf{z}) = \\frac{\\exp[z_k]}{\\sum_{h=1}^{K} \\exp[z_h]}\\]"
  },
  {
    "objectID": "lectures/lecture4.html#maximum-likelihood-estimation",
    "href": "lectures/lecture4.html#maximum-likelihood-estimation",
    "title": "Lecture 4: Linear Models: The Starting Point",
    "section": "4. Maximum Likelihood Estimation",
    "text": "4. Maximum Likelihood Estimation\nThese common linear models can all be viewed through the lens of maximum likelihood estimation (MLE) from probability distributions in the exponential family.\n4.1. The Normal Distribution and Linear Regression\nFor linear regression, assuming normally distributed errors:\n\\[Pr(y_i | \\mathbf{x}_i, \\boldsymbol{\\beta}, \\sigma^2) = \\mathcal{N}(y_i | \\mathbf{x}_i^T \\boldsymbol{\\beta}, \\sigma^2)\\]\nThe negative log-likelihood simplifies to mean squared error plus a constant, showing that maximizing the likelihood for Gaussian regression is equivalent to minimizing MSE:\n\\[-\\ell\\ell(\\boldsymbol{\\beta}^* | \\mathbf{X}, \\mathbf{y}, \\sigma^2) = C + \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\mathbf{x}_i^T \\boldsymbol{\\beta})^2\\]\n4.2. The Bernoulli Distribution and Logistic Regression\nFor binary logistic regression, assuming Bernoulli-distributed outcomes:\n\\[Pr(y_i | \\mathbf{x}_i, \\boldsymbol{\\beta}) = \\theta_i^{y_i} (1 - \\theta_i)^{1-y_i}\\]\nWhere \\(\\theta_i = \\sigma(\\mathbf{x}_i^T \\boldsymbol{\\beta})\\) is the probability that \\(y_i = 1\\).\nUnlike linear regression, logistic regression has no analytical solution, necessitating gradient-based optimization methods."
  },
  {
    "objectID": "lectures/lecture4.html#regularization-for-linear-models",
    "href": "lectures/lecture4.html#regularization-for-linear-models",
    "title": "Lecture 4: Linear Models: The Starting Point",
    "section": "5. Regularization for Linear Models",
    "text": "5. Regularization for Linear Models\nRegularization improves model generalization by helping to bridge the gap between training error and generalization error.\n5.1. Common Regularization Approaches\nTwo primary norm-based penalties:\n\nL2 Regularization (Ridge Regression):\n\\[\\text{NLL} + \\lambda \\|\\boldsymbol{\\beta}\\|_2^2\\]\nL1 Regularization (LASSO):\n\\[\\text{NLL} + \\lambda \\|\\boldsymbol{\\beta}\\|_1\\]\n\n5.2. Effects of Regularization\nEmpirical examples demonstrate:\n\nAs \\(\\lambda\\) increases, coefficients shrink toward zero\nL1 regularization promotes sparsity (some coefficients become exactly zero)\nL2 regularization tends to distribute the penalty across all coefficients\nThere often exists an optimal \\(\\lambda\\) that minimizes generalization error\n\nConclusion:\nThis framework unifies linear models through empirical risk minimization and maximum likelihood estimation. Regularization techniques integrate into this framework to improve model generalization. This sets the foundation for optimization methods needed when analytical solutions don’t exist, particularly for logistic regression and other non-linear models."
  },
  {
    "objectID": "lectures/lecture28.html",
    "href": "lectures/lecture28.html",
    "title": "Lecture 28: Ethics and Fairness in Deep Learning and Modern Artificial Intelligence",
    "section": "",
    "text": "Increasing accessibility of AI systems (e.g., ChatGPT) to general public\nGrowing capability to create realistic-looking fake information\nPotential for harmful applications:\n\nMisinformation and synthetic media (deepfakes)\nPersonalized cyberbullying\nIdentity theft and catfishing\nPrivacy violations\n\nNeed for ethical frameworks to guide AI development and deployment\n\n\n\n\n\n\n\nCore principle: Maximize happiness, minimize harm for the greatest number\nApplication to AI: Evaluate based on broad societal outcomes\nStrengths:\n\nPractical and outcome-focused\nConsiders widespread impacts\nClear criterion (maximize welfare)\n\nWeaknesses:\n\nRequires quantification of harm/benefit\nMay sacrifice minority interests\nCan justify harmful means for “positive” ends\n\n\n\n\n\n\nCore principle: Follow universal ethical rules and duties, regardless of outcomes\nApplication to AI: Focus on respecting rights and following ethical constraints\nStrengths:\n\nProvides clear ethical boundaries\nProtects individual rights\nPromotes consistency\n\nWeaknesses:\n\nMay ignore beneficial outcomes\nDifficult to resolve conflicting duties\nCan be inflexible in complex situations\n\n\n\n\n\n\nCore principle: Develop moral character and virtues; act as a morally virtuous person would\nApplication to AI: Design systems that embody positive moral values\nStrengths:\n\nIntegrates moral reasoning beyond rules\nAcknowledges complexity of ethical decisions\nFocuses on intention and character\n\nWeaknesses:\n\nLess concrete guidance\nMore questions than answers\nSubjective interpretations\n\n\n\n\n\n\n\n\n\n\nAI should promote equity and treat individuals/groups without discrimination\nChallenges:\n\nAI systems amplify existing societal biases if unchecked\nBiased outcomes erode trust in AI systems\n\nPotential solutions:\n\nDatasheets for datasets (transparency)\nActive bias detection and mitigation strategies\nDiverse training data and development teams\n\n\n\n\n\n\nAI should produce content that accurately represents reality\nSystems should transparently disclose artificial origins\nChallenges:\n\nIncreasing capability creates more convincing falsehoods\nTension between complexity/capability and potential for misuse\nDifficulty differentiating AI-generated from human content\n\n\n\n\n\n\nAI should respect individual autonomy through explicit permission\nChallenges:\n\nDetermining appropriate consent mechanisms\nNavigating existing content (e.g., social media)\nIntellectual property concerns (e.g., training on copyrighted material)\n\nExamples:\n\nGitHub Copilot trained on public repositories containing proprietary code\nAI art generators replicating copyrighted artistic styles\n\n\n\n\n\n\nNo definitive answers to many ethical dilemmas in AI\nResponsibility falls to technically informed experts who understand the tools\nOngoing dialogue required as technology advances\n\n\n\n\n\n\n\n\n\n\nComplexity theory and VC dimensionality\nBias-variance tradeoffs\nBayesian statistics\nRegularization techniques\nUniversal approximation theory\nConvex optimization and stochastic minimization\n\n\n\n\n\nMultilayer perceptrons\nDeep network architectures\nActivation functions\nGeneralization methods\n\n\n\n\n\nConvolutional neural networks\nResidual networks and batch normalization\nModern CNN architectures\nObject detection with bounding boxes\nSemantic segmentation and U-Nets\n\n\n\n\n\nRecurrent neural networks\nLong short-term memory networks\nSequence-to-sequence models\nAttention mechanisms\nSelf-attention and transformers\nBERT and GPT architectures\n\n\n\n\n\nAutoregressive models\nVariational autoencoders\nGenerative adversarial networks\nDiffusion models\n\n\n\n\n\n\nAmbitious pace covering cutting-edge topics\nFocus on neural network theory alongside applications\nEmphasis on understanding components of complex models\nInsight that advanced models (transformers, stable diffusion) are combinations of simpler building blocks\n\n\n\n\n\nContinued refinement of course materials and approach\nAdjustment to homework structure for better learning outcomes\nTransition from PyTorch Lightning to base PyTorch\nOngoing development based on student feedback"
  },
  {
    "objectID": "lectures/lecture28.html#ethics-in-modern-ai",
    "href": "lectures/lecture28.html#ethics-in-modern-ai",
    "title": "Lecture 28: Ethics and Fairness in Deep Learning and Modern Artificial Intelligence",
    "section": "",
    "text": "Increasing accessibility of AI systems (e.g., ChatGPT) to general public\nGrowing capability to create realistic-looking fake information\nPotential for harmful applications:\n\nMisinformation and synthetic media (deepfakes)\nPersonalized cyberbullying\nIdentity theft and catfishing\nPrivacy violations\n\nNeed for ethical frameworks to guide AI development and deployment\n\n\n\n\n\n\n\nCore principle: Maximize happiness, minimize harm for the greatest number\nApplication to AI: Evaluate based on broad societal outcomes\nStrengths:\n\nPractical and outcome-focused\nConsiders widespread impacts\nClear criterion (maximize welfare)\n\nWeaknesses:\n\nRequires quantification of harm/benefit\nMay sacrifice minority interests\nCan justify harmful means for “positive” ends\n\n\n\n\n\n\nCore principle: Follow universal ethical rules and duties, regardless of outcomes\nApplication to AI: Focus on respecting rights and following ethical constraints\nStrengths:\n\nProvides clear ethical boundaries\nProtects individual rights\nPromotes consistency\n\nWeaknesses:\n\nMay ignore beneficial outcomes\nDifficult to resolve conflicting duties\nCan be inflexible in complex situations\n\n\n\n\n\n\nCore principle: Develop moral character and virtues; act as a morally virtuous person would\nApplication to AI: Design systems that embody positive moral values\nStrengths:\n\nIntegrates moral reasoning beyond rules\nAcknowledges complexity of ethical decisions\nFocuses on intention and character\n\nWeaknesses:\n\nLess concrete guidance\nMore questions than answers\nSubjective interpretations"
  },
  {
    "objectID": "lectures/lecture28.html#ethical-pillars-for-ai-development",
    "href": "lectures/lecture28.html#ethical-pillars-for-ai-development",
    "title": "Lecture 28: Ethics and Fairness in Deep Learning and Modern Artificial Intelligence",
    "section": "",
    "text": "AI should promote equity and treat individuals/groups without discrimination\nChallenges:\n\nAI systems amplify existing societal biases if unchecked\nBiased outcomes erode trust in AI systems\n\nPotential solutions:\n\nDatasheets for datasets (transparency)\nActive bias detection and mitigation strategies\nDiverse training data and development teams\n\n\n\n\n\n\nAI should produce content that accurately represents reality\nSystems should transparently disclose artificial origins\nChallenges:\n\nIncreasing capability creates more convincing falsehoods\nTension between complexity/capability and potential for misuse\nDifficulty differentiating AI-generated from human content\n\n\n\n\n\n\nAI should respect individual autonomy through explicit permission\nChallenges:\n\nDetermining appropriate consent mechanisms\nNavigating existing content (e.g., social media)\nIntellectual property concerns (e.g., training on copyrighted material)\n\nExamples:\n\nGitHub Copilot trained on public repositories containing proprietary code\nAI art generators replicating copyrighted artistic styles\n\n\n\n\n\n\nNo definitive answers to many ethical dilemmas in AI\nResponsibility falls to technically informed experts who understand the tools\nOngoing dialogue required as technology advances"
  },
  {
    "objectID": "lectures/lecture28.html#course-review-and-reflection",
    "href": "lectures/lecture28.html#course-review-and-reflection",
    "title": "Lecture 28: Ethics and Fairness in Deep Learning and Modern Artificial Intelligence",
    "section": "",
    "text": "Complexity theory and VC dimensionality\nBias-variance tradeoffs\nBayesian statistics\nRegularization techniques\nUniversal approximation theory\nConvex optimization and stochastic minimization\n\n\n\n\n\nMultilayer perceptrons\nDeep network architectures\nActivation functions\nGeneralization methods\n\n\n\n\n\nConvolutional neural networks\nResidual networks and batch normalization\nModern CNN architectures\nObject detection with bounding boxes\nSemantic segmentation and U-Nets\n\n\n\n\n\nRecurrent neural networks\nLong short-term memory networks\nSequence-to-sequence models\nAttention mechanisms\nSelf-attention and transformers\nBERT and GPT architectures\n\n\n\n\n\nAutoregressive models\nVariational autoencoders\nGenerative adversarial networks\nDiffusion models\n\n\n\n\n\n\nAmbitious pace covering cutting-edge topics\nFocus on neural network theory alongside applications\nEmphasis on understanding components of complex models\nInsight that advanced models (transformers, stable diffusion) are combinations of simpler building blocks\n\n\n\n\n\nContinued refinement of course materials and approach\nAdjustment to homework structure for better learning outcomes\nTransition from PyTorch Lightning to base PyTorch\nOngoing development based on student feedback"
  },
  {
    "objectID": "lectures/lecture26.html",
    "href": "lectures/lecture26.html",
    "title": "Lecture 26: Diffusion Models and Stable Diffusion",
    "section": "",
    "text": "← Previous: Lecture 25\n\n\nNext: Lecture 27 →"
  },
  {
    "objectID": "lectures/lecture26.html#overview-of-generative-models",
    "href": "lectures/lecture26.html#overview-of-generative-models",
    "title": "Lecture 26: Diffusion Models and Stable Diffusion",
    "section": "Overview of Generative Models",
    "text": "Overview of Generative Models\n\nComparison of Approaches\n\nAutoregressive models: Direct \\(P(\\mathbf{x})\\) computation, high quality, but slow training/generation, no latent code\nVAEs: Fast generation, rich latent codes, but lower bound optimization, blurry images\nGANs: Fast generation, assumption-free, good image editing, but minimal control, difficult training\n\n\n\nDiffusion Models Introduction\n\nCombination of VAE and autoregressive model concepts\nSequential process: add noise to images until purely random, then learn reverse mapping\nNew approach to generative modeling with high-quality results"
  },
  {
    "objectID": "lectures/lecture26.html#diffusion-model-theory",
    "href": "lectures/lecture26.html#diffusion-model-theory",
    "title": "Lecture 26: Diffusion Models and Stable Diffusion",
    "section": "Diffusion Model Theory",
    "text": "Diffusion Model Theory\n\nForward Process (Encoder)\n\nStart with image \\(\\mathbf{x} = \\mathbf{z}_0\\)\nDefine sequence of latent variables: \\(\\{\\mathbf{z}_0, \\mathbf{z}_1, ..., \\mathbf{z}_T\\}\\)\nForward diffusion step: \\(\\mathbf{z}_t = \\sqrt{1 - \\beta_t} \\mathbf{z}_{t-1} + \\sqrt{\\beta_t} \\epsilon_t\\)\nConditional distribution: \\(Q(\\mathbf{z}_t | \\mathbf{z}_{t-1}) = \\mathcal{N}(\\mathbf{z}_t | \\sqrt{1 - \\beta_t} \\mathbf{z}_{t-1}, \\beta_t \\mathcal{I})\\)\nMarkovian process: \\(P(\\mathbf{z}_1, \\mathbf{z}_2, ..., \\mathbf{z}_T | \\mathbf{z}_0) = \\prod_{t=1}^T P(\\mathbf{z}_t | \\mathbf{z}_{t-1})\\)\n\n\n\nDiffusion Kernel\n\nDirect computation from \\(\\mathbf{z}_0\\) to any time \\(t\\): \\(P(\\mathbf{z}_t | \\mathbf{z}_0) = \\mathcal{N}\\left(\\mathbf{z}_t | \\sqrt{\\tilde{\\beta}_t} \\mathbf{z}_0, (1 - \\tilde{\\beta}_t) \\mathcal{I}\\right)\\)\nWhere \\(\\tilde{\\beta}_t = \\prod_{s=1}^t (1 - \\beta_s)\\)\nMean approaches zero and variance approaches identity as \\(t \\to T\\)\nAll images converge to \\(\\mathbf{z}_T \\sim \\mathcal{N}(\\mathbf{0}, \\mathcal{I})\\)\n\n\n\nReverse Process (Decoder)\n\nGoal: Learn mapping from noise back to original image\nTarget: \\(P(\\mathbf{z}_{t-1} | \\mathbf{z}_t)\\) or \\(P(\\mathbf{z}_0 | \\mathbf{z}_t)\\)\nChallenge: Marginal distributions \\(P(\\mathbf{z}_t)\\) and \\(P(\\mathbf{z}_{t-1})\\) are intractable\nForward normal assumption doesn’t guarantee reverse normality\n\n\n\nTractable Reverse Conditional\n\nKnown conditional: \\(P(\\mathbf{z}_{t-1} | \\mathbf{z}_t, \\mathbf{z}_0)\\) (conditioned on original input)\nUsing Bayes’ rule and Markov property: \\(P(\\mathbf{z}_{t-1} | \\mathbf{z}_t, \\mathbf{z}_0) \\propto P(\\mathbf{z}_t | \\mathbf{z}_{t-1})P(\\mathbf{z}_{t-1} | \\mathbf{z}_0)\\)\nResult: \\(P(\\mathbf{z}_{t-1} | \\mathbf{z}_t, \\mathbf{z}_0) = \\mathcal{N}\\left(\\mathbf{z}_{t-1} | \\frac{1 - \\tilde{\\beta}_{t-1}}{1 - \\tilde{\\beta}_t} \\sqrt{1 - \\beta_t} \\mathbf{z}_t + \\frac{\\sqrt{\\tilde{\\beta}_{t-1}}\\beta_t}{1 - \\tilde{\\beta}_t} \\mathbf{z}_0, \\frac{\\beta_t (1 - \\tilde{\\beta}_{t-1})}{1 - \\tilde{\\beta}_t} \\mathcal{I}\\right)\\)"
  },
  {
    "objectID": "lectures/lecture26.html#training-diffusion-models",
    "href": "lectures/lecture26.html#training-diffusion-models",
    "title": "Lecture 26: Diffusion Models and Stable Diffusion",
    "section": "Training Diffusion Models",
    "text": "Training Diffusion Models\n\nApproximation Strategy\n\nApproximate reverse mapping without \\(\\mathbf{z}_0\\): \\(Q(\\mathbf{z}_{t-1} | \\mathbf{z}_t, \\boldsymbol{\\theta}_t) = \\mathcal{N}(\\mathbf{z}_{t-1} | g(\\mathbf{z}_t, \\boldsymbol{\\theta}_t), \\sigma^2_t \\mathcal{I})\\)\nObjective: \\(\\hat{\\boldsymbol{\\theta}}_{1,2,...,T} = \\underset{\\boldsymbol{\\theta}}{\\text{argmax}} \\left[\\sum_{i=1}^N \\log P(\\mathbf{x}_i | \\boldsymbol{\\theta}_{1,2,...,T})\\right]\\)\nIntractable marginal: \\(P(\\mathbf{x} | \\boldsymbol{\\theta}_{1,2,...,T}) = \\int P(\\mathbf{x}, \\mathbf{z}_1, ..., \\mathbf{z}_T | \\boldsymbol{\\theta}_{1,2,...,T}) d\\mathbf{z}_1 ... d\\mathbf{z}_T\\)\n\n\n\nEvidence Lower Bound (ELBO)\n\nApproximate posterior: \\(Q(\\mathbf{z}_{1...T} | \\mathbf{x})\\)\nELBO form: \\(E_Q[\\log P(\\mathbf{x} | \\mathbf{z}_1, \\boldsymbol{\\theta}_1)] - \\sum_{t=2}^T E_Q[D_{KL}[P(\\mathbf{z}_{t-1} | \\mathbf{z}_t, \\mathbf{x}) || Q(\\mathbf{z}_{t-1} | \\mathbf{z}_t, \\boldsymbol{\\theta}_t)]]\\)\n\n\n\nLoss Function\n\nAnalytical loss for KL divergence between two normals: \\(\\sum_{i=1}^N -\\log \\mathcal{N}(\\mathbf{x}_i | g(\\mathbf{z}_{i1}, \\boldsymbol{\\theta}_1), \\sigma^2_1 \\mathcal{I}) + \\sum_{t=2}^T \\frac{1}{2\\sigma^2_t} \\left\\| \\frac{1 - \\tilde{\\beta}_{t-1}}{1 - \\tilde{\\beta}_t} \\sqrt{1 - \\beta_t} \\mathbf{z}_{it} + \\frac{\\sqrt{\\tilde{\\beta}_{t-1}} \\beta_t}{1 - \\tilde{\\beta}_t} \\mathbf{x}_i - g_t[\\mathbf{z}_{it}, \\boldsymbol{\\theta}_t] \\right\\|^2\\)\nFirst term: reconstruction error\nSecond term: distance between target and approximated means"
  },
  {
    "objectID": "lectures/lecture26.html#implementation-considerations",
    "href": "lectures/lecture26.html#implementation-considerations",
    "title": "Lecture 26: Diffusion Models and Stable Diffusion",
    "section": "Implementation Considerations",
    "text": "Implementation Considerations\n\nArchitecture Challenges\n\nEach diffusion step requires separate parameters \\(\\boldsymbol{\\theta}_t\\)\nTraining different networks for each step is impractical\nNeed image-to-image prediction capability\n\n\n\nUNet with Time Embeddings\n\nSingle UNet architecture for all time steps\nPositional embeddings encode time information\nEach block represents a time step in diffusion process\nSelf-attention across all time points to relate diffusion steps\nTransformer-style architecture with encoder-side attention\n\n\n\nTraining Considerations\n\nLarge \\(T\\), small \\(\\beta\\): More steps with smaller noise increments improve quality\nComputational intensity: Requires significant GPU resources for training\nParallelization: Self-attention enables GPU parallelization\nPractical reparameterization: Often reformulated in terms of noise for easier training"
  },
  {
    "objectID": "lectures/lecture26.html#stable-diffusion",
    "href": "lectures/lecture26.html#stable-diffusion",
    "title": "Lecture 26: Diffusion Models and Stable Diffusion",
    "section": "Stable Diffusion",
    "text": "Stable Diffusion\n\nOverview\n\nText-to-image generation using diffusion models\nCombines multiple advances in generative and discriminative modeling\nInput: \\(N\\) images with \\(N\\) associated text prompts\nHigh-quality, high-resolution image generation from text\n\n\n\nArchitecture Components\n\nStep 1: VAE Compression\n\nEncode high-resolution image to smaller latent space (e.g., 32×32)\nStructured latent space as smaller image rather than vector\nReduces computational requirements while preserving essential information\nAddresses VAE blurriness through subsequent diffusion refinement\n\n\n\nStep 2: Forward Diffusion\n\nApply diffusion process to compressed latent representation\nHundreds of steps with small noise variance for training\nComputationally less intensive than full-resolution diffusion\n\n\n\nStep 3: Prompt Embeddings\n\nText prompt processing using pre-trained BERT model\n512 or 1024 dimensional embeddings\nEnables conditioning on textual descriptions\n\n\n\nStep 4: Denoising UNet\n\nCross-attention with prompt embeddings at each step\nSelf-attention across all diffusion time steps\nPositional embeddings for time dependencies\nHigh-parameter models: separate UNet for each time transition (resource-intensive)\n\n\n\nStep 5: VAE Decoder\n\nTransform low-resolution output back to high-resolution\nComplete the generation pipeline from text to final image\n\n\n\n\nTraining and Properties\n\nEnd-to-end backpropagation: Entire pipeline trainable as single network\nSuperior image quality: Matches or exceeds GAN performance\nEnhanced control: Text conditioning more flexible than conditional GANs\nResource requirements: Requires substantial GPU resources for training"
  },
  {
    "objectID": "lectures/lecture26.html#advantages-and-limitations",
    "href": "lectures/lecture26.html#advantages-and-limitations",
    "title": "Lecture 26: Diffusion Models and Stable Diffusion",
    "section": "Advantages and Limitations",
    "text": "Advantages and Limitations\n\nAdvantages\n\nHigh-quality generation: Superior image quality compared to many alternatives\nText conditioning: Natural integration of textual control\nTheoretical foundation: Well-grounded mathematical framework\nFlexibility: Fewer distributional assumptions than some alternatives\n\n\n\nLimitations\n\nComputational cost: Slower than GANs and VAEs\nResource requirements: Training requires extensive computational resources\nGeneration speed: Multiple denoising steps needed for generation\nComplexity: Multi-component architecture increases implementation complexity\n\n\n\nResearch Directions\n\nUnderstanding why transformer-style diffusion outperforms alternatives\nEfficiency improvements for computational requirements\nComparison with self-attention GANs\nApplications beyond image generation"
  },
  {
    "objectID": "lectures/lecture24.html",
    "href": "lectures/lecture24.html",
    "title": "Lecture 24: More on Variational Autoencoders and Introduction to Generative Adversarial Networks",
    "section": "",
    "text": "← Previous: Lecture 23\n\n\nNext: Lecture 25 →"
  },
  {
    "objectID": "lectures/lecture24.html#vae-extensions",
    "href": "lectures/lecture24.html#vae-extensions",
    "title": "Lecture 24: More on Variational Autoencoders and Introduction to Generative Adversarial Networks",
    "section": "VAE Extensions",
    "text": "VAE Extensions\n\nβ-VAEs (Beta Variational Autoencoders)\n\nModified training objective: \\(\\mathcal{L} = E_Q[\\log P(\\mathbf{x}|\\mathbf{z})] - \\beta \\cdot D_{KL}(Q(\\mathbf{z}|\\mathbf{x}) || P(\\mathbf{z}))\\)\nβ parameter controls trade-off between reconstruction quality and regularization\nβ &gt; 1: Stronger regularization, encourages disentangled representations\nβ &lt; 1: Emphasizes reconstruction, may lead to entangled latent factors\nBenefits: Improved disentanglement of latent factors, more interpretable representations\n\n\n\nImage Editing with VAEs\n\nLatent space interpolation: Smooth transitions between encoded images\nAttribute manipulation: Edit specific features by moving in learned latent directions\nArithmetic operations: Combine latent representations to transfer attributes\nProcess:\n\nEncode images to latent space: \\(\\mathbf{z} = \\text{Encoder}(\\mathbf{x})\\)\nPerform operations in latent space\nDecode modified latent vectors: \\(\\mathbf{x}' = \\text{Decoder}(\\mathbf{z}')\\)\n\n\n\n\nConditional VAEs (CVAEs)\n\nExtend VAEs to incorporate conditioning information \\(\\mathbf{c}\\)\nModified architecture:\n\nEncoder: \\(Q(\\mathbf{z}|\\mathbf{x}, \\mathbf{c})\\)\nDecoder: \\(P(\\mathbf{x}|\\mathbf{z}, \\mathbf{c})\\)\nPrior: \\(P(\\mathbf{z}|\\mathbf{c})\\) or unchanged \\(P(\\mathbf{z})\\)\n\nTraining objective: \\(\\mathcal{L} = E_Q[\\log P(\\mathbf{x}|\\mathbf{z}, \\mathbf{c})] - D_{KL}(Q(\\mathbf{z}|\\mathbf{x}, \\mathbf{c}) || P(\\mathbf{z}|\\mathbf{c}))\\)\nApplications: Class-conditional generation, attribute-specific sampling"
  },
  {
    "objectID": "lectures/lecture24.html#generative-adversarial-networks-gans",
    "href": "lectures/lecture24.html#generative-adversarial-networks-gans",
    "title": "Lecture 24: More on Variational Autoencoders and Introduction to Generative Adversarial Networks",
    "section": "Generative Adversarial Networks (GANs)",
    "text": "Generative Adversarial Networks (GANs)\n\nMotivation and Overview\n\nAlternative approach to generative modeling that avoids explicit density estimation\nUses adversarial training between two neural networks\nDoes not require inference networks or variational approximations\nFocuses on generating samples that are indistinguishable from real data\n\n\n\nDensity Ratio Perspective\n\nConsider distinguishing between two distributions: \\(P_{data}(\\mathbf{x})\\) and \\(P_{model}(\\mathbf{x})\\)\nOptimal classifier for this task: \\(f^*(\\mathbf{x}) = \\frac{P_{data}(\\mathbf{x})}{P_{data}(\\mathbf{x}) + P_{model}(\\mathbf{x})}\\)\nWhen distributions are identical: \\(P_{data}(\\mathbf{x}) = P_{model}(\\mathbf{x})\\), then \\(f^*(\\mathbf{x}) = 0.5\\)\nKey insight: Train classifier to distinguish real from generated data\n\n\n\nGAN Architecture\n\nGenerator network \\(G(\\mathbf{z}; \\boldsymbol{\\theta}_g)\\): Maps noise \\(\\mathbf{z} \\sim P_z(\\mathbf{z})\\) to data space\nDiscriminator network \\(D(\\mathbf{x}; \\boldsymbol{\\theta}_d)\\): Outputs probability that input is real data\nGenerator objective: Fool discriminator by generating realistic samples\nDiscriminator objective: Correctly classify real vs. generated samples\n\n\n\nMinimax Game Formulation\n\nTwo-player minimax game with value function: \\(\\min_G \\max_D V(D,G) = E_{\\mathbf{x} \\sim P_{data}(\\mathbf{x})}[\\log D(\\mathbf{x})] + E_{\\mathbf{z} \\sim P_z(\\mathbf{z})}[\\log(1 - D(G(\\mathbf{z})))]\\)\nDiscriminator maximization: \\(\\max_D \\{E_{\\mathbf{x} \\sim P_{data}}[\\log D(\\mathbf{x})] + E_{\\mathbf{z} \\sim P_z}[\\log(1 - D(G(\\mathbf{z})))]\\}\\)\nGenerator minimization: \\(\\min_G E_{\\mathbf{z} \\sim P_z}[\\log(1 - D(G(\\mathbf{z})))]\\)\n\n\n\nTraining Algorithm\n\nDiscriminator update: Train to maximize ability to distinguish real from fake\n\nSample mini-batch of real data \\(\\{\\mathbf{x}^{(1)}, ..., \\mathbf{x}^{(m)}\\}\\)\nSample mini-batch of noise \\(\\{\\mathbf{z}^{(1)}, ..., \\mathbf{z}^{(m)}\\}\\)\nUpdate discriminator by ascending gradient: \\(\\nabla_{\\boldsymbol{\\theta}_d} \\frac{1}{m} \\sum_{i=1}^m [\\log D(\\mathbf{x}^{(i)}) + \\log(1 - D(G(\\mathbf{z}^{(i)})))]\\)\n\nGenerator update: Train to minimize discriminator’s ability to detect fakes\n\nSample mini-batch of noise \\(\\{\\mathbf{z}^{(1)}, ..., \\mathbf{z}^{(m)}\\}\\)\nUpdate generator by descending gradient: \\(\\nabla_{\\boldsymbol{\\theta}_g} \\frac{1}{m} \\sum_{i=1}^m \\log(1 - D(G(\\mathbf{z}^{(i)})))\\)\n\n\n\n\nTheoretical Properties\n\nGlobal optimum: When \\(P_g = P_{data}\\), the global minimum is achieved with \\(D^*(\\mathbf{x}) = \\frac{1}{2}\\)\nConvergence: Under ideal conditions, the minimax game converges to Nash equilibrium\nMode collapse: Generator may learn to produce limited variety of samples\nTraining instability: Adversarial training can be difficult to stabilize\n\n\n\nPractical Considerations\n\nAlternative generator objective: \\(\\max_G E_{\\mathbf{z} \\sim P_z}[\\log D(G(\\mathbf{z}))]\\) (instead of minimizing \\(\\log(1-D(G(\\mathbf{z})))\\))\nProvides stronger gradients early in training when discriminator is confident\nCareful balance required between generator and discriminator training\nVarious techniques developed to improve training stability"
  },
  {
    "objectID": "lectures/lecture22.html",
    "href": "lectures/lecture22.html",
    "title": "Lecture 22: Bayesian Machine Learning",
    "section": "",
    "text": "← Previous: Lecture 21\n\n\nNext: Lecture 23 →"
  },
  {
    "objectID": "lectures/lecture22.html#bayesian-framework",
    "href": "lectures/lecture22.html#bayesian-framework",
    "title": "Lecture 22: Bayesian Machine Learning",
    "section": "Bayesian Framework",
    "text": "Bayesian Framework\n\nFundamentals\n\nTreats model parameters \\(\\boldsymbol{\\theta}\\) as random variables with probability distributions\nPosterior distribution defined via Bayes’ theorem: \\(f(\\boldsymbol{\\theta} | \\mathcal{D}) = \\frac{f(\\mathcal{D} | \\boldsymbol{\\theta})f(\\boldsymbol{\\theta})}{\\int f(\\mathcal{D} | \\boldsymbol{\\theta})f(\\boldsymbol{\\theta}) d\\boldsymbol{\\theta}}\\)\nKey components:\n\nLikelihood: \\(f(\\mathcal{D} | \\boldsymbol{\\theta})\\) - probability of data given parameters\nPrior: \\(f(\\boldsymbol{\\theta})\\) - initial beliefs about parameters\nMarginal likelihood: \\(\\int f(\\mathcal{D} | \\boldsymbol{\\theta})f(\\boldsymbol{\\theta}) d\\boldsymbol{\\theta}\\) - model evidence\nPosterior: \\(f(\\boldsymbol{\\theta} | \\mathcal{D})\\) - updated parameter beliefs after observing data\n\n\n\n\nNormal Distribution with Known Variance\n\nLikelihood: \\(f(\\mathbf{y} | \\mu, \\sigma^2) = \\prod_{i=1}^N \\mathcal{N}(y_i | \\mu, \\sigma^2)\\)\nPrior: \\(f(\\mu) \\sim \\mathcal{N}(\\mu | \\mu_0, \\sigma^2_0)\\)\nPosterior: \\(f(\\mu | \\mathbf{y}) \\sim \\mathcal{N}(\\mu | \\hat{\\mu}, \\hat{\\sigma}^2)\\)\n\n\\(\\hat{\\sigma}^2 = \\left(\\frac{N}{\\sigma^2} + \\frac{1}{\\sigma^2_0}\\right)^{-1}\\)\n\\(\\hat{\\mu} = \\hat{\\sigma}^2\\left(\\frac{1}{\\sigma^2}\\sum y_i + \\frac{1}{\\sigma^2_0}\\mu_0\\right)\\)\n\nProperties:\n\nWith diffuse prior (\\(\\sigma^2_0 \\to \\infty\\)), posterior mean converges to sample mean\nAs \\(N \\to \\infty\\), prior influence diminishes and posterior approaches MLE"
  },
  {
    "objectID": "lectures/lecture22.html#bayesian-linear-regression",
    "href": "lectures/lecture22.html#bayesian-linear-regression",
    "title": "Lecture 22: Bayesian Machine Learning",
    "section": "Bayesian Linear Regression",
    "text": "Bayesian Linear Regression\n\nModel Formulation\n\nLikelihood: \\(f(\\mathbf{y} | \\mathbf{X}, \\boldsymbol{\\beta}, \\sigma^2) \\sim \\mathcal{N}_N(\\mathbf{y} | \\mathbf{X}\\boldsymbol{\\beta}, \\sigma^2\\mathcal{I}_N)\\)\nPrior: \\(f(\\boldsymbol{\\beta}) \\sim \\mathcal{N}_P(\\boldsymbol{\\beta} | \\boldsymbol{\\mu}_0, \\boldsymbol{\\Sigma}_0)\\)\nPosterior: \\(f(\\boldsymbol{\\beta} | \\mathbf{X}, \\mathbf{y}, \\sigma^2) \\sim \\mathcal{N}_P(\\boldsymbol{\\beta} | \\hat{\\boldsymbol{\\mu}}, \\hat{\\boldsymbol{\\Sigma}})\\)\n\n\\(\\hat{\\boldsymbol{\\Sigma}} = \\left(\\frac{1}{\\sigma^2}\\mathbf{X}^T\\mathbf{X} + \\boldsymbol{\\Sigma}_0^{-1}\\right)^{-1}\\)\n\\(\\hat{\\boldsymbol{\\mu}} = \\hat{\\boldsymbol{\\Sigma}}\\left(\\frac{1}{\\sigma^2}\\mathbf{X}^T\\mathbf{y} + \\boldsymbol{\\Sigma}^{-1}_0\\boldsymbol{\\mu}_0\\right)\\)\n\n\n\n\nConnections to Regularization\n\nWith \\(\\boldsymbol{\\mu}_0 = \\mathbf{0}\\) and \\(\\boldsymbol{\\Sigma}_0 = \\tau^2\\mathcal{I}_P\\): \\(\\hat{\\boldsymbol{\\mu}} = (\\mathbf{X}^T\\mathbf{X} + \\lambda\\mathcal{I}_P)^{-1}\\mathbf{X}^T\\mathbf{y}\\)\nThis is equivalent to ridge regression with regularization parameter \\(\\lambda = \\frac{\\sigma^2}{\\tau^2}\\)\nL2 regularization (weight decay) is a Bayesian solution to the generalization problem\nBayesian models are inherently regularized\nPrior variance controls model complexity similar to regularization strength\n\n\n\nPosterior Summarization\n\nMaximum a posteriori (MAP) estimate: \\(\\hat{\\boldsymbol{\\theta}} = \\underset{\\boldsymbol{\\theta}}{\\text{argmax}} f(\\boldsymbol{\\theta} | \\mathcal{D})\\)\nFor normal posteriors, the MAP estimate is the posterior mean\nBayesian 95% credible intervals for normal posterior: \\(\\hat{\\boldsymbol{\\mu}}_j \\pm 1.96 \\hat{\\boldsymbol{\\Sigma}}_{j,j}\\)"
  },
  {
    "objectID": "lectures/lecture22.html#map-approximations",
    "href": "lectures/lecture22.html#map-approximations",
    "title": "Lecture 22: Bayesian Machine Learning",
    "section": "MAP Approximations",
    "text": "MAP Approximations\n\nLaplace Approximation\n\nApproximates posterior with a multivariate normal distribution\nParameters of approximation:\n\nMean: \\(\\hat{\\boldsymbol{\\mu}} = \\underset{\\boldsymbol{\\Theta}}{\\text{argmax}}\\log f(\\boldsymbol{\\Theta} | \\mathbf{X})\\)\nCovariance: \\(\\hat{\\boldsymbol{\\Sigma}} = -\\left[\\frac{\\partial^2 \\log f(\\hat{\\boldsymbol{\\mu}} | \\mathbf{X})}{\\partial \\boldsymbol{\\Theta} \\partial \\boldsymbol{\\Theta}}\\right]^{-1}\\)\n\nBernstein-von Mises theorem: posterior converges to this normal approximation as \\(N \\to \\infty\\)\nMinimizes Kullback-Leibler divergence between true posterior and normal approximation\n\n\n\nNon-Conjugate Priors\n\nLaplace prior example: \\(f(\\boldsymbol{\\beta}) = \\prod_{j=1}^P \\frac{1}{2b_0}\\exp\\left[-\\frac{|\\beta_j|}{b_0}\\right]\\)\nMAP estimation with Laplace prior leads to LASSO regression: \\(\\log f(\\boldsymbol{\\beta}, \\alpha | \\mathbf{X}, \\mathbf{y}, \\sigma^2) \\propto -\\left[\\sum_{i=1}^N (y_i - \\mathbf{x}_i^T\\boldsymbol{\\beta} - \\alpha)^2 + \\frac{2\\sigma^2}{b_0}\\sum_{j=1}^P |\\beta_j|\\right]\\)\nNo closed-form solution for LASSO, requires numerical optimization"
  },
  {
    "objectID": "lectures/lecture22.html#marginal-likelihood",
    "href": "lectures/lecture22.html#marginal-likelihood",
    "title": "Lecture 22: Bayesian Machine Learning",
    "section": "Marginal Likelihood",
    "text": "Marginal Likelihood\n\nModel Evidence\n\nThe denominator in Bayes’ theorem: \\(f(\\mathcal{D}) = \\int f(\\mathcal{D} | \\boldsymbol{\\theta})f(\\boldsymbol{\\theta}) d\\boldsymbol{\\theta}\\)\nMeasures how well model explains data, integrating over all possible parameter values\nServes as a Bayesian measure of generalization error\nApproximately equivalent to leave-one-out cross-validation\n\n\n\nComputation and Applications\n\nFor linear regression with normal prior: \\(\\int \\mathcal{N}_N(\\mathbf{y} | \\mathbf{X}\\boldsymbol{\\beta}, \\sigma^2\\mathcal{I}_N)\\mathcal{N}_P(\\boldsymbol{\\beta} | \\boldsymbol{\\mu}_0, \\boldsymbol{\\Sigma}_0)d\\boldsymbol{\\beta} = \\mathcal{N}_N(\\mathbf{y} | \\mathbf{X}\\boldsymbol{\\mu}_0, \\sigma^2\\mathcal{I}_N + \\mathbf{X}\\boldsymbol{\\Sigma}_0\\mathbf{X}^T)\\)\nWith Laplace approximation: \\(\\int f(\\mathbf{X} | \\boldsymbol{\\theta})f(\\boldsymbol{\\theta})d\\boldsymbol{\\theta} \\approx f(\\mathbf{y} | \\mathbf{X}, \\hat{\\boldsymbol{\\mu}})f(\\hat{\\boldsymbol{\\mu}})(2\\pi)^{P/2}\\text{det}[\\hat{\\boldsymbol{\\Sigma}}]N^{-P/2}\\)\nUsed for hyperparameter selection, e.g., finding optimal \\(\\lambda\\) in ridge regression"
  },
  {
    "objectID": "lectures/lecture22.html#posterior-predictive-distribution",
    "href": "lectures/lecture22.html#posterior-predictive-distribution",
    "title": "Lecture 22: Bayesian Machine Learning",
    "section": "Posterior Predictive Distribution",
    "text": "Posterior Predictive Distribution\n\nDefinition and Properties\n\nDistribution of predictions at new points, accounting for parameter uncertainty: \\(f(y_0 | \\mathbf{x}_0, \\mathbf{X}, \\mathbf{y}, \\hat{\\boldsymbol{\\theta}}) = \\int f(\\mathbf{y}_0 | \\mathbf{x}_0, \\mathbf{y}, \\mathbf{X}, \\hat{\\boldsymbol{\\theta}})f(\\hat{\\boldsymbol{\\theta}} | \\mathcal{D})d\\hat{\\boldsymbol{\\theta}}\\)\nFor linear regression with normal prior: \\(f(y_0 | \\mathbf{x}_0, \\sigma^2, \\mathbf{X}, \\mathbf{y}) = \\mathcal{N}(y_0 | \\hat{\\boldsymbol{\\mu}}^T\\mathbf{x}_0, \\sigma^2 + \\mathbf{x}_0^T\\hat{\\boldsymbol{\\Sigma}}\\mathbf{x}_0)\\)\nPrediction variance includes both inherent noise (\\(\\sigma^2\\)) and parameter uncertainty (\\(\\mathbf{x}_0^T\\hat{\\boldsymbol{\\Sigma}}\\mathbf{x}_0\\))"
  },
  {
    "objectID": "lectures/lecture20.html",
    "href": "lectures/lecture20.html",
    "title": "Lecture 20: Intro to Generative Models: Autoregressives and Autoencoders",
    "section": "",
    "text": "← Previous: Lecture 19\n\n\nNext: Lecture 21 →"
  },
  {
    "objectID": "lectures/lecture20.html#foundations-of-generative-modeling",
    "href": "lectures/lecture20.html#foundations-of-generative-modeling",
    "title": "Lecture 20: Intro to Generative Models: Autoregressives and Autoencoders",
    "section": "Foundations of Generative Modeling",
    "text": "Foundations of Generative Modeling\nGenerative modeling represents a fundamental shift from the predictive focus of supervised learning to modeling the intrinsic structure of data. While supervised learning aims to model \\(P(y|\\mathbf{x})\\), generative approaches target the joint distribution \\(P(\\mathbf{x})\\).\n\nGenerative vs. Discriminative Learning\nThe core distinction between these paradigms lies in their objectives:\n\nDiscriminative Models learn decision boundaries between classes\nGenerative Models learn the underlying data distribution\n\nThis distinction is crucial because modeling the full data distribution enables:\n\nSampling new instances that resemble training data\nEstimating the likelihood of observed data\nUncovering latent structure within data\nPerforming conditional generation given constraints\n\n\n\nChallenges in Generative Modeling\nGenerative modeling faces significant challenges, particularly for high-dimensional data:\n\nDimensionality: A 6000×4000 RGB image has approximately 72 million dimensions\nComplex Dependencies: Pixels/tokens exhibit intricate spatial and contextual relationships\nStructure Preservation: Generated outputs must maintain semantic coherence\nEvaluation: Success requires both perceptual quality and statistical fidelity\n\nDespite these challenges, the success of modern generative models demonstrates that capturing the statistical structure of complex data is possible, enabling the creation of realistic synthetic content across modalities."
  },
  {
    "objectID": "lectures/lecture20.html#autoregressive-models",
    "href": "lectures/lecture20.html#autoregressive-models",
    "title": "Lecture 20: Intro to Generative Models: Autoregressives and Autoencoders",
    "section": "Autoregressive Models",
    "text": "Autoregressive Models\nAutoregressive models provide a tractable approach to generative modeling by factorizing the joint distribution using the chain rule of probability.\n\nChain Rule Factorization\nFor a sequence \\(\\mathbf{x} = [x_1, x_2, ..., x_P]\\), the joint distribution can be expressed as:\n\\[P(\\mathbf{x}) = P(x_1) \\cdot P(x_2|x_1) \\cdot P(x_3|x_1,x_2) \\cdot ... \\cdot P(x_P|x_1,...,x_{P-1})\\]\nThis factorization transforms the complex task of modeling a high-dimensional joint distribution into a sequence of more manageable conditional distributions.\n\n\nText Generation with GPT\nGenerative Pre-trained Transformers (GPT) implement autoregressive modeling for text:\n\nUse masked self-attention to enforce the autoregressive constraint\nProcess the input sequence with transformer decoder blocks\nOutput a probability distribution over the next token\nSample from this distribution and append to the sequence\nRepeat until generation is complete\n\nGPT’s success in language generation comes from its ability to capture complex patterns in linguistic data through this autoregressive approach.\n\n\nImage Generation with PixelRNN/PixelCNN\nAutoregressive image generation follows similar principles but must define a spatial ordering for pixels:\n\nTypically process pixels in raster scan order (row by row from top-left)\nModel RGB values at each position conditioned on all previously generated pixels\nUse recurrent architectures (PixelRNN) or masked convolutions (PixelCNN) to preserve the autoregressive property\n\nThe hidden state update for PixelRNN can be expressed as:\n\\[\\mathbf{h}_{x,y} = f(\\mathbf{h}_{x-1,y}, \\mathbf{h}_{x,y-1})\\]\nWhere the function \\(f\\) is implemented using LSTM units to capture spatial dependencies.\n\n\nLimitations of Autoregressive Generation\nDespite their theoretical elegance, autoregressive models face significant practical limitations:\n\nSequential Generation: Sampling must proceed one element at a time\nComputational Intensity: Particularly problematic for high-dimensional data like images\nResolution Constraints: Practical implementations often limited to small images (e.g., 64×64)\nTraining Time: Requires extensive computational resources\n\nThese limitations have motivated the development of alternative approaches that can generate high-dimensional data more efficiently."
  },
  {
    "objectID": "lectures/lecture20.html#dimensionality-reduction-and-autoencoders",
    "href": "lectures/lecture20.html#dimensionality-reduction-and-autoencoders",
    "title": "Lecture 20: Intro to Generative Models: Autoregressives and Autoencoders",
    "section": "Dimensionality Reduction and Autoencoders",
    "text": "Dimensionality Reduction and Autoencoders\nAn alternative approach to generative modeling begins with dimensionality reduction techniques that learn compact representations of data.\n\nPrincipal Component Analysis\nPCA represents a classical approach to dimensionality reduction that finds a linear projection minimizing reconstruction error:\n\\[\\min_{\\mathbf{W}} \\frac{1}{N} \\sum_{i=1}^N \\|\\mathbf{x}_i - \\mathbf{W}\\mathbf{W}^T\\mathbf{x}_i\\|^2_2\\]\nWhere:\n\n\\(\\mathbf{W}\\) is a \\(P \\times K\\) matrix of orthonormal basis vectors\n\\(K \\ll P\\) is the dimensionality of the latent space\n\\(\\mathbf{z}_i = \\mathbf{W}^T\\mathbf{x}_i\\) represents the latent encoding\n\nPCA provides the optimal linear reconstruction under squared error, but its linearity severely restricts its expressive power for complex data like images or text.\n\n\nDeterministic Autoencoders\nAutoencoders generalize the dimensionality reduction framework by replacing linear projections with neural networks:\n\nEncoder \\(e(\\mathbf{x})\\): Maps input data to a low-dimensional latent code \\(\\mathbf{z}\\)\nBottleneck: Forces the model to learn a compressed representation\nDecoder \\(d(\\mathbf{z})\\): Reconstructs the original input from the latent code\n\nThe model is trained to minimize reconstruction error:\n\\[\\min_{e,d} \\frac{1}{N} \\sum_{i=1}^N \\|\\mathbf{x}_i - d(e(\\mathbf{x}_i))\\|^2_2\\]\nAutoencoders can leverage arbitrary neural architectures, including convolutional networks for images, enabling them to capture complex non-linear relationships in data.\n\n\nLimitations as Generative Models\nWhile autoencoders excel at representation learning, they have significant limitations as generative models:\n\nNo Explicit Density Model: They don’t define a probability distribution over data\nDiscontinuous Latent Space: Random points in latent space may decode to unrealistic samples\nNo Sampling Mechanism: No principled way to generate novel samples\nOverfitting Risk: May memorize training examples rather than learning data distribution\n\nThese limitations arise because standard autoencoders focus solely on reconstruction rather than learning a proper statistical model of data."
  },
  {
    "objectID": "lectures/lecture20.html#toward-probabilistic-generative-models",
    "href": "lectures/lecture20.html#toward-probabilistic-generative-models",
    "title": "Lecture 20: Intro to Generative Models: Autoregressives and Autoencoders",
    "section": "Toward Probabilistic Generative Models",
    "text": "Toward Probabilistic Generative Models\nTo overcome the limitations of deterministic autoencoders, we need to incorporate probabilistic elements.\n\nFactor Analysis\nFactor analysis represents an early probabilistic approach to dimensionality reduction:\n\\[P(\\mathbf{z}) = \\mathcal{N}(\\mathbf{z}|0, \\mathbf{I})\\] \\[P(\\mathbf{x}|\\mathbf{z}) = \\mathcal{N}(\\mathbf{x}|\\mathbf{W}\\mathbf{z} + \\boldsymbol{\\alpha}, \\boldsymbol{\\Psi})\\]\nThis formulation explicitly models:\n\nA prior distribution over latent variables\nA conditional distribution generating observations from latent variables\nUncertainty in the reconstruction process\n\nThis probabilistic perspective lays the foundation for more sophisticated models like variational autoencoders, which extend these ideas to deep neural networks while maintaining a principled probabilistic interpretation.\n\n\nToward Variational Autoencoders\nThe framework of factor analysis points toward variational autoencoders, which:\n\nReplace linear mappings with neural networks\nMaintain probabilistic interpretations of encodings and decodings\nEnable principled sampling and density estimation\nSupport conditional generation\n\nThis combination of deep learning with probabilistic modeling delivers both the flexibility of neural networks and the statistical rigor of generative models."
  },
  {
    "objectID": "lectures/lecture19.html",
    "href": "lectures/lecture19.html",
    "title": "Lecture 19: Representation Learning and Vision Transformers",
    "section": "",
    "text": "← Previous: Lecture 18\n\n\nNext: Lecture 20 →"
  },
  {
    "objectID": "lectures/lecture19.html#representation-learning-with-bert",
    "href": "lectures/lecture19.html#representation-learning-with-bert",
    "title": "Lecture 19: Representation Learning and Vision Transformers",
    "section": "Representation Learning with BERT",
    "text": "Representation Learning with BERT\nWhile generative transformer models like GPT focus on predicting the next token in a sequence, representation learning approaches the problem from a fundamentally different perspective.\n\nThe Representation Learning Paradigm\nRepresentation learning models like BERT (Bidirectional Encoder Representations from Transformers) aim to develop rich contextual understanding of inputs rather than generating continuations:\n\nFocus on capturing the meaning and relationships within data\nLeverage bidirectional context rather than unidirectional prediction\nCreate versatile representations that can be applied to multiple downstream tasks\nEmphasize understanding over generation\n\nThis approach is particularly valuable for tasks where complete context is available at inference time, such as text classification, entity recognition, or question answering.\n\n\nBERT Architecture and Training\nBERT uses the encoder portion of the transformer architecture with several key modifications:\n\nInput Structure: BERT uses special tokens to structure inputs:\n\n&lt;CLS&gt; token at the beginning, which aggregates sequence-level representations\n&lt;SEP&gt; tokens to separate different sentences within an input\n&lt;MASK&gt; tokens that replace words to be predicted during training\n\nPre-training Objectives: BERT is trained with two simultaneous tasks:\n\nMasked Language Modeling (MLM): Randomly mask 15% of input tokens and train the model to predict them based on bidirectional context\nNext Sentence Prediction (NSP): Predict whether two sentences appear consecutively in text\n\nInput Representation: Each token receives three types of embeddings:\n\nToken embeddings representing the word identity\nSegment embeddings indicating which sentence the token belongs to\nPosition embeddings encoding sequential position\n\nModel Variants:\n\nBERT-base: 12 layers, 768-dimensional hidden states, 12 attention heads (110M parameters)\nBERT-large: 24 layers, 1024-dimensional hidden states, 16 attention heads (340M parameters)\n\n\n\n\nTransfer Learning with BERT\nBERT’s power stems from its ability to transfer contextual language understanding to various downstream tasks:\n\nPre-train on massive text corpora (Wikipedia + BooksCorpus)\nFine-tune for specific tasks by:\n\nAdding task-specific layers on top of the pre-trained representations\nUpdating all parameters end-to-end with a small learning rate\nAchieving state-of-the-art performance with minimal task-specific data\n\n\nThis transfer learning approach dramatically reduces the computational resources needed for specific NLP tasks, as the general language understanding is already encoded in the pre-trained model."
  },
  {
    "objectID": "lectures/lecture19.html#vision-transformers",
    "href": "lectures/lecture19.html#vision-transformers",
    "title": "Lecture 19: Representation Learning and Vision Transformers",
    "section": "Vision Transformers",
    "text": "Vision Transformers\nThe success of transformers in NLP has inspired their application to computer vision, challenging the dominance of convolutional neural networks.\n\nAdapting Transformers for Images\nVision Transformers (ViT) apply self-attention mechanisms to image data, but face a fundamental challenge: images contain significantly more “tokens” than text, with each pixel potentially representing an individual token.\nThe key innovation in ViT addresses this challenge through:\n\nPatch-based Tokenization: Instead of treating individual pixels as tokens:\n\nDivide the image into fixed-size patches (typically 16×16 pixels)\nFlatten each patch into a vector\nProject these vectors to the transformer’s working dimension\n\nClass Token: Similar to BERT’s &lt;CLS&gt; token, a learnable embedding is prepended to the sequence of patch embeddings to aggregate information for classification.\nPosition Embeddings: Learnable position embeddings are added to patch embeddings to retain spatial information lost in the flattening process.\n\n\n\nViT Architecture\nThe ViT architecture closely follows the transformer encoder:\n\nInput image is divided into patches and linearly embedded\nPosition embeddings are added to patch embeddings\nThe resulting sequence is processed through multiple transformer encoder blocks:\n\nMulti-head self-attention\nMLP blocks\nLayer normalization and residual connections\n\nThe final representation of the class token is fed into a classification head\n\n\n\nPerformance and Scaling Properties\nVision Transformers demonstrate several important properties:\n\nThey perform competitively with CNNs only when trained on large datasets\nThey scale very efficiently with more data and model size\nThey require fewer computational resources at comparable performance levels\nThey lack the inductive biases of CNNs (locality, translation equivariance)\nThey can capture long-range dependencies more efficiently than CNNs\n\nRecent developments like Swin Transformers combine the strengths of both approaches by incorporating local attention windows that progressively merge, similar to how CNNs hierarchically process visual information."
  },
  {
    "objectID": "lectures/lecture19.html#autoregressive-generation",
    "href": "lectures/lecture19.html#autoregressive-generation",
    "title": "Lecture 19: Representation Learning and Vision Transformers",
    "section": "Autoregressive Generation",
    "text": "Autoregressive Generation\nAutoregressive generation represents a powerful approach to modeling complex data distributions by factorizing them as products of conditional probabilities.\n\nPrinciples of Autoregressive Modeling\nThe core insight of autoregressive models stems from the chain rule of probability:\n\\[P(\\mathbf{x}) = P(x_1) \\cdot P(x_2|x_1) \\cdot P(x_3|x_1,x_2) \\cdot ... \\cdot P(x_n|x_1,...,x_{n-1})\\]\nThis allows modeling a joint distribution as a sequence of conditional distributions, where each element depends on all previous elements. For generation:\n\nSample first element from \\(P(x_1)\\)\nSample second element from \\(P(x_2|x_1)\\)\nContinue until the entire sequence is generated\n\nThis approach provides explicit likelihood modeling and allows direct sampling from the learned distribution.\n\n\nLanguage Generation with GPT\nGPT (Generative Pre-trained Transformer) implements autoregressive modeling for text:\n\nUses masked self-attention to prevent looking at future tokens\nPredicts probability distribution over next token given all previous tokens\nScales effectively with more data and parameters\nCan be primed with initial text to generate contextually relevant continuations\n\nGPT’s masked self-attention mechanism is perfectly suited for autoregressive modeling, as it naturally implements the conditional probability structure required.\n\n\nImage Generation with PixelRNN/PixelCNN\nAutoregressive models can also generate images by treating them as sequences of pixels:\n\nSequence Definition: Define an ordering of pixels (typically row by row, from top-left)\nContext Modeling: Model each pixel as dependent on all previous pixels\nNetwork Architecture: Use RNNs (PixelRNN) or masked convolutions (PixelCNN) to capture dependencies\n\nPixelRNN specifically uses LSTM units to model the conditional distribution of each pixel:\n\\[\\mathbf{h}_{x,y} = f(\\mathbf{h}_{x-1,y}, \\mathbf{h}_{x,y-1})\\] \\[P(\\mathbf{x}_{x,y}|\\mathbf{x}_{&lt;(x,y)}) = g(\\mathbf{h}_{x,y})\\]\nWhere \\(\\mathbf{h}_{x,y}\\) is the hidden state for position \\((x,y)\\) and \\(g\\) is a function that outputs a distribution over pixel values.\n\n\nLimitations of Autoregressive Generation\nWhile theoretically elegant, autoregressive models face practical challenges:\n\nSequential Generation: Generating samples is inherently sequential and cannot be parallelized\nComputational Complexity: For high-dimensional data like images, generation becomes extremely slow\nLimited Resolution: Practical applications for images are often limited to small resolutions (e.g., 64×64)\nLong-range Dependencies: Capturing dependencies between distant elements remains challenging\n\nThese limitations have motivated alternative approaches to generative modeling, such as variational autoencoders and generative adversarial networks, which offer different tradeoffs between likelihood modeling, sampling efficiency, and generation quality."
  },
  {
    "objectID": "lectures/lecture17.html",
    "href": "lectures/lecture17.html",
    "title": "Lecture 17: Sequence Models and Attention",
    "section": "",
    "text": "← Previous: Lecture 16\n\n\nNext: Lecture 18 →"
  },
  {
    "objectID": "lectures/lecture17.html#sequence-to-sequence-modeling",
    "href": "lectures/lecture17.html#sequence-to-sequence-modeling",
    "title": "Lecture 17: Sequence Models and Attention",
    "section": "Sequence-to-Sequence Modeling",
    "text": "Sequence-to-Sequence Modeling\nSequence-to-sequence (Seq2Seq) models represent a powerful framework for transforming one sequence into another, with applications across numerous domains.\n\nArchitecture Overview\nThe standard Seq2Seq architecture consists of two primary components:\n\nEncoder: Processes the input sequence and compresses its information into context vectors\n\nTypically implemented as an RNN (often LSTM) that reads the input sequence\nProduces hidden states at each step, with the final hidden state serving as the “context” vector\n\nDecoder: Generates the output sequence based on the encoded information\n\nAlso implemented as an RNN that generates one output element at a time\nTakes the previous output element and previous hidden state as inputs\nInitialized with the final encoder hidden state (context vector)\n\n\nThis encoder-decoder framework provides a flexible approach for mapping arbitrary-length input sequences to arbitrary-length output sequences.\n\n\nBottleneck Problem\nDespite its elegance, the standard Seq2Seq architecture suffers from a fundamental limitation:\n\nAll information from the input sequence must be compressed into a fixed-length context vector\nFor long sequences, this creates an information bottleneck\nThe model struggles to retain details from early parts of the input sequence\nPerformance degrades as sequence length increases\n\nFor example, when translating a long sentence, information about the first few words may be lost by the time the encoder processes the entire input."
  },
  {
    "objectID": "lectures/lecture17.html#attention-mechanisms",
    "href": "lectures/lecture17.html#attention-mechanisms",
    "title": "Lecture 17: Sequence Models and Attention",
    "section": "Attention Mechanisms",
    "text": "Attention Mechanisms\nAttention mechanisms address the bottleneck problem by allowing the decoder to focus on different parts of the input sequence at each decoding step.\n\nIntuition and Motivation\nAttention draws inspiration from human cognition:\n\nWhen translating a sentence, humans don’t process the entire source sentence at once\nInstead, they focus on specific words or phrases relevant to the current word being translated\nAttention allows neural networks to implement a similar focusing mechanism\n\n\n\nAttention in Seq2Seq Models\nIn attention-augmented Seq2Seq models:\n\nThe encoder still processes the entire input sequence\nHowever, instead of using only the final hidden state, all encoder hidden states are preserved\nAt each decoding step, the decoder computes a weighted sum of these encoder states\nThe weights determine how much “attention” to pay to each input position\n\nThis creates direct pathways between each decoder step and all encoder positions, effectively bypassing the bottleneck.\n\n\nComputing Attention Weights\nAttention scores between a decoder state and encoder states are computed through various mechanisms:\n\nBahdanau Attention (Additive Attention): \\[e_{ti} = \\mathbf{w}_2^T \\odot \\tanh(\\mathbf{W}_1 [\\mathbf{h}_i, \\mathbf{s}_{t-1}])\\]\n\nConcatenates encoder and decoder states\nPasses them through a small neural network\nLearns alignment through trainable parameters \\(\\mathbf{W}_1\\) and \\(\\mathbf{w}_2\\)\n\nScaled Dot-Product Attention: \\[e_{ti} = \\frac{\\mathbf{h}_i^T \\mathbf{W}_g \\mathbf{s}_{t-1}}{\\sqrt{m}}\\]\n\nComputes similarity through dot product of transformed vectors\nScaling by \\(\\sqrt{m}\\) prevents dot products from growing too large in magnitude\nMore computationally efficient than additive attention\n\n\nThe attention weights are then normalized using softmax: \\[a_{ti} = \\frac{\\exp(e_{ti})}{\\sum_j \\exp(e_{tj})}\\]\n\n\nContext Vector Computation\nThe context vector for each decoder step is computed as a weighted sum of encoder hidden states:\n\\[\\mathbf{c}_t = \\sum_{i=1}^{T_e} a_{ti} \\mathbf{h}_i\\]\nThis context vector is then used as an additional input to the decoder:\n\\[\\mathbf{s}_t = f(\\mathbf{y}_{t-1}, \\mathbf{s}_{t-1}, \\mathbf{c}_t)\\] \\[\\mathbf{y}_t = g(\\mathbf{s}_t, \\mathbf{y}_{t-1}, \\mathbf{c}_t)\\]\nWhere \\(f\\) and \\(g\\) are the state update and output functions, respectively.\n\n\nVisualizing Attention\nAttention weights can be visualized as a matrix where each cell represents how much the decoder focuses on a particular encoder position when generating a specific output:\n\nRows correspond to output positions\nColumns correspond to input positions\nBrighter cells indicate higher attention weights\n\nIn language translation, attention weights often reveal alignment between source and target words, capturing:\n\nWord reordering across languages\nOne-to-many and many-to-one word correspondences\nRelevant contextual information beyond direct translations"
  },
  {
    "objectID": "lectures/lecture17.html#query-key-value-framework",
    "href": "lectures/lecture17.html#query-key-value-framework",
    "title": "Lecture 17: Sequence Models and Attention",
    "section": "Query-Key-Value Framework",
    "text": "Query-Key-Value Framework\nThe attention mechanism can be generalized using the Query-Key-Value (QKV) framework, which provides a more flexible way to think about attention.\n\nCore Components\n\nQuery (Q): The vector representing the current decoder state\nKey (K): The vectors against which the query is compared (encoder states)\nValue (V): The vectors that are weighted to produce the context (typically the same as keys)\n\n\n\nAttention Layer Computation\nThe attention layer performs the following operations:\n\nTransform input vectors into keys and values: \\[\\mathbf{K} = \\mathbf{X}\\mathbf{W}_K\\] \\[\\mathbf{V} = \\mathbf{X}\\mathbf{W}_V\\]\nCompute similarity scores between queries and keys: \\[\\mathbf{E} = \\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{m}}\\]\nNormalize scores to obtain attention weights: \\[\\mathbf{A} = \\text{softmax}(\\mathbf{E})\\]\nCompute weighted values: \\[\\mathbf{Y} = \\mathbf{A}\\mathbf{V}\\]\n\nThis framework generalizes attention beyond the encoder-decoder context, allowing for applications in various network architectures."
  },
  {
    "objectID": "lectures/lecture17.html#beyond-sequential-processing",
    "href": "lectures/lecture17.html#beyond-sequential-processing",
    "title": "Lecture 17: Sequence Models and Attention",
    "section": "Beyond Sequential Processing",
    "text": "Beyond Sequential Processing\nThe QKV framework suggests that attention can operate without requiring sequential processing of inputs:\n\nTraditional RNNs must process inputs sequentially due to their recurrent nature\nAttention can theoretically compute weights for all positions simultaneously\nHowever, standard attention still lacks positional information\n\nThis insight leads to several questions:\n\nCan we replace sequential encoding entirely with attention-based mechanisms?\nHow do we incorporate positional information without sequential processing?\nCan attention look at relationships between input elements themselves?\n\nThese questions set the stage for self-attention mechanisms and transformer architectures, where attention becomes the primary computational mechanism rather than an augmentation of recurrent networks."
  },
  {
    "objectID": "lectures/lecture15.html",
    "href": "lectures/lecture15.html",
    "title": "Lecture 15: Semantic Segmentation",
    "section": "",
    "text": "← Previous: Lecture 14\n\n\nNext: Lecture 16 →"
  },
  {
    "objectID": "lectures/lecture15.html#semantic-segmentation",
    "href": "lectures/lecture15.html#semantic-segmentation",
    "title": "Lecture 15: Semantic Segmentation",
    "section": "Semantic Segmentation",
    "text": "Semantic Segmentation\nSemantic segmentation represents a pixel-level classification task in computer vision, where each pixel in an image is assigned a semantic class label.\n\nTask Definition\nUnlike object detection that provides bounding boxes around objects, semantic segmentation:\n\nClassifies every pixel in the image\nDoes not differentiate between instances of the same class (all cows are labeled as “cow”)\nIncludes both foreground objects and background elements\n\nThe output is a label map of the same dimensions as the input image, where each pixel value corresponds to a class index.\n\n\nNaive Approach and Limitations\nA straightforward approach might attempt to:\n\nApply a standard CNN backbone\nUse fully connected layers to classify each pixel independently\n\nThis approach presents critical limitations:\n\nComputationally expensive when processing high-resolution images\nFails to effectively utilize spatial context and neighborhood relationships\nLoses localization information during downsampling"
  },
  {
    "objectID": "lectures/lecture15.html#u-net-architecture",
    "href": "lectures/lecture15.html#u-net-architecture",
    "title": "Lecture 15: Semantic Segmentation",
    "section": "U-Net Architecture",
    "text": "U-Net Architecture\nU-Net represents a specialized encoder-decoder architecture that addresses the challenges of semantic segmentation.\n\nEncoder-Decoder Framework\nThe U-Net architecture consists of two primary components:\nEncoder Path (Contracting):\n\nProgressively reduces spatial dimensions through downsampling\nIncreases feature channels to capture abstract representations\nFollows traditional CNN pattern of convolution + pooling\n\nDecoder Path (Expanding):\n\nProgressively increases spatial dimensions through upsampling\nDecreases feature channels to approach segmentation map\nUses transposed convolutions to recover spatial information\n\n\n\nSkip Connections\nThe defining feature of U-Net is its skip connections that:\n\nConnect corresponding layers between encoder and decoder paths\nPreserve high-resolution spatial information lost during downsampling\nConcatenate feature maps from encoding path to decoding path\nForm the characteristic “U” shape in the architecture diagram\n\n\n\nTransposed Convolution for Upsampling\nTransposed convolution (sometimes called deconvolution) enables learnable upsampling:\n\\[\\mathbf{X} \\circledast^{-1} \\mathbf{K} \\rightarrow \\text{larger output}\\]\nKey properties include:\n\nInverse operation to standard convolution with stride &gt; 1\nExpands input dimensions rather than contracting them\nWeights are learned during training, unlike fixed interpolation methods\nStride in transposed convolution refers to output matrix spacing\n\nConceptually, transposed convolution can be viewed as a learnable interpolation method that reconstructs higher-resolution features from compressed representations.\n\n\nTraining and Loss Functions\nU-Net training typically employs:\n\nPixel-wise cross-entropy loss for classification accuracy\nIntersection over Union (IoU) metrics for evaluation: \\[IoU = \\frac{\\text{True Positives}}{\\text{True Positives + False Positives + False Negatives}}\\]"
  },
  {
    "objectID": "lectures/lecture15.html#instance-segmentation",
    "href": "lectures/lecture15.html#instance-segmentation",
    "title": "Lecture 15: Semantic Segmentation",
    "section": "Instance Segmentation",
    "text": "Instance Segmentation\nInstance segmentation extends semantic segmentation by distinguishing individual instances of objects.\n\nTask Definition\nFor each object instance in the image:\n\nIdentify the class label (as in object detection)\nGenerate a pixel-perfect mask (as in semantic segmentation)\nAssign a unique identifier to each instance\n\nUnlike semantic segmentation, instance segmentation:\n\nDifferentiates between multiple instances of the same class\nFocuses primarily on foreground objects rather than background\nRequires both classification and boundary delineation\n\n\n\nMask R-CNN\nMask R-CNN extends Faster R-CNN by adding a branch for predicting segmentation masks:\n\nUses a standard object detection backbone to identify regions of interest\nFor each region, generates both class predictions and bounding boxes\nAdds a parallel branch that predicts a binary mask for each detected object\n\n\n\nSegment Anything Model (SAM)\nMeta’s Segment Anything Model (SAM) represents a recent advancement in instance segmentation:\n\nTrained on an unprecedented dataset of 11 million images with 1.1 billion masks\nFunctions as a foundation model for segmentation tasks\nCan identify object “blobs” without specific class labels\nSupports interactive prompting (e.g., point-based queries)\nDesigned for zero-shot transfer to new segmentation tasks\n\nSAM demonstrates how large-scale pretraining can create universal segmentation capabilities across diverse visual domains."
  },
  {
    "objectID": "lectures/lecture15.html#generative-applications-of-cnns",
    "href": "lectures/lecture15.html#generative-applications-of-cnns",
    "title": "Lecture 15: Semantic Segmentation",
    "section": "Generative Applications of CNNs",
    "text": "Generative Applications of CNNs\nCNNs can be inverted to generate images rather than classify them, shifting from discriminative models \\(P(y|x)\\) to generative models \\(P(x|y)\\).\n\nBayesian Framework\nThis inversion uses Bayes’ theorem:\n\\[P(x|y=c) = \\frac{1}{Z}P(y=c|x)P(x)\\]\nWhere:\n\n\\(P(y=c|x)\\) is provided by the CNN classifier (likelihood)\n\\(P(x)\\) is an image prior\n\\(Z\\) is a normalizing constant\n\n\n\nLangevin Dynamics for Image Generation\nThe unadjusted Langevin algorithm enables sampling from the posterior by iteratively updating:\n\\[x_{t+1} = x_t + \\epsilon_1 \\frac{\\partial \\log P(x_t)}{\\partial x_t} + \\epsilon_2 \\frac{\\partial \\log P(y=c|x_t)}{\\partial x_t}\\]\nKey components include:\n\nGradient of the CNN classifier with respect to input\nGradient of the image prior\nStep sizes \\(\\epsilon_1\\) and \\(\\epsilon_2\\) to balance these influences\n\n\n\nTotal Variation Prior\nA common differentiable image prior is the total variation (TV) prior:\n\\[TV(x) = \\sum_{i,j,k} (x_{i,j,k} - x_{i+1,j,k})^2 + (x_{i,j,k} - x_{i,j+1,k})^2\\]\nThis prior:\n\nEncourages smoothness between adjacent pixels\nPenalizes sharp transitions and noise\nBalances the classifier’s tendency to generate exaggerated features\n\n\n\nCreative Applications\nWhile computationally intensive, this approach has enabled creative applications:\n\nDeepDream: Amplifying patterns recognized by CNN layers\nNeural Style Transfer: Combining content from one image with style from another\n\nThese techniques demonstrate how CNNs can not only analyze visual data but also generate novel visual content by leveraging learned representations."
  },
  {
    "objectID": "lectures/lecture13.html",
    "href": "lectures/lecture13.html",
    "title": "Lecture 13: CNN Architectures and Transfer Learning",
    "section": "",
    "text": "← Previous: Lecture 12\n\n\nNext: Lecture 14 →"
  },
  {
    "objectID": "lectures/lecture13.html#cnn-architecture-design-principles",
    "href": "lectures/lecture13.html#cnn-architecture-design-principles",
    "title": "Lecture 13: CNN Architectures and Transfer Learning",
    "section": "CNN Architecture Design Principles",
    "text": "CNN Architecture Design Principles\nConvolutional Neural Networks (CNNs) require careful architectural design to achieve optimal performance on complex image classification tasks. Several key design principles have emerged from empirical research:\n\nFeature Map Progression\nEffective CNN architectures follow a consistent pattern:\n\nStart with rich input representations (e.g., 3×32×32 for RGB images)\nApply successive convolution and pooling layers to downsample spatial dimensions\nCompensate for downsampling by increasing the number of filters\nPreserve approximate volume through the network (H×W×C remains roughly constant)\n\nFor example, a typical progression might transform:\n\nInput: 3×32×32 (3,072 values)\nEarly layers: 64×16×16 (16,384 values)\nMiddle layers: 128×8×8 (8,192 values)\nDeep layers: 256×4×4 (4,096 values)\n\n\n\nBatch Normalization\nBatch normalization is essential for training deep CNNs, helping to address vanishing gradients. For each channel in the feature maps:\n\\[\\mu_c = \\frac{1}{N \\times H \\times W} \\sum_{i,j,k} x_{i,j,k}\\]\n\\[\\sigma^2_c = \\frac{1}{N \\times H \\times W} \\sum_{i,j,k} (x_{i,j,k} - \\mu_c)^2\\]\n\\[\\hat{x}_{i,j,k} = \\frac{x_{i,j,k} - \\mu_c}{\\sqrt{\\sigma^2_c + \\epsilon}}\\]\n\\[y_{i,j,k} = \\gamma_c \\hat{x}_{i,j,k} + \\delta_c\\]\nWhere:\n\n\\(\\gamma_c\\) and \\(\\delta_c\\) are learnable parameters\nNormalization layers typically follow convolution layers\nThis process stabilizes and accelerates training"
  },
  {
    "objectID": "lectures/lecture13.html#advanced-cnn-architectures",
    "href": "lectures/lecture13.html#advanced-cnn-architectures",
    "title": "Lecture 13: CNN Architectures and Transfer Learning",
    "section": "Advanced CNN Architectures",
    "text": "Advanced CNN Architectures\nAs image recognition tasks became increasingly complex, CNN architectures evolved significantly to address various challenges.\n\nVGG Architecture\nThe VGG architecture introduced systematic design principles:\n\nUse consistent 3×3 convolutions with stride 1\nApply 2×2 max pooling with stride 2\nDouble the number of filters after each pooling operation\n\nThis systematic approach made networks deeper while maintaining computational efficiency, with improved performance over earlier architectures like AlexNet.\n\n\nAddressing Overfitting\nTwo primary techniques help CNNs generalize better:\n\nDropout: Randomly deactivating connections during training:\n\nForces the network to learn redundant representations\nCreates an implicit ensemble of networks\nSignificantly improves generalization performance\n\nData Augmentation: Systematically creating variations of training images:\n\nFlipping (horizontal/vertical)\nRotation and scaling\nColor jittering\nRandom cropping\n\nThese transformations help the model learn invariant features and generalize to unseen images.\n\n\n\nResidual Networks (ResNets)\nA breakthrough in deep CNN architecture, ResNets introduced skip connections that bypass layers:\n\\[H(x) = F(x) + x\\]\nThis simple modification has profound effects on gradient flow:\nTraditional CNN gradient: \\[\\frac{\\partial \\mathcal{L}}{\\partial x} = \\frac{\\partial \\mathcal{L}}{\\partial x_k} \\cdot \\prod_{i=1}^{k} \\left( \\phi_i'(z_i) \\, W_i \\right)\\]\nResNet gradient: \\[\\frac{\\partial \\mathcal{L}}{\\partial x} = \\frac{\\partial \\mathcal{L}}{\\partial x_n} \\cdot \\prod_{i=1}^{k} \\left( \\mathcal{I} + J_{F_i}(x_i) \\right)\\]\nThe identity matrix term prevents gradient vanishing, allowing for much deeper networks:\n\nEven if \\(J_{F_i}(x_i)\\) has small eigenvalues, gradients can still flow\nEach residual block learns an incremental transformation\nNetworks can reach hundreds of layers while still training effectively\n\nResNets also employ global average pooling instead of fully connected layers, where each feature map is reduced to a single value by averaging, significantly reducing parameters while maintaining performance."
  },
  {
    "objectID": "lectures/lecture13.html#visualizing-cnn-features",
    "href": "lectures/lecture13.html#visualizing-cnn-features",
    "title": "Lecture 13: CNN Architectures and Transfer Learning",
    "section": "Visualizing CNN Features",
    "text": "Visualizing CNN Features\nUnderstanding what features CNNs extract helps interpret their behavior:\n\nExemplar Approach: Finding images that maximally activate specific filters\n\nCompute activations for all images at a given filter\nIdentify images with the highest activation values\nLook for common patterns across these high-activation images\n\nActivation Maximization: Synthesizing images that maximize specific activations\n\nStart with random noise\nIteratively modify the image to increase activation at a target filter\nAscend the gradient to visualize what patterns the filter detects"
  },
  {
    "objectID": "lectures/lecture13.html#transfer-learning-for-cnns",
    "href": "lectures/lecture13.html#transfer-learning-for-cnns",
    "title": "Lecture 13: CNN Architectures and Transfer Learning",
    "section": "Transfer Learning for CNNs",
    "text": "Transfer Learning for CNNs\nTransfer learning has become the standard approach for applying CNNs to new image tasks. This approach leverages pre-trained feature extractors:\n\nMotivation\n\nCNNs trained on ImageNet (14+ million images, 20,000+ categories) learn general-purpose visual features\nLower layers detect universal patterns (edges, textures, simple shapes)\nThese features are transferable across most visual tasks\nTraining such networks requires enormous computational resources\n\n\n\nImplementation Process\n\nFeature Extraction:\n\nTake a pre-trained CNN (e.g., ResNet-50)\nRemove the final classification layer\nPass your images through this truncated network\nExtract the feature maps from the last layer\n\nNew Classifier Training:\n\nUse these extracted features as inputs to a new classifier\nTrain only this new classifier on your specific task\nOptionally fine-tune some of the later layers of the pre-trained network\n\n\n\n\nAdvantages\n\nDramatically reduces training time and computational requirements\nImproves performance on small datasets\nLeverages knowledge from millions of images\nEnables state-of-the-art performance with modest resources\n\nTransfer learning has democratized advanced computer vision, making sophisticated image analysis accessible without requiring massive computational resources or enormous labeled datasets."
  },
  {
    "objectID": "lectures/lecture11.html",
    "href": "lectures/lecture11.html",
    "title": "Lecture 11: Vanishing Gradients and Generalization for Deep Neural Networks",
    "section": "",
    "text": "← Previous: Lecture 10\n\n\nNext: Lecture 12 →"
  },
  {
    "objectID": "lectures/lecture11.html#backpropagation-review",
    "href": "lectures/lecture11.html#backpropagation-review",
    "title": "Lecture 11: Vanishing Gradients and Generalization for Deep Neural Networks",
    "section": "Backpropagation Review",
    "text": "Backpropagation Review\nBackpropagation is the primary algorithm for training deep neural networks through a two-step process:\n\nForward Pass: Compute intermediate values through the network based on current parameter values\nBackward Pass: Compute gradients of the loss with respect to parameters by propagating derivatives backward\n\nFor a neural network with multiple layers, the computational graph allows systematic derivative calculation using the chain rule:\n\\[\\text{Downstream Gradient} = \\text{Local Gradient} \\times \\text{Upstream Gradient}\\]\nAt each layer, gradients follow predictable patterns, with the output layer gradients for mean squared error:\n\\[\\frac{\\partial \\mathcal{L}_i}{\\partial z} = -2(y_i - z_i)\\]\n\\[\\frac{\\partial \\mathcal{L}_i}{\\partial \\boldsymbol{\\beta}} = -2(y_i - z_i) \\otimes \\mathbf{h}_2\\]\n\\[\\frac{\\partial \\mathcal{L}_i}{\\partial \\mathbf{h}_2} = -2(y_i - z_i) \\otimes \\boldsymbol{\\beta}^T\\]\nThe gradient at any layer depends on all upstream derivatives, creating chains of multiplication across layers:\n\\[\\mathbf{u} = \\frac{\\partial \\mathcal{L}}{\\partial z} \\otimes \\boldsymbol{\\beta}^T \\odot \\varphi'(\\mathbf{q}_D) \\otimes \\mathbf{W}_D^T \\odot \\varphi'(\\mathbf{q}_{D-1}) \\otimes \\mathbf{W}_{D-1}^T \\odot \\cdots\\]"
  },
  {
    "objectID": "lectures/lecture11.html#vanishing-gradients",
    "href": "lectures/lecture11.html#vanishing-gradients",
    "title": "Lecture 11: Vanishing Gradients and Generalization for Deep Neural Networks",
    "section": "Vanishing Gradients",
    "text": "Vanishing Gradients\nVanishing gradients occur when the product chain of derivatives approaches zero as it propagates backward through the network. This creates two significant problems:\n\nParameters in early layers receive minimal updates\nThe network cannot learn long-range dependencies\n\n\nSigmoid Activation Problems\nThe sigmoid activation function is a primary culprit in vanishing gradients:\n\\[\\varphi(q) = \\sigma(q) = \\frac{1}{1 + \\exp[-q]}\\]\nIts derivative:\n\\[\\varphi'(q) = \\sigma(q)(1 - \\sigma(q))\\]\nThis derivative approaches zero when inputs are either very large (&gt;3) or very small (&lt;-3), causing gradients to effectively vanish during backpropagation.\n\n\nActivation Function Solutions\nSeveral activation functions address the vanishing gradient problem:\n\nReLU (Rectified Linear Unit): \\[\\varphi(q) = \\max(0, q)\\]\n\nNon-saturating for positive values\nInduces sparsity in hidden representations\nSimple and computationally efficient\nBut can suffer from “dead ReLUs” when units persistently output zero\n\nLeaky ReLU: \\[\\varphi(q) = \\max(\\alpha q, q), \\text{ where } 0 &lt; \\alpha &lt; 1\\]\n\nPrevents complete deactivation of neurons\nSmall slope for negative values keeps gradient flowing\nTypically α = 0.01 or 0.1\n\nELU (Exponential Linear Unit): \\[\\varphi(q) = \\begin{cases}\nq & \\text{if } q &gt; 0 \\\\\n\\alpha(e^q - 1) & \\text{if } q \\leq 0\n\\end{cases}\n\\]\n\nSmooth transition at q=0\nNegative saturation reduces noise influence\nCan help with faster convergence\n\n\n\n\nBatch Normalization\nBatch normalization provides an architectural solution to gradient problems by normalizing activations:\n\\[\\tilde{\\mathbf{q}}_k = \\gamma \\odot \\hat{\\mathbf{q}}_k + \\delta\\]\n\\[\\hat{\\mathbf{q}}_k = \\frac{\\mathbf{q}_k - \\mu_k}{\\sqrt{\\sigma^2_k + \\epsilon}}\\]\nWhere μ₍ and σ₍² are the batch mean and variance. This technique:\n\nStandardizes activations at each layer\nPrevents weights from becoming too large or too small\nAllows training with higher learning rates\nActs as a form of regularization\nReduces internal covariate shift"
  },
  {
    "objectID": "lectures/lecture11.html#generalization-in-neural-networks",
    "href": "lectures/lecture11.html#generalization-in-neural-networks",
    "title": "Lecture 11: Vanishing Gradients and Generalization for Deep Neural Networks",
    "section": "Generalization in Neural Networks",
    "text": "Generalization in Neural Networks\nNeural networks can achieve near-zero training error but often struggle with overfitting due to their expressive capacity. Several techniques address this challenge:\n\nEarly Stopping\n\nMonitor validation loss during training\nStop when validation loss begins to increase\nPrevents learning noise in the training data\nCan be viewed as controlling the effective number of parameters\n\n\n\nWeight Decay (L2 Regularization)\nAdds a penalty term to the loss function:\n\\[\\frac{1}{N} \\sum_{i=1}^N \\mathcal{L}(\\boldsymbol{\\theta} | \\mathbf{x}_i, y_i) + \\lambda \\sum_{d=1}^D \\|\\mathbf{W}_d\\|^2_F\\]\nWhere the Frobenius norm is:\n\\[\\|\\mathbf{W}_d\\|^2_F = \\sum_{i=1}^{K_{d-1}} \\sum_{j=1}^{K_d} w_{ij}^2\\]\nThis regularization:\n\nPenalizes large weights\nSlows learning to find better generalizable solutions\nHelps close the gap between training and generalization error\nEffectively controls the number of hidden units without retraining\n\n\n\nDropout\nDuring training, randomly set weights to zero with probability p:\n\\[\\theta_{dij} = \\mathbf{W}_{dij} \\epsilon_{di}\\]\nWhere ε_di = 0 with probability p and ε_di = 1 with probability (1-p).\nAt inference time, weights are scaled by (1-p).\nDropout:\n\nForces the network to learn robust feature dependencies\nFunctions like an ensemble of multiple network configurations\nPrevents co-adaptation of hidden units\nProvides implicit regularization similar to ridge regression\nTypically p = 0.5, but 0.2-0.25 often works well in practice\n\nThese techniques together—proper activation functions, batch normalization, early stopping, weight decay, and dropout—form a powerful toolkit for training deep neural networks that generalize well to unseen data."
  },
  {
    "objectID": "lectures/lecture1.html",
    "href": "lectures/lecture1.html",
    "title": "QTM 447 Lecture 1: Introduction",
    "section": "",
    "text": "Advanced statistical methods underlying modern machine learning techniques\nFocus on extending introductory machine learning concepts with statistical/mathematical foundations\nThree main parts:\n\nReview of fundamentals, regression methods, regularization, first-order optimization\nNeural Networks and Deep Learning (tabular data, CNNs, RNNs, Transformers)\nGenerative Machine Learning (PCA, Autoencoders, Autoregressive Models, GANs)\n\n\n\n\n\n\nLinear regression (QTM 220 level)\nMachine Learning (QTM 347 level)\nCalculus 1-3\nLinear algebra\nBasic understanding of core concepts, though perfect recall not required\n\n\n\n\n\n\n\n\nTuesday/Thursday 2:30 PM - 3:45 PM\nAttendance strongly encouraged\nInteractive participation expected\nSimulcast on Zoom (not recorded)\nSlides posted before class\n\n\n\n\n\n6-8 assignments throughout semester\nImplementation and extension of class materials\nMix of coding, software implementation, derivations, and proofs\nGroup work allowed (max 3 students)\nMarkdown format required (Quarto or Jupyter Notebook)\nOne free late submission (up to 5 days)\n10% per day penalty thereafter\n\n\n\n\n\nSignificant application of course methods\nThree checkpoints:\n\nTeam presentation (March 25th, 5%)\nProject poster presentation (April 25th, 20%)\nFinal paper submission (May 7th, 25%)\n\nPortfolio-worthy project encouraged\n\n\n\n\n\n\n\n\nPython 3.10 primary language\nVSCode recommended IDE\nJupyter Notebooks/Quarto for documentation\nPyTorch and PyTorch Lightning for deep learning\nGitHub Copilot and ChatGPT encouraged for coding assistance\nGoogle Colab Pro recommended for GPU access\n\n\n\n\n\n\n\nMitchell (1997): “A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.”\n\n\n\n\n\n\nRegression\n\nPredicting continuous outcomes\nInput features → real-valued predictions\n\nClassification\n\nPredicting discrete classes\nInput features → class predictions\n\n\n\n\n\n\nClassification with Missing Inputs\n\nHandling incomplete feature vectors\nGenerative vs. discriminative approaches\n\nStructured Inputs/Outputs\n\nHandling sequences (text, images)\nManaging correlated outputs\n\nSynthesis\n\nGenerating new examples similar to training data\nApplications in text and image generation (e.g., DALL-E, ChatGPT)\n\n\n\n\n\n\n\nMedical diagnosis with incomplete test data\nSurvey response analysis with missing values\nSports prediction with incomplete matchup data\nImage generation and manipulation\nNatural language processing and generation"
  },
  {
    "objectID": "lectures/lecture1.html#lecture-summary",
    "href": "lectures/lecture1.html#lecture-summary",
    "title": "QTM 447 Lecture 1: Introduction",
    "section": "",
    "text": "Advanced statistical methods underlying modern machine learning techniques\nFocus on extending introductory machine learning concepts with statistical/mathematical foundations\nThree main parts:\n\nReview of fundamentals, regression methods, regularization, first-order optimization\nNeural Networks and Deep Learning (tabular data, CNNs, RNNs, Transformers)\nGenerative Machine Learning (PCA, Autoencoders, Autoregressive Models, GANs)\n\n\n\n\n\n\nLinear regression (QTM 220 level)\nMachine Learning (QTM 347 level)\nCalculus 1-3\nLinear algebra\nBasic understanding of core concepts, though perfect recall not required\n\n\n\n\n\n\n\n\nTuesday/Thursday 2:30 PM - 3:45 PM\nAttendance strongly encouraged\nInteractive participation expected\nSimulcast on Zoom (not recorded)\nSlides posted before class\n\n\n\n\n\n6-8 assignments throughout semester\nImplementation and extension of class materials\nMix of coding, software implementation, derivations, and proofs\nGroup work allowed (max 3 students)\nMarkdown format required (Quarto or Jupyter Notebook)\nOne free late submission (up to 5 days)\n10% per day penalty thereafter\n\n\n\n\n\nSignificant application of course methods\nThree checkpoints:\n\nTeam presentation (March 25th, 5%)\nProject poster presentation (April 25th, 20%)\nFinal paper submission (May 7th, 25%)\n\nPortfolio-worthy project encouraged\n\n\n\n\n\n\n\n\nPython 3.10 primary language\nVSCode recommended IDE\nJupyter Notebooks/Quarto for documentation\nPyTorch and PyTorch Lightning for deep learning\nGitHub Copilot and ChatGPT encouraged for coding assistance\nGoogle Colab Pro recommended for GPU access\n\n\n\n\n\n\n\nMitchell (1997): “A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.”\n\n\n\n\n\n\nRegression\n\nPredicting continuous outcomes\nInput features → real-valued predictions\n\nClassification\n\nPredicting discrete classes\nInput features → class predictions\n\n\n\n\n\n\nClassification with Missing Inputs\n\nHandling incomplete feature vectors\nGenerative vs. discriminative approaches\n\nStructured Inputs/Outputs\n\nHandling sequences (text, images)\nManaging correlated outputs\n\nSynthesis\n\nGenerating new examples similar to training data\nApplications in text and image generation (e.g., DALL-E, ChatGPT)\n\n\n\n\n\n\n\nMedical diagnosis with incomplete test data\nSurvey response analysis with missing values\nSports prediction with incomplete matchup data\nImage generation and manipulation\nNatural language processing and generation"
  },
  {
    "objectID": "lectures/glossary.html",
    "href": "lectures/glossary.html",
    "title": "Machine Learning Glossary",
    "section": "",
    "text": "This glossary provides definitions for key machine learning concepts covered in QTM 447. Each entry includes a concise, academically-focused explanation derived directly from course lectures, along with links to the specific lectures where the concept is discussed in detail. Use this resource as a reference guide throughout the course."
  },
  {
    "objectID": "lectures/glossary.html#introduction",
    "href": "lectures/glossary.html#introduction",
    "title": "Machine Learning Glossary",
    "section": "",
    "text": "This glossary provides definitions for key machine learning concepts covered in QTM 447. Each entry includes a concise, academically-focused explanation derived directly from course lectures, along with links to the specific lectures where the concept is discussed in detail. Use this resource as a reference guide throughout the course."
  },
  {
    "objectID": "lectures/glossary.html#a",
    "href": "lectures/glossary.html#a",
    "title": "Machine Learning Glossary",
    "section": "A",
    "text": "A\n\nAccuracy\nA performance metric quantifying the proportion of correct predictions (both true positives and true negatives) among all cases examined. While useful for balanced datasets, accuracy can be misleading when class distributions are skewed.\nRelated lectures: Lecture 3 (Generalization Error), Lecture 4 (Linear Models and Likelihood)\n\n\nActivation Function\nMathematical functions applied to neural network layer outputs that introduce non-linearity, enabling networks to learn complex patterns. Key activation functions include:\n\nReLU (Rectified Linear Unit): \\(f(x) = \\max(0, x)\\), widely used due to efficient gradient computation and helping mitigate vanishing gradients\nSigmoid: \\(f(x) = \\frac{1}{1+e^{-x}}\\), maps values to [0,1], useful for binary classification outputs\nTanh: \\(f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\\), maps values to [-1,1], often performs better than sigmoid in hidden layers\n\nRelated lectures: Lecture 8 (Introduction to Neural Networks), Lecture 9 (Deep Neural Networks)\n\n\nAdam Optimizer\nAn adaptive learning rate optimization algorithm that combines ideas from momentum and RMSProp, maintaining per-parameter learning rates adapted based on first and second moments of gradients. Features include:\n\nBias correction for moments\nAdaptive step sizes for each parameter\nComputational efficiency\nGood performance across a wide range of problems without extensive hyperparameter tuning\n\nRelated lectures: Lecture 6 (Adaptive Methods for Minimization)\n\n\nArea Under the Curve (AUC)\nA performance metric for binary classification that measures the area under the ROC curve. AUC represents the probability that a randomly chosen positive instance ranks higher than a randomly chosen negative instance. An AUC of 1.0 indicates perfect ranking, while 0.5 represents random chance.\nRelated lectures: Lecture 2 (Machine Learning Fundamentals)\n\n\nAttention Mechanism\nA neural network component that allows models to focus on different parts of the input when producing outputs. Attention computes weighted sums of input elements based on relevance scores, enabling models to selectively emphasize important information. This mechanism is fundamental to modern sequence processing architectures like Transformers.\nRelated lectures: Lecture 18 (Transformers and the Attention Revolution)\n\n\nAutoencoder\nNeural networks designed to learn efficient data representations (encodings) in an unsupervised manner by attempting to reconstruct their own inputs. The architecture consists of:\n\nEncoder: Compresses input data into a lower-dimensional latent representation\nBottleneck/latent space: The compressed representation capturing essential features\nDecoder: Reconstructs the original input from the latent representation\n\nApplications include dimensionality reduction, denoising, anomaly detection, and as components in generative models.\nRelated lectures: Lecture 20 (Generative Models: Autoregressives and Autoencoders)\n\n\nAutoregressive Models\nModels that predict sequential outputs where each element depends on previously generated elements, modeling the probability distribution as:\n\\[p(x_1, x_2, ..., x_n) = \\prod_{i=1}^n p(x_i | x_1, x_2, ..., x_{i-1})\\]\nUsed extensively in sequential data modeling such as language models, time series forecasting, and some generative models. Examples include traditional AR models, RNN-based language models, and GPT architectures.\nRelated lectures: Lecture 16 (Recurrent Neural Networks and Sequence Modeling), Lecture 20 (Generative Models: Autoregressives and Autoencoders)"
  },
  {
    "objectID": "lectures/glossary.html#b",
    "href": "lectures/glossary.html#b",
    "title": "Machine Learning Glossary",
    "section": "B",
    "text": "B\n\nBackpropagation\nThe primary algorithm for training neural networks by computing gradients of the loss function with respect to model parameters. Consists of two main phases:\n\nForward pass: Computing all intermediate values through the network to calculate the output and loss\nBackward pass: Efficiently calculating gradients using the chain rule by propagating derivatives backward from the loss\n\nThe algorithm leverages computational graphs to track dependencies and facilitates efficient parameter updates through gradient-based optimization.\nRelated lectures: Lecture 10 (Backpropagation and Gradient Problems)\n\n\nBatch Normalization\nA technique that normalizes layer inputs across mini-batches during training, maintaining internal representations with zero mean and unit variance:\n\\[\\hat{x}^{(k)} = \\frac{x^{(k)} - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}\\]\nAfter normalization, learnable parameters \\(\\gamma\\) and \\(\\beta\\) allow the network to undo the normalization if optimal:\n\\[y^{(k)} = \\gamma^{(k)} \\hat{x}^{(k)} + \\beta^{(k)}\\]\nAdvantages include:\n\nAccelerated training by reducing internal covariate shift\nImproved gradient flow\nRegularizing effect that reduces overfitting\nEnables higher learning rates\n\nRelated lectures: Lecture 11 (Vanishing Gradients and Generalization), Lecture 13 (Advanced CNN Architectures and Transfer Learning)\n\n\nBayesian Machine Learning\nA probabilistic approach to machine learning that applies Bayesian statistics to model uncertainty by:\n\nIncorporating prior beliefs about parameters using probability distributions\nUpdating these beliefs with observed data using Bayes’ theorem\nProducing posterior distributions over parameters rather than point estimates\nEnabling principled uncertainty quantification in predictions\n\nKey methods include Bayesian linear regression, Bayesian neural networks, and Gaussian processes.\nRelated lectures: Lecture 22 (Bayesian Machine Learning)\n\n\nBias-Variance Tradeoff\nA fundamental concept in statistical learning theory that decomposes prediction error into three components:\n\nBias: Systematic error from incorrect model assumptions or insufficient complexity\nVariance: Error from sensitivity to training data fluctuations\nIrreducible error: Inherent noise that cannot be eliminated\n\nAs model complexity increases, bias typically decreases while variance increases. Finding the optimal complexity minimizes the combined error (bias² + variance) to achieve better generalization.\nRelated lectures: Lecture 3 (Generalization Error)\n\n\nBoosting\nAn ensemble learning approach where weak learners (typically shallow decision trees) are trained sequentially, with each new model focusing on the mistakes of previous models. Examples include:\n\nAdaBoost: Adjusts instance weights to emphasize previously misclassified examples\nGradient Boosting: Iteratively fits models to the residual errors of previous models\nXGBoost/LightGBM: Optimized implementations with regularization and efficient training\n\nRelated lectures: Lecture 27 (Interpretable Machine Learning)"
  },
  {
    "objectID": "lectures/glossary.html#c",
    "href": "lectures/glossary.html#c",
    "title": "Machine Learning Glossary",
    "section": "C",
    "text": "C\n\nCategorical Cross-Entropy Loss\nA loss function for multi-class classification problems measuring the difference between predicted probability distribution \\(\\hat{p}\\) and true distribution \\(p\\) (usually one-hot encoded):\n\\[L = -\\sum_{i=1}^C p_i \\log(\\hat{p}_i)\\]\nWhere \\(C\\) is the number of classes, \\(p_i\\) is the true probability of class \\(i\\) (typically 0 or 1), and \\(\\hat{p}_i\\) is the predicted probability. Minimizing this loss maximizes the log-likelihood of the correct classifications.\nRelated lectures: Lecture 2 (Machine Learning Fundamentals), Lecture 5 (Optimization Foundations and Stochastic Gradient Descent), Lecture 8 (Introduction to Neural Networks)\n\n\nComputational Graph\nA directed acyclic graph representation of mathematical operations where nodes represent operations (addition, multiplication, activation functions) and edges represent data flow. This structure enables:\n\nSystematic computation of gradients using the chain rule\nEfficient implementation of backpropagation by storing intermediate values\nAutomatic differentiation in deep learning frameworks\n\nRelated lectures: Lecture 10 (Backpropagation and Gradient Problems)\n\n\nConfusion Matrix\nA table used to evaluate classification model performance, showing the counts of true positives, true negatives, false positives, and false negatives. Helps calculate precision, recall, F1 score, and other metrics.\nRelated lectures: Lecture 2 (Machine Learning Fundamentals), Lecture 3 (Generalization Error)\n\n\nConvolutional Neural Network (CNN)\nNeural network architectures specialized for processing grid-structured data (especially images) through specialized layers:\n\nConvolutional layers: Apply learned filters across spatial dimensions, detecting local patterns while sharing parameters\nPooling layers: Reduce spatial dimensions by summarizing regions (max pooling, average pooling)\nFully connected layers: Typically used in final stages for classification or regression\n\nKey benefits include translation invariance, parameter efficiency through weight sharing, and hierarchical feature learning. Notable architectures include LeNet, AlexNet, VGG, and ResNet.\nRelated lectures: Lecture 12 (Image Classification with Convolutional Neural Networks), Lecture 13 (Advanced CNN Architectures and Transfer Learning)\n\n\nCross-Validation\nA model evaluation technique that partitions data into training and validation sets multiple times to estimate performance. Common approaches include:\n\nK-fold cross-validation: Divides data into k equal subsets (folds), trains on k-1 folds and tests on the remaining fold, rotating through all folds\nLeave-one-out cross-validation: Uses a single observation for validation and all others for training, repeated for each observation\nStratified cross-validation: Maintains class distribution proportions in each fold\n\nCross-validation provides more reliable performance estimates than single train-test splits, especially with limited data.\nRelated lectures: Lecture 3 (Generalization Error)\n\n\nCurriculum Learning\nTraining strategy where models are taught easier concepts before harder ones, similar to how humans learn. This approach can lead to better performance and faster convergence by gradually increasing task complexity during training.\nRelated lectures: Lecture 13 (Advanced CNN Architectures and Transfer Learning)"
  },
  {
    "objectID": "lectures/glossary.html#d",
    "href": "lectures/glossary.html#d",
    "title": "Machine Learning Glossary",
    "section": "D",
    "text": "D\n\nData Augmentation\nA regularization technique that artificially expands training datasets by applying label-preserving transformations to existing samples. Common transformations include:\n\nImage domain: Rotations, flips, crops, color adjustments, elastic distortions\nText domain: Synonym substitution, back-translation, random deletion\nAudio domain: Time stretching, pitch shifting, adding noise\n\nRelated lectures: Lecture 13 (Advanced CNN Architectures and Transfer Learning)\n\n\nDecision Boundaries\nThe separating surfaces in feature space that demarcate different class regions in classification models. In linear models, these are hyperplanes; in nonlinear models, they can form complex shapes. Decision boundaries directly relate to model complexity - more flexible models can create more intricate decision boundaries.\nRelated lectures: Lecture 8 (Introduction to Neural Networks), Lecture 9 (Deep Neural Networks)\n\n\nDecision Trees\nNon-parametric supervised learning models that recursively partition the feature space based on feature values to create a tree-like model of decisions. Key components include:\n\nNodes: Test a particular feature against a threshold\nBranches: Outcomes of the tests\nLeaf nodes: Final predictions or class assignments\nSplitting criteria: Metrics like entropy, information gain, or Gini impurity that determine optimal splits\n\nRelated lectures: Lecture 7 (Nonlinearities and Expressive Learning Methods)\n\n\nDeep Learning\nA subset of machine learning using neural networks with multiple layers to progressively extract higher-level features from raw input. Characterized by:\n\nAutomatic feature extraction without manual engineering\nEnd-to-end learning from raw data to final output\nHierarchical representations increasing in abstraction with network depth\nCapacity to model extremely complex, non-linear relationships\n\nRelated lectures: Lecture 8 (Introduction to Neural Networks), Lecture 9 (Deep Neural Networks)\n\n\nDiffusion Models\nGenerative models that learn to convert noise into structured data by reversing a gradual noising process. The approach involves:\n\nForward process: Gradually adding Gaussian noise to training data according to a fixed schedule until it becomes pure noise\nReverse process: Training a neural network to denoise images step by step\nSampling: Generating new data by starting with random noise and iteratively denoising\n\nKey advantages include high-quality generation, stable training, and good sample diversity. Examples include DDPM and Stable Diffusion.\nRelated lectures: Lecture 26 (Diffusion Models)\n\n\nDropout\nA regularization technique where randomly selected neurons are temporarily removed during training iterations:\n\nFor each training batch, each neuron has probability \\(p\\) of being retained\nDuring testing, all neurons are used but outputs are scaled by \\(p\\) to maintain expected activation levels\n\nThis prevents co-adaptation of neurons, effectively training an ensemble of subnetworks, which reduces overfitting and improves generalization.\nRelated lectures: Lecture 11 (Vanishing Gradients and Generalization)"
  },
  {
    "objectID": "lectures/glossary.html#e",
    "href": "lectures/glossary.html#e",
    "title": "Machine Learning Glossary",
    "section": "E",
    "text": "E\n\nEarly Stopping\nA regularization technique where training is halted when performance on a validation set stops improving. Implementation typically involves:\n\nMonitoring validation loss/metric during training\nStopping when validation performance has not improved for a specified number of iterations (patience)\nReverting to the model weights that achieved best validation performance\n\nThis prevents overfitting by not allowing the model to continue learning noise in the training data.\nRelated lectures: Lecture 11 (Vanishing Gradients and Generalization)\n\n\nEmbedding\nA learned mapping from discrete entities (words, categories, user IDs) to continuous vector spaces of lower dimensionality. Embeddings:\n\nCapture semantic relationships between entities in vector space\nEnable neural networks to process categorical inputs\nPlace similar entities close together in the embedding space\nServe as dense, information-rich features for downstream tasks\n\nCommon applications include word embeddings (Word2Vec, GloVe), entity embeddings, and graph embeddings.\nRelated lectures: Lecture 17 (Sequence-to-Sequence Models and Attention Mechanisms), Lecture 19 (Representation Learning, Vision Transformers, and Autoregressive Generation)\n\n\nEnsemble Methods\nTechniques that combine multiple models to improve predictive performance and robustness. Major approaches include:\n\nBagging (Bootstrap Aggregating): Trains models on random subsets of data with replacement and averages predictions\nBoosting: Sequentially trains models with each focusing on errors from previous models\nStacking: Uses a meta-model that learns how to best combine predictions from base models\nRandom Forests: Combines bagging with random feature subset selection in decision trees\n\nEnsembles typically achieve better generalization by reducing variance or bias depending on the method used.\nRelated lectures: Lecture 11 (Vanishing Gradients and Generalization), Lecture 27 (Interpretable Machine Learning)\n\n\nExplainable AI (XAI)\nMethods and techniques that make AI systems’ decisions understandable and interpretable to humans. Approaches include: - Post-hoc methods: Feature importance, SHAP values, LIME, attention visualization - Glass-box models: Inherently interpretable models like decision trees, linear models - Concept attribution: Mapping activations to human-understandable concepts - Counterfactual explanations: Identifying minimal changes to inputs that would change outputs\nXAI is crucial for building trust, ensuring fairness, meeting regulatory requirements, and identifying potential biases in models.\nRelated lectures: Lecture 27 (Interpretable Machine Learning)\n\n\nEvidence Lower Bound (ELBO)\nA key concept in variational inference that provides a tractable lower bound on the marginal log-likelihood of observed data:\n\\[\\text{ELBO} = \\mathbb{E}_Q[\\log P(\\mathbf{x}|\\mathbf{z},\\boldsymbol{\\theta})] - D_{KL}(Q(\\mathbf{z}|\\mathbf{x},\\boldsymbol{\\phi}) || P(\\mathbf{z}))\\]\nThe ELBO consists of two components:\n\nReconstruction term: Expected log-likelihood of data given latent variables\nRegularization term: KL divergence between the approximate posterior and the prior\n\nMaximizing the ELBO is equivalent to minimizing the KL divergence between the approximate and true posterior distributions. This objective function is central to training variational autoencoders and other approximate Bayesian methods.\nRelated lectures: Lecture 23 (Variational Autoencoders)"
  },
  {
    "objectID": "lectures/glossary.html#f",
    "href": "lectures/glossary.html#f",
    "title": "Machine Learning Glossary",
    "section": "F",
    "text": "F\n\nFeature Engineering\nThe process of using domain knowledge to extract or create features from raw data to improve machine learning model performance. Techniques include:\n\nTransformation: Applying mathematical functions to create new representations\nEncoding: Converting categorical variables to numerical form\nDiscretization: Binning continuous variables into categories\nInteraction features: Creating new features from combinations of existing ones\nFeature extraction: Deriving informative features from raw data\n\nRelated lectures: Lecture 8 (Introduction to Neural Networks), Lecture 9 (Deep Neural Networks)\n\n\nFeature Selection\nTechniques for identifying and selecting the most relevant variables for model building. Common approaches include:\n\nFilter methods: Evaluate features independently using statistical measures (correlation, chi-square)\nWrapper methods: Test feature subsets using the model itself (recursive feature elimination)\nEmbedded methods: Perform selection during model training (L1 regularization)\n\nEffective feature selection improves model performance, reduces training time, and enhances model interpretability.\nRelated lectures: Lecture 3 (Generalization Error), Lecture 4 (Linear Models and Likelihood)\n\n\nForward and Backward Passes\nThe two fundamental computational phases in neural network training:\n\nForward Pass: The computation flow from input to output where:\n\nInput data propagates through the network layer by layer\nEach layer applies weights, biases, and activation functions\nIntermediate values and final outputs are computed and stored\nThe loss function evaluates prediction quality\n\nBackward Pass: The computation flow from output back to input where:\n\nGradients of the loss with respect to parameters are computed\nThe chain rule is applied to propagate error signals backward\nParameter updates are calculated for optimization algorithms\nWeights and biases are adjusted to minimize the loss\n\n\nRelated lectures: Lecture 10 (Backpropagation and Gradient Problems)\n\n\nFine-tuning\nThe process of further training a pre-trained model on a specific target task with domain-specific data. The approach typically involves:\n\nStarting with a model pre-trained on a large general dataset\nReplacing or modifying the output layers for the target task\nTraining on the target dataset, typically with lower learning rates\nPotentially freezing early layers to preserve general features\n\nFine-tuning leverages knowledge transfer to achieve better performance with less task-specific data.\nRelated lectures: Lecture 19 (Transfer Learning)\n\n\nF1 Score\nA performance metric that combines precision and recall into a single harmonized score:\n\\[F1 = 2 \\cdot \\frac{\\text{precision} \\cdot \\text{recall}}{\\text{precision} + \\text{recall}}\\]\nF1 score ranges from 0 (worst) to 1 (best) and is particularly valuable for imbalanced classification problems where both false positives and false negatives are important considerations.\nRelated lectures: Lecture 6 (Classification Performance Metrics)"
  },
  {
    "objectID": "lectures/glossary.html#g",
    "href": "lectures/glossary.html#g",
    "title": "Machine Learning Glossary",
    "section": "G",
    "text": "G\n\nGated Recurrent Unit (GRU)\nA type of recurrent neural network architecture that uses gating mechanisms to control information flow. Simpler than LSTMs with fewer parameters, GRUs use reset and update gates to handle the vanishing gradient problem while maintaining good performance.\nRelated lectures: Lecture 16 (Recurrent Neural Networks and Sequence Modeling)\n\n\nGeneralization Error\nThe expected error when applying a model to new, unseen data. It measures how well a model performs beyond its training data and can be estimated using validation sets or cross-validation. Good generalization is the ultimate goal of machine learning.\nRelated lectures: Lecture 3 (Generalization Error)\n\n\nGenerative Adversarial Networks (GANs)\nFramework where two neural networks compete in a game-theoretic scenario:\n\nGenerator: Creates synthetic data samples trying to fool the discriminator\nDiscriminator: Distinguishes between real and generated samples\n\nThrough this adversarial process, the generator learns to produce increasingly realistic data, while the discriminator becomes better at detecting fakes. This minimax game continues until equilibrium, resulting in a generator that creates high-quality synthetic data.\nRelated lectures: Lecture 24 (VAE Extensions and GANs Introduction), Lecture 25 (Generative Adversarial Networks)\n\n\nGenerative Models\nModels that learn to generate new data samples from the same distribution as the training data. Types include:\n\nAutoregressive models: Model sequential dependencies (e.g., PixelCNN, GPT)\nVariational Autoencoders: Learn probabilistic latent representations\nGANs: Use adversarial training to generate realistic samples\nDiffusion models: Generate data by reversing a noising process\nFlow-based models: Use invertible transformations to model probability distributions\n\nRelated lectures: Lecture 20 (Generative Models: Autoregressives and Autoencoders), Lecture 24 (VAE Extensions and GANs Introduction), Lecture 25 (Generative Adversarial Networks), Lecture 26 (Diffusion Models)\n\n\nGeneralized Linear Models (GLMs)\nExtension of linear regression that allows for response variables with non-normal distributions. GLMs consist of:\n\nRandom component: Specifies the distribution of the response variable (e.g., Gaussian, Binomial, Poisson)\nSystematic component: Linear combination of predictors\nLink function: Connects the expected value of the response to the linear predictor\n\nCommon examples include logistic regression, Poisson regression, and log-linear models.\nRelated lectures: Lecture 4 (Linear Models and Likelihood)\n\n\nGradient Descent\nAn optimization algorithm used to minimize loss functions by iteratively adjusting parameters in the direction of steepest descent of the gradient. Variants include:\n\nBatch Gradient Descent: Uses entire dataset for each update\nStochastic Gradient Descent (SGD): Uses a single sample for each update\nMini-batch Gradient Descent: Uses a small random subset of data for each update\n\nRelated lectures: Lecture 5 (Loss Minimization and Optimization), Lecture 10 (Backpropagation and Gradient Problems)"
  },
  {
    "objectID": "lectures/glossary.html#h",
    "href": "lectures/glossary.html#h",
    "title": "Machine Learning Glossary",
    "section": "H",
    "text": "H\n\nHyperparameter\nParameters set before training begins (unlike model parameters that are learned during training). Examples include learning rate, number of layers, number of neurons per layer, batch size, and regularization strength. Tuning hyperparameters typically requires techniques like grid search, random search, or Bayesian optimization.\nRelated lectures: Lecture 8 (Introduction to Neural Networks), Lecture 11 (Vanishing Gradients and Generalization)"
  },
  {
    "objectID": "lectures/glossary.html#i",
    "href": "lectures/glossary.html#i",
    "title": "Machine Learning Glossary",
    "section": "I",
    "text": "I\n\nInformation Gain\nA metric used in decision trees to determine the quality of a split, based on the reduction in entropy after a dataset is split. Higher information gain indicates a more useful split.\nRelated lectures: Lecture 7 (Nonlinearities and Expressive Learning Methods)\n\n\nInstance Segmentation\nComputer vision task that involves identifying each instance of each object in an image at the pixel level. It combines elements of object detection (finding and classifying objects) with semantic segmentation (determining the class of each pixel).\nRelated lectures: Lecture 14 (Object Detection in Computer Vision), Lecture 15 (Semantic Segmentation and Advanced CNN Applications)"
  },
  {
    "objectID": "lectures/glossary.html#k",
    "href": "lectures/glossary.html#k",
    "title": "Machine Learning Glossary",
    "section": "K",
    "text": "K\n\nK-means Clustering\nAn unsupervised learning algorithm that partitions data into K clusters by minimizing the sum of squared distances between data points and their assigned cluster centroids. The algorithm works by:\n\nRandomly initializing K cluster centroids\nAssigning each data point to the nearest centroid\nUpdating centroids based on assigned points\nRepeating steps 2-3 until convergence\n\nRelated lectures: Lecture 2 (Machine Learning Fundamentals)\n\n\nKernel Methods\nTechniques that implicitly transform data into higher-dimensional spaces without explicitly computing the coordinates in that space. The kernel trick allows for efficient computation of inner products in these spaces, making non-linear patterns linearly separable. Support Vector Machines often use kernel methods.\nRelated lectures: Lecture 7 (Nonlinearities and Expressive Learning Methods)\n\n\nKullback-Leibler (KL) Divergence\nA statistical measure that quantifies the difference between two probability distributions P and Q:\n\\[D_{KL}(P||Q) = \\sum_x P(x) \\log \\frac{P(x)}{Q(x)} \\text{ or } \\int P(x) \\log \\frac{P(x)}{Q(x)} dx\\]\nKey properties include:\n\nAlways non-negative (≥ 0)\nEquals zero only when the distributions are identical\nAsymmetric: \\(D_{KL}(P||Q) \\neq D_{KL}(Q||P)\\)\n\nIn machine learning, KL divergence serves as:\n\nA regularization term in variational autoencoders\nA component in the evidence lower bound (ELBO)\nA loss function for matching probability distributions\nA way to measure how much information is lost when approximating one distribution with another\n\nRelated lectures: Lecture 22 (Bayesian Machine Learning), Lecture 23 (Variational Autoencoders)"
  },
  {
    "objectID": "lectures/glossary.html#m",
    "href": "lectures/glossary.html#m",
    "title": "Machine Learning Glossary",
    "section": "M",
    "text": "M\n\nMachine Learning Fundamentals\nThe core components that define machine learning as articulated by Tom Mitchell (1997):\n\nTasks (T): The objectives the algorithm aims to achieve:\n\nPrediction: Forecasting future or unknown values\nDescription: Uncovering patterns or structures in data\nInference/Explanation: Understanding causal relationships and variable impacts\n\nExperience (E): The data that fuels learning:\n\nSupervised Learning: Uses labeled data\nUnsupervised Learning: Uses unlabeled data\nSemi-supervised Learning: Uses both labeled and unlabeled data\nReinforcement Learning: Learning through environment interaction\n\nPerformance Measure (P): Metrics quantifying algorithm performance, typically through loss functions\n\nRelated lectures: Lecture 2 (Machine Learning Fundamentals)"
  },
  {
    "objectID": "lectures/glossary.html#l",
    "href": "lectures/glossary.html#l",
    "title": "Machine Learning Glossary",
    "section": "L",
    "text": "L\n\nL1 Regularization (LASSO)\nRegularization technique that adds the sum of absolute values of coefficients to the loss function, promoting sparsity by shrinking less important feature coefficients to exactly zero. The regularization term is \\(\\lambda\\sum|w_i|\\), where \\(\\lambda\\) controls the strength of the penalty.\nRelated lectures: Lecture 4 (Linear Models and Likelihood)\n\n\nL2 Regularization (Ridge)\nRegularization technique that adds the sum of squared coefficients to the loss function, preventing overfitting by shrinking all coefficients toward zero but rarely setting them exactly to zero. The regularization term is \\(\\lambda\\sum w_i^2\\), where \\(\\lambda\\) controls the strength of the penalty.\nRelated lectures: Lecture 4 (Linear Models and Likelihood), Lecture 11 (Vanishing Gradients and Generalization)\n\n\nLaplace Approximation\nA method for approximating Bayesian posterior distributions with a multivariate Gaussian distribution centered at the maximum a posteriori (MAP) estimate:\n\\[P(\\boldsymbol{\\theta}|\\mathcal{D}) \\approx \\mathcal{N}(\\boldsymbol{\\theta}|\\hat{\\boldsymbol{\\mu}}, \\hat{\\boldsymbol{\\Sigma}})\\]\nWhere:\n\n\\(\\hat{\\boldsymbol{\\mu}} = \\arg\\max_{\\boldsymbol{\\theta}} P(\\boldsymbol{\\theta}|\\mathcal{D})\\) is the MAP estimate\n\\(\\hat{\\boldsymbol{\\Sigma}} = -\\left[\\frac{\\partial^2 \\log P(\\hat{\\boldsymbol{\\mu}}|\\mathcal{D})}{\\partial \\boldsymbol{\\theta} \\partial \\boldsymbol{\\theta}'}\\right]^{-1}\\) is the inverse Hessian of the negative log posterior\n\nThis approximation is particularly useful for models with intractable posterior distributions, providing both parameter estimates and uncertainty quantification. The Bernstein-von Mises theorem guarantees that under certain conditions, the posterior distribution converges to this normal approximation as sample size increases.\nRelated lectures: Lecture 22 (Bayesian Machine Learning)\n\n\nLatent Space\nA compressed representation in lower dimensions that captures the essential features of the input data. In autoencoders and VAEs, this is the bottleneck layer. Points that are close in latent space should represent semantically similar inputs.\nRelated lectures: Lecture 20 (Generative Models: Autoregressives and Autoencoders), Lecture 24 (VAE Extensions and GANs Introduction)\n\n\nLearning Rate\nA hyperparameter that controls how much to adjust model weights in response to the estimated error during training. Too high a learning rate can cause training to diverge; too low can cause it to converge too slowly or get stuck in suboptimal solutions.\nRelated lectures: Lecture 5 (Loss Minimization and Optimization), Lecture 10 (Backpropagation and Gradient Problems)\n\n\nLearning Rate Scheduling\nTechniques to adjust the learning rate during training, typically reducing it over time. Common approaches include step decay, exponential decay, cosine annealing, and warm restarts.\nRelated lectures: Lecture 5 (Loss Minimization and Optimization), Lecture 6 (Adaptive Methods for Minimization)\n\n\nLikelihood\nIn statistical inference, the conditional probability of observing the data given specific parameter values, expressed as \\(P(\\mathcal{D}|\\boldsymbol{\\theta})\\). The likelihood function:\n\nMeasures how well different parameter values explain observed data\nForms the basis for maximum likelihood estimation (MLE) when maximizing \\(P(\\mathcal{D}|\\boldsymbol{\\theta})\\)\nServes as a critical component in Bayes’ theorem: \\(P(\\boldsymbol{\\theta}|\\mathcal{D}) \\propto P(\\mathcal{D}|\\boldsymbol{\\theta})P(\\boldsymbol{\\theta})\\)\nOften expressed and optimized in logarithmic form (log-likelihood) for computational stability\n\nCommon examples include Gaussian likelihood for regression and Bernoulli likelihood for binary classification.\nRelated lectures: Lecture 4 (Linear Models and Likelihood), Lecture 22 (Bayesian Machine Learning)\n\n\nLong Short-Term Memory (LSTM)\nA specialized recurrent neural network architecture designed to address the vanishing gradient problem in standard RNNs. LSTMs use gating mechanisms (input, forget, and output gates) to retain information over long sequences.\nRelated lectures: Lecture 16 (Recurrent Neural Networks and Sequence Modeling)\n\n\nLogistic Regression\nStatistical model used for binary classification that applies the logistic (sigmoid) function to a linear combination of features, transforming an unbounded output to a probability between 0 and 1. The formula is:\n\\[P(y = 1 | \\mathbf x) = \\frac{1}{1 + \\exp[-\\mathbf w^T \\mathbf x - b]}\\]\nRelated lectures: Lecture 4 (Linear Models and Likelihood), Lecture 5 (Loss Minimization and Optimization)\n\n\nLoss Function\nA function that quantifies how well a model’s predictions match the true values. Common loss functions include:\n\nMean Squared Error (MSE): For regression problems\nCross-Entropy Loss: For classification problems\nHinge Loss: Used in SVMs\nKullback-Leibler Divergence: Measures difference between probability distributions\n\nRelated lectures: Lecture 2 (Machine Learning Fundamentals), Lecture 3 (Generalization Error), Lecture 5 (Loss Minimization and Optimization)"
  },
  {
    "objectID": "lectures/glossary.html#m-1",
    "href": "lectures/glossary.html#m-1",
    "title": "Machine Learning Glossary",
    "section": "M",
    "text": "M\n\nMarginal Likelihood\nAlso known as model evidence, the marginal likelihood represents the probability of observing the data integrated over all possible parameter values:\n\\[P(\\mathcal{D}) = \\int P(\\mathcal{D}|\\boldsymbol{\\theta})P(\\boldsymbol{\\theta})d\\boldsymbol{\\theta}\\]\nKey properties and applications include:\n\nServes as the normalizing constant in Bayes’ theorem\nAutomatically implements Occam’s razor, balancing model fit and complexity\nUsed for model comparison and hyperparameter selection\nOften computationally intractable, requiring approximation methods\nFor linear regression with normal prior: \\(P(\\mathbf{y}|\\mathbf{X}) = \\mathcal{N}(\\mathbf{y}|\\mathbf{X}\\boldsymbol{\\mu}_0, \\sigma^2\\mathbf{I} + \\mathbf{X}\\boldsymbol{\\Sigma}_0\\mathbf{X}^T)\\)\n\nRelated lectures: Lecture 22 (Bayesian Machine Learning), Lecture 23 (Variational Autoencoders)\n\n\nMaximum Likelihood Estimation (MLE)\nA method for estimating the parameters of a statistical model by finding values that maximize the likelihood function, which measures how probable the observed data is given the parameter values.\nRelated lectures: Lecture 4 (Linear Models and Likelihood)\n\n\nModel Calibration\nThe property that a model’s predicted probabilities accurately reflect true probabilities. A well-calibrated model’s confidence should match its accuracy. Techniques include Platt scaling and temperature scaling.\nRelated lectures: Lecture 4 (Linear Models and Likelihood), Lecture 27 (Interpretable Machine Learning)\n\n\nMulti-Head Attention\nAn extension of self-attention that runs multiple attention mechanisms in parallel, allowing the model to focus on different aspects of the input simultaneously. Each “head” projects the input into different subspaces before computing attention, capturing diverse relationships in the data. The outputs from all heads are concatenated and linearly transformed to produce the final result. This approach enhances model expressivity by enabling attention to different representation subspaces and positions.\nRelated lectures: Lecture 18 (Transformers and the Attention Revolution)\n\n\nMulti-Layer Perceptron (MLP)\nA feedforward neural network with at least one hidden layer between input and output layers. Each neuron in one layer connects to every neuron in the next layer, using non-linear activation functions to learn complex patterns.\nRelated lectures: Lecture 8 (Introduction to Neural Networks), Lecture 9 (Deep Neural Networks)"
  },
  {
    "objectID": "lectures/glossary.html#n",
    "href": "lectures/glossary.html#n",
    "title": "Machine Learning Glossary",
    "section": "N",
    "text": "N\n\nNeural Network Architecture\nThe specific arrangement of layers, neurons, and connections in a neural network. Includes the number of layers (depth), neurons per layer (width), types of layers, activation functions, and connectivity patterns.\nRelated lectures: Lecture 9 (Deep Neural Networks), Lecture 13 (Advanced CNN Architectures and Transfer Learning)\n\n\nNormalization\nTechniques that adjust the scale of inputs or internal representations to improve neural network training. Examples include:\n\nBatch Normalization: Normalizes layer inputs within a mini-batch\nLayer Normalization: Normalizes across features for each sample\nInstance Normalization: Normalizes across spatial dimensions for each sample\nGroup Normalization: Normalizes across subsets of channels\n\nRelated lectures: Lecture 11 (Vanishing Gradients and Generalization), Lecture 13 (Advanced CNN Architectures and Transfer Learning)"
  },
  {
    "objectID": "lectures/glossary.html#o",
    "href": "lectures/glossary.html#o",
    "title": "Machine Learning Glossary",
    "section": "O",
    "text": "O\n\nObject Detection\nComputer vision task that involves identifying and localizing multiple objects in images. Combines classification (what objects are present) with localization (where they are). Common architectures include R-CNN, Fast R-CNN, Faster R-CNN, YOLO, and SSD.\nRelated lectures: Lecture 14 (Object Detection in Computer Vision)\n\n\nOne-Hot Encoding\nA representation of categorical variables as binary vectors where each position corresponds to a category, and only one position has a value of 1 (the category that applies), while all others are 0.\nRelated lectures: Lecture 2 (Machine Learning Fundamentals), Lecture 6 (Adaptive Methods for Minimization)\n\n\nOptimization Algorithms\nMethods used to minimize the loss function and update model parameters. Beyond basic gradient descent, advanced algorithms include:\n\nMomentum: Adds a fraction of the previous update to accelerate convergence\nRMSProp: Adapts learning rates using a moving average of squared gradients\nAdam: Combines momentum and RMSProp approaches\nAdamW: A variant of Adam with improved weight decay regularization\n\nRelated lectures: Lecture 5 (Loss Minimization and Optimization), Lecture 6 (Adaptive Methods for Minimization)\n\n\nOverfitting\nWhen a model learns the training data too well, capturing noise instead of the underlying pattern, leading to poor generalization. Signs include:\n\nHigh accuracy on training data but low accuracy on validation/test data\nComplex model with many parameters relative to the amount of training data\nModel memorizes training examples rather than learning general patterns\n\nRelated lectures: Lecture 3 (Generalization Error), Lecture 4 (Linear Models and Likelihood)"
  },
  {
    "objectID": "lectures/glossary.html#p",
    "href": "lectures/glossary.html#p",
    "title": "Machine Learning Glossary",
    "section": "P",
    "text": "P\n\nParameter Sharing\nA technique in neural networks where the same parameters are used across different parts of the model. Most prominently used in CNNs, where the same convolutional filters are applied across the entire image, drastically reducing the number of parameters and making the network translation invariant.\nRelated lectures: Lecture 12 (Image Classification with Convolutional Neural Networks)\n\n\nPooling\nOperations in convolutional neural networks that reduce spatial dimensions by summarizing values in local regions. Common pooling types include:\n\nMax pooling: Takes the maximum value in each window\nAverage pooling: Takes the average value in each window\nGlobal pooling: Reduces each feature map to a single value\n\nRelated lectures: Lecture 12 (Image Classification with Convolutional Neural Networks)\n\n\nPre-training\nTraining a model on a large dataset for a general task before fine-tuning it on a specific task with less data. Enables transfer learning by learning general features that can be adapted to new tasks.\nRelated lectures: Lecture 13 (Advanced CNN Architectures and Transfer Learning), Lecture 19 (Representation Learning, Vision Transformers, and Autoregressive Generation)\n\n\nPrincipal Component Analysis (PCA)\nA dimensionality reduction technique that transforms data into a new coordinate system where the greatest variance lies along the first coordinate (first principal component), the second greatest variance along the second coordinate, and so on.\nRelated lectures: Lecture 7 (Nonlinearities and Expressive Learning Methods)\n\n\nPrior\nA probability distribution \\(P(\\boldsymbol{\\theta})\\) that expresses beliefs about model parameters before observing data. Within Bayesian statistics, priors:\n\nEncode domain knowledge or subjective beliefs\nRange from informative (strong beliefs) to diffuse/non-informative (minimal assumptions)\nAre updated to posterior distributions through Bayes’ theorem\nCan have significant impact with small datasets but diminish in influence as data increases\n\nCommon examples include:\n\nNormal priors for regression coefficients\nLaplace priors leading to L1 regularization (LASSO)\nNormal priors leading to L2 regularization (Ridge)\nDirichlet priors for multinomial parameters\nConjugate priors that result in closed-form posteriors\n\nRelated lectures: Lecture 22 (Bayesian Machine Learning)\n\n\nPrecision\nA classification metric measuring the proportion of true positive predictions among all positive predictions: TP/(TP+FP). High precision means a low false positive rate, which is important when the cost of a false positive is high.\nRelated lectures: Lecture 3 (Generalization Error)"
  },
  {
    "objectID": "lectures/glossary.html#q",
    "href": "lectures/glossary.html#q",
    "title": "Machine Learning Glossary",
    "section": "Q",
    "text": "Q\n\nQ-Learning\nA model-free reinforcement learning algorithm that learns the value of actions in states (Q-values) and selects actions based on these values. The goal is to find an optimal policy that maximizes the expected cumulative reward.\nRelated lectures: Lecture 27 (Interpretable Machine Learning)"
  },
  {
    "objectID": "lectures/glossary.html#r",
    "href": "lectures/glossary.html#r",
    "title": "Machine Learning Glossary",
    "section": "R",
    "text": "R\n\nRandom Forest\nAn ensemble learning method that constructs multiple decision trees during training and outputs the mode of the classes (for classification) or mean prediction (for regression) of the individual trees. Reduces overfitting by combining many trees trained on different subsets of data and features.\nRelated lectures: Lecture 7 (Nonlinearities and Expressive Learning Methods)\n\n\nRecall\nA classification metric measuring the proportion of actual positive cases that were correctly identified: TP/(TP+FN). High recall means a low false negative rate, which is important when the cost of missing a positive case is high.\nRelated lectures: Lecture 3 (Generalization Error)\n\n\nRecurrent Neural Networks (RNNs)\nNeural network architectures designed to process sequential data by maintaining internal state through recurrent connections. The defining feature is the recurrence relation: h_t = f_w(h_{t-1}, x_t), where h_t is the current hidden state, h_{t-1} is the previous hidden state, and x_t is the current input.\nRNNs can handle various sequence relationships:\n\nOne-to-Many: Single input generates a sequence (e.g., image captioning)\nMany-to-One: Sequence produces single output (e.g., sentiment analysis)\nMany-to-Many (Unaligned): Input sequence yields different-length output sequence (e.g., translation)\nMany-to-Many (Aligned): Each input element corresponds to an output element (e.g., part-of-speech tagging)\n\nChallenges include vanishing and exploding gradients when processing long sequences, which specialized architectures like LSTMs and GRUs address.\nRelated lectures: Lecture 16 (Recurrent Neural Networks and Sequence Modeling)\n\n\nRegularization\nTechniques to prevent overfitting by adding constraints or penalties to model complexity. Common methods include:\n\nL1 regularization: Encourages sparse models by penalizing absolute weight values\nL2 regularization: Penalizes large weights using squared magnitude\nDropout: Randomly ignores neurons during training\nEarly stopping: Halts training when validation performance stops improving\nData augmentation: Artificially expands training data with transformations\nWeight decay: Gradually reduces weights during training\n\nRelated lectures: Lecture 4 (Linear Models and Likelihood), Lecture 11 (Vanishing Gradients and Generalization)\n\n\nResNet (Residual Network)\nA deep neural network architecture that addresses the vanishing gradient problem by introducing skip connections (residual connections) that allow gradients to flow directly through the network. This innovation enables training of very deep networks with hundreds of layers.\nRelated lectures: Lecture 13 (Advanced CNN Architectures and Transfer Learning)"
  },
  {
    "objectID": "lectures/glossary.html#s",
    "href": "lectures/glossary.html#s",
    "title": "Machine Learning Glossary",
    "section": "S",
    "text": "S\n\nSequence Modeling\nA machine learning paradigm focusing on data where order and temporal dependencies matter. Sequence models can handle various input-output relationships:\n\nOne-to-Many: Single input generates a sequence (e.g., image captioning)\nMany-to-One: Sequence produces single output (e.g., sentiment analysis)\nMany-to-Many (Unaligned): Input sequence yields different-length output sequence (e.g., machine translation)\nMany-to-Many (Aligned): Each input element corresponds to an output element (e.g., part-of-speech tagging)\n\nArchitectures for sequence modeling include RNNs, LSTMs, GRUs, Transformers, and specialized convolutional architectures.\nRelated lectures: Lecture 16 (Recurrent Neural Networks and Sequence Modeling)\n\n\nSemantic Segmentation\nComputer vision task that classifies each pixel in an image into a predefined category, assigning a semantic label to every pixel. Unlike instance segmentation, it doesn’t distinguish between different instances of the same class.\nRelated lectures: Lecture 15 (Semantic Segmentation and Advanced CNN Applications)\n\n\nSelf-Attention\nA neural network mechanism that allows models to directly connect any position in a sequence to any other position, enabling the model to focus on different parts of the input when producing outputs. The self-attention operation works by:\n\nComputing three vectors for each token through learned linear projections:\n\nQuery vector (q_i): Represents what information the token is “looking for”\nKey vector (k_i): Represents what information the token “contains”\nValue vector (v_i): Represents the actual content of the token\n\nCalculating attention weights by comparing queries to keys: e_{ij} = (q_i^T k_j)/√d_k\nNormalizing weights using softmax function\nComputing a weighted sum of all value vectors\n\nThis mechanism forms the core of transformer architectures and enables constant computational complexity regardless of sequence distance.\nRelated lectures: Lecture 18 (Transformers and the Attention Revolution)\n\n\nSelf-Supervised Learning\nTraining paradigm where models generate supervisory signals from the data itself without explicit labels. The model creates pretext tasks from unlabeled data (like predicting masked tokens, image rotations, or contrastive objectives) to learn useful representations, which can then be fine-tuned for downstream tasks with limited labeled data.\nRelated lectures: Lecture 19 (Representation Learning, Vision Transformers, and Autoregressive Generation)\n\n\nSoftmax Function\nA function that converts a vector of real numbers into a probability distribution. It exponentiates each input and normalizes by the sum of all exponentiated values, ensuring all outputs are between 0 and 1 and sum to 1. Commonly used for multi-class classification.\nRelated lectures: Lecture 4 (Linear Models and Likelihood), Lecture 8 (Introduction to Neural Networks)\n\n\nStochastic Gradient Descent (SGD)\nAn optimization algorithm that updates model parameters using the gradient of the loss function with respect to those parameters, computed on randomly selected subsets (mini-batches) of the training data rather than the entire dataset.\nRelated lectures: Lecture 5 (Loss Minimization and Optimization), Lecture 10 (Backpropagation and Gradient Problems)\n\n\nSupport Vector Machine (SVM)\nA supervised learning algorithm that finds a hyperplane in an N-dimensional space that distinctly separates data points of different classes. SVMs maximize the margin between classes and can perform non-linear classification using the kernel trick.\nRelated lectures: Lecture 7 (Nonlinearities and Expressive Learning Methods)"
  },
  {
    "objectID": "lectures/glossary.html#t",
    "href": "lectures/glossary.html#t",
    "title": "Machine Learning Glossary",
    "section": "T",
    "text": "T\n\nTemperature Scaling\nA post-processing calibration technique for deep neural networks that divides the logits (pre-softmax outputs) by a scalar parameter called temperature. Higher temperatures produce softer probability distributions, while lower temperatures make them sharper.\nRelated lectures: Lecture 4 (Linear Models and Likelihood), Lecture 27 (Interpretable Machine Learning)\n\n\nTransfer Learning\nA machine learning technique where knowledge gained from training a model on one task is applied to a different but related task. This approach is particularly effective when the target task has limited training data. Common strategies include:\n\nFeature extraction: Reuse learned features from pre-trained models\nFine-tuning: Adapt a pre-trained model by updating some or all of its parameters for a new task\n\nRelated lectures: Lecture 13 (Advanced CNN Architectures and Transfer Learning), Lecture 19 (Representation Learning, Vision Transformers, and Autoregressive Generation)\n\n\nTransformers\nNeural network architecture introduced in “Attention is All You Need” (Vaswani et al., 2017) that overcomes limitations of recurrent networks through self-attention mechanisms. Transformers offer several key advantages:\n\nParallel processing of sequences rather than sequential computation\nConstant path length between any positions in a sequence\nDirect modeling of long-range dependencies\nHighly parallelizable computation\n\nThe architecture consists of:\n\nMulti-head self-attention: Allows the model to focus on different aspects of the input simultaneously\nPosition encodings: Incorporate sequence order information since attention has no inherent position awareness\nFeed-forward networks: Process attention outputs with two linear transformations and a non-linearity\nLayer normalization: Stabilizes training by normalizing activations\nResidual connections: Help with gradient flow in deep networks\n\nTransformers have revolutionized NLP with models like BERT (bidirectional encoding), GPT (autoregressive generation), and T5 (sequence-to-sequence), and have been adapted for vision (ViT), audio, and multimodal tasks.\nRelated lectures: Lecture 18 (Transformers and the Attention Revolution), Lecture 19 (Representation Learning, Vision Transformers, and Autoregressive Generation)"
  },
  {
    "objectID": "lectures/glossary.html#u",
    "href": "lectures/glossary.html#u",
    "title": "Machine Learning Glossary",
    "section": "U",
    "text": "U\n\nUnderfitting\nWhen a model is too simple to capture the underlying pattern in the data, resulting in poor performance on both training and test data. Signs include high bias and high error on training data.\nRelated lectures: Lecture 3 (Generalization Error), Lecture 4 (Linear Models and Likelihood)\n\n\nUniversal Approximation Theorem\nA mathematical result stating that a feedforward neural network with a single hidden layer containing a finite number of neurons can approximate any Borel measurable function from one finite-dimensional space to another with arbitrary precision, under mild assumptions about the activation function. Key aspects include:\n\nProvides the theoretical foundation for neural networks’ ability to model complex, nonlinear relationships\nExplains why neural networks are considered “universal approximators”\nDespite theoretical capability to represent any function, practical limitations arise from optimization challenges and overfitting\nIndicates neural networks’ capacity to achieve zero training error given sufficient resources\n\nRelated lectures: Lecture 7 (Nonlinearities and Expressive Learning Methods)\n\n\nUnsupervised Learning\nA type of machine learning where the algorithm is given data without explicit instructions on what to do with it. The system tries to learn the underlying structure or distribution of the data on its own. Common applications include clustering, dimensionality reduction, and density estimation.\nRelated lectures: Lecture 2 (Machine Learning Fundamentals)"
  },
  {
    "objectID": "lectures/glossary.html#v",
    "href": "lectures/glossary.html#v",
    "title": "Machine Learning Glossary",
    "section": "V",
    "text": "V\n\nValidation Set\nA portion of the data set aside from training data to tune hyperparameters and evaluate model performance during development. It helps detect overfitting and guides model selection without contaminating the test set.\nRelated lectures: Lecture 3 (Generalization Error)\n\n\nVariational Autoencoders (VAEs)\nProbabilistic generative models that learn a latent space representation of data using an encoder-decoder architecture with a variational inference approach. VAEs:\n\nEncode inputs to parameters of probability distributions in latent space (typically mean and variance of Gaussian distributions)\nRegularize the latent space by forcing it towards a standard normal distribution using KL divergence\nSample from the latent distribution using the reparameterization trick\nDecode the sampled point to reconstruct the input\nEnable both reconstruction and generation of new data points\n\nRelated lectures: Lecture 23 (Variational Autoencoders)\n\n\nVariational Bayesian Inference\nAn approximate Bayesian inference method that transforms posterior computation into an optimization problem by:\n\nIntroducing a simpler parametric family of distributions \\(Q(\\boldsymbol{\\theta}|\\boldsymbol{\\phi})\\) to approximate the true posterior \\(P(\\boldsymbol{\\theta}|\\mathcal{D})\\)\nFinding the parameters \\(\\boldsymbol{\\phi}\\) that minimize the KL divergence between the approximation and the true posterior\nOptimizing the Evidence Lower Bound (ELBO): \\(\\mathcal{L}(\\boldsymbol{\\phi}) = \\mathbb{E}_Q[\\log P(\\mathcal{D}|\\boldsymbol{\\theta})] - D_{KL}(Q(\\boldsymbol{\\theta}|\\boldsymbol{\\phi})||P(\\boldsymbol{\\theta}))\\)\n\nKey advantages include:\n\nScalability to large datasets through stochastic optimization\nApplicability to models with non-conjugate priors\nAbility to handle complex posterior distributions\nAmortized inference in the case of variational autoencoders\n\nRelated lectures: Lecture 22 (Bayesian Machine Learning), Lecture 23 (Variational Autoencoders)\n\n\nVanishing/Exploding Gradients\nChallenges in training deep neural networks where gradients become extremely small (vanishing) or large (exploding) during backpropagation through many layers. These problems:\n\nMake it difficult to learn long-range dependencies\nCause unstable or stalled training\nCan be addressed through techniques like careful weight initialization, gradient clipping, residual connections, batch normalization, and specialized architectures like LSTMs and GRUs\n\nRelated lectures: Lecture 10 (Backpropagation and Gradient Problems), Lecture 11 (Vanishing Gradients and Generalization)\n\n\nVision Transformer (ViT)\nAn adaptation of the transformer architecture for computer vision tasks that treats an image as a sequence of patches, using self-attention to process relationships between all patches simultaneously. ViTs have achieved state-of-the-art performance on image classification and other vision tasks.\nRelated lectures: Lecture 19 (Representation Learning, Vision Transformers, and Autoregressive Generation)"
  },
  {
    "objectID": "lectures/glossary.html#w",
    "href": "lectures/glossary.html#w",
    "title": "Machine Learning Glossary",
    "section": "W",
    "text": "W\n\nWasserstein GAN (WGAN)\nA variant of Generative Adversarial Networks that uses the Wasserstein distance (Earth Mover’s distance) instead of Jensen-Shannon divergence to measure the difference between the generated and real data distributions. WGANs offer more stable training and better quality gradients.\nRelated lectures: Lecture 25 (Generative Adversarial Networks)\n\n\nWeight Initialization\nStrategies for setting the initial values of neural network weights before training begins. Proper initialization helps prevent vanishing/exploding gradients and accelerates convergence. Common methods include Xavier/Glorot initialization and He initialization.\nRelated lectures: Lecture 10 (Backpropagation and Gradient Problems), Lecture 13 (Advanced CNN Architectures and Transfer Learning)"
  },
  {
    "objectID": "lectures/glossary.html#z",
    "href": "lectures/glossary.html#z",
    "title": "Machine Learning Glossary",
    "section": "Z",
    "text": "Z\n\nZero-shot Learning\nA machine learning paradigm where a model can recognize objects or perform tasks it hasn’t encountered during training. The model learns to generalize from seen classes to unseen classes by leveraging additional information like semantic attributes or embeddings.\nRelated lectures: Lecture 19 (Representation Learning, Vision Transformers, and Autoregressive Generation)"
  },
  {
    "objectID": "index.html#about-this-course",
    "href": "index.html#about-this-course",
    "title": "",
    "section": "About This Course",
    "text": "About This Course\nWelcome to QTM 447! This course extends the basic statistical learning topics covered in QTM 347, diving deep into modern machine learning methods and their applications.\n\n\nInstructor: Kevin McAlister\nEmail: kevin.mcalister@emory.edu\n\n\nClass Time: T/Th 2:30 - 3:45 PM EST\nLocation: Atwood 215\n\n\nOffice Hours:\nMondays 4:00 - 5:30 PM (PAIS 579)\nWednesdays 5:30 - 6:30 PM (PAIS 579)"
  },
  {
    "objectID": "index.html#course-overview",
    "href": "index.html#course-overview",
    "title": "",
    "section": "Course Overview",
    "text": "Course Overview\n\n\n\n\n\n\nReview and Advanced Theory\n\n\n\nPredictive modeling principles\n\n\nStatistical learning theory\n\n\nOptimization methods (gradient descent, ADAM)\n\n\n\n\n\n\n\n\nNeural Networks and Deep Learning\n\n\n\nPerceptrons and universal approximators\n\n\nDeep vs. Shallow learning\n\n\nCNNs, RNNs, and Transformers\n\n\n\n\n\n\n\n\nGeneration Theory\n\n\n\nDensity estimation\n\n\nBayesian approaches\n\n\nSampling methods\n\n\n\n\n\n\n\n\nGenerative Methods\n\n\n\nVariational Autoencoders\n\n\nAutoregressive Models (GPT)\n\n\nGenerative Adversarial Networks"
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "",
    "section": "Prerequisites",
    "text": "Prerequisites\n\n\n\n\n\n\nRequired Coursework\n\n\nStudents should have completed coursework equivalent to:\n\n\n\nAn introduction to research design\n\n\nAn introduction to probability theory and statistics\n\n\nAn introduction to applied statistics and linear regression\n\n\nAn introduction to statistical machine learning\n\n\nAt least one college level programming class\n\n\n\n\n\n\n\n\nMathematical Foundation\n\n\nStudents should have a strong foundation in:\n\n\n\nCalculus\n\n\nMultivariable calculus\n\n\nBasic matrix calculus\n\n\n\n\nLinear Algebra"
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "",
    "section": "Getting Started",
    "text": "Getting Started\n\n\n\n\n\n\n\nReview the Syllabus\n\n\nCheck the syllabus for detailed course policies, grading criteria, and important deadlines.\n\n\n\n\n\n\n\n\n\nAccess Course Materials\n\n\nVisit the lectures page to find slides, code notebooks, and supplementary resources.\n\n\n\n\n\n\n\n\n\nSet Up Your Environment\n\n\nInstall Python and necessary packages for machine learning projects and assignments.\n\n\n\n\n\n\n\n\n\nProgramming Note\n\n\nThis course uses Python and various machine learning frameworks. While coding expertise isn’t required, you should be comfortable with basic programming concepts. We’ll be using tools like TensorFlow, PyTorch, and Google Colab for computationally intensive tasks.\n\n\n\nQuestions?\n\n\n\n\n\nIf you have any questions, please don’t hesitate to reach out via email or during office hours. For the most efficient communication, please check Canvas and the syllabus before asking questions.\n\n\n  Email    Course Policies"
  },
  {
    "objectID": "index.html#questions",
    "href": "index.html#questions",
    "title": "",
    "section": "Questions?",
    "text": "Questions?\n\n\n\n\n\nIf you have any questions, please don’t hesitate to reach out via email or during office hours. For the most efficient communication, please check Canvas and the syllabus before asking questions.\n\n\n  Email    Course Policies"
  },
  {
    "objectID": "lectures/index.html",
    "href": "lectures/index.html",
    "title": "Course Lectures",
    "section": "",
    "text": "Welcome to the lecture materials for QTM 447! Each lecture page contains three types of resources to help you master the concepts of statistical machine learning:\n\n\n\nInteractive HTML presentations covering theoretical concepts, mathematical foundations, and practical examples.\n\n\n\nPython notebooks containing code to reproduce all figures and demonstrations from the slides.\n\n\n\nSupplementary Python notebooks with extended examples and implementations of key algorithms."
  },
  {
    "objectID": "lectures/index.html#course-materials",
    "href": "lectures/index.html#course-materials",
    "title": "Course Lectures",
    "section": "",
    "text": "Welcome to the lecture materials for QTM 447! Each lecture page contains three types of resources to help you master the concepts of statistical machine learning:\n\n\n\nInteractive HTML presentations covering theoretical concepts, mathematical foundations, and practical examples.\n\n\n\nPython notebooks containing code to reproduce all figures and demonstrations from the slides.\n\n\n\nSupplementary Python notebooks with extended examples and implementations of key algorithms."
  },
  {
    "objectID": "lectures/lecture10.html",
    "href": "lectures/lecture10.html",
    "title": "Lecture 10: Backpropogation: The microwave dinner of optimization algorithms",
    "section": "",
    "text": "← Previous: Lecture 9\n\n\nNext: Lecture 11 →"
  },
  {
    "objectID": "lectures/lecture10.html#backpropagation",
    "href": "lectures/lecture10.html#backpropagation",
    "title": "Lecture 10: Backpropogation: The microwave dinner of optimization algorithms",
    "section": "Backpropagation",
    "text": "Backpropagation\nBackpropagation is an algorithmic approach to compute gradients efficiently in deep neural networks. The algorithm consists of two key steps:\n\nForward Pass: Starting from input, compute all intermediate values up to the loss function\nBackward Pass: Propagate gradients from the loss function back to all parameters\n\nFor a deep neural network, the loss can be expressed as:\n\\[\\mathcal{L}(\\boldsymbol{\\theta} | \\mathbf{X}, \\mathbf{y}) = \\frac{1}{N} \\sum_{i=1}^N \\mathcal{L}(\\theta_i | \\mathbf{x}_i, y_i)\\]\nWhere the output \\(\\theta_i\\) is defined as:\n\\[\\theta_i = g(\\boldsymbol{\\beta}^T \\varphi(\\mathbf{W}_D^T \\varphi(\\mathbf{W}_{D-1}^T \\varphi(\\mathbf{W}_{D-2}^T ... \\varphi(\\mathbf{W}_1^T \\mathbf{x}_i)))))\\]\n\nComputational Graphs\nBackpropagation operates on computational graphs, where:\n\nEach operation in the network is represented as a node in a directed acyclic graph\nThe graph consists of primitive operations (multiplication, addition) and activation functions\nParameters to be learned are represented as special nodes\n\nDuring the backward pass, gradients are computed using the chain rule:\n\\[\\text{Downstream Gradient} = \\text{Local Gradient} \\times \\text{Upstream Gradient}\\]\nOr mathematically:\n\\[\\frac{\\partial \\mathcal{L}}{\\partial x} = \\frac{\\partial \\mathcal{L}}{\\partial q} \\frac{\\partial q}{\\partial x}\\]\nIn a simple logistic regression, the gradient for the coefficients can be derived as:\n\\[\\frac{\\partial \\mathcal{L}_i}{\\partial \\mathbf{W}} = (\\sigma(z_i) - y_i)\\mathbf{x}_i\\]\n\\[\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}} = \\frac{1}{N} \\sum_{i=1}^N (\\sigma(z_i) - y_i)\\mathbf{x}_i\\]\n\n\nNeural Network Gradients\nFor multi-layer neural networks, gradients become more complex. In a two-layer network with ReLU activations:\n\\[\\mathcal{L}(\\mathbf{z} | \\mathbf{X}, \\mathbf{y}) = -\\frac{1}{N} \\sum_{i=1}^N y_i \\log \\sigma(z_i) + (1 - y_i)(1 - \\log\\sigma(z_i))\\]\n\\[z_i = \\boldsymbol{\\beta}^T \\mathbf{h}_{i2} + a\\]\n\\[\\mathbf{h}_{i2} = \\varphi(\\mathbf{q}_{i2})\\]\n\\[\\mathbf{q}_{i2} = \\mathbf{W}_2^T\\mathbf{h}_{i1} + \\mathbf{b}_2\\]\n\\[\\mathbf{h}_{i1} = \\varphi(\\mathbf{q}_{i1})\\]\n\\[\\mathbf{q}_{i1} = \\mathbf{W}_1^T\\mathbf{x}_i + \\mathbf{b}_1\\]\nThe gradient flows backward through the network, with each parameter receiving gradients that are products of all upstream derivatives:\n\\[\\frac{\\partial \\mathcal{L}_i}{\\partial \\mathbf{W}_1} = \\mathbf{u}^T \\times \\mathbf{x}_i^T\\]\nWhere \\(\\mathbf{u}\\) represents the accumulated upstream gradient:\n\\[\\mathbf{u} = [\\sigma(z_i) - y_i] \\otimes \\boldsymbol{\\beta}^T \\odot I(\\mathbf{q}_{i2} &gt; 0) \\otimes \\mathbf{W}_{2} \\odot I(\\mathbf{q}_{i1} &gt; 0)\\]"
  },
  {
    "objectID": "lectures/lecture10.html#gradient-problems",
    "href": "lectures/lecture10.html#gradient-problems",
    "title": "Lecture 10: Backpropogation: The microwave dinner of optimization algorithms",
    "section": "Gradient Problems",
    "text": "Gradient Problems\nAs neural networks become deeper, two major gradient-related problems can arise:\n\nExploding Gradients\nGradient explosion occurs when the chain of multiplication in backpropagation causes gradients to become extremely large:\n\\[(1 + \\epsilon)^D \\rightarrow \\infty\\]\nWhere \\(D\\) is the depth of the network and \\(\\epsilon\\) is a small positive value. This causes:\n\nUnstable training with large, erratic parameter updates\nInability to converge to a solution\nPotential numerical overflow issues\n\nApproaches to address exploding gradients include:\n\nGradient Clipping: Limiting the magnitude of gradients\n\\[\\mathbf{g}'(\\theta_{t-1}) = \\text{min}\\left(1, \\frac{c}{||\\mathbf{g}(\\theta_{t-1})||}\\right)\\mathbf{g}(\\theta_{t-1})\\]\nWhere \\(c\\) is a positive threshold value (often around 3).\nFeature Normalization: Ensuring input features are standardized or scaled to [0,1]\nWeight Initialization: Using appropriate initialization methods to control gradient scale\n\n\n\nVanishing Gradients\nVanishing gradients occur when the chain of multiplication causes gradients to become extremely small:\n\\[(1 - \\epsilon)^D \\rightarrow 0\\]\nThis leads to:\n\nParameters in early layers receiving minimal updates\nDifficulty in learning long-range dependencies\nTraining stagnation\n\nSpecific instances include “dead ReLUs,” where ReLU units output zero for all inputs, causing gradients to be zero.\n\n\nProper Initialization\nWeight initialization plays a crucial role in preventing gradient problems:\n\nRandom initialization from a normal distribution can lead to variance explosion as network depth increases\nThe variance of hidden units scales with the number of incoming connections (\\(N_{in}\\))\nThe variance of gradients scales with the number of outgoing connections (\\(N_{out}\\))\n\nGlorot Initialization addresses this by sampling weights from a distribution with:\n\\[\\sigma^2_{W_k} = \\frac{2}{N_{in} + N_{out}}\\]\nThis keeps the variance of activations and gradients stable across layers, enabling more effective training of deep networks."
  },
  {
    "objectID": "lectures/lecture12.html",
    "href": "lectures/lecture12.html",
    "title": "Lecture 12: Convolutional Neural Networks for Image Classification",
    "section": "",
    "text": "← Previous: Lecture 11\n\n\nNext: Lecture 13 →"
  },
  {
    "objectID": "lectures/lecture12.html#limitations-of-fully-connected-networks-for-images",
    "href": "lectures/lecture12.html#limitations-of-fully-connected-networks-for-images",
    "title": "Lecture 12: Convolutional Neural Networks for Image Classification",
    "section": "Limitations of Fully Connected Networks for Images",
    "text": "Limitations of Fully Connected Networks for Images\nFully connected neural networks (FCNNs) encounter several challenges when applied to image data:\n\nLoss of Spatial Information: When images are flattened into vectors, the spatial relationships between neighboring pixels are lost.\nLack of Translational Invariance: FCNNs are sensitive to the exact position of features within an image, making them inefficient for pattern recognition tasks where the position of a feature might vary.\nParameter Explosion: For even modest-sized images, FCNNs require an enormous number of parameters. A 28×28 grayscale image with 128 hidden units requires 100,352 parameters in just the first layer.\nInefficient Feature Learning: FCNNs struggle to identify localized patterns that are critical for image recognition."
  },
  {
    "objectID": "lectures/lecture12.html#convolutional-neural-networks",
    "href": "lectures/lecture12.html#convolutional-neural-networks",
    "title": "Lecture 12: Convolutional Neural Networks for Image Classification",
    "section": "Convolutional Neural Networks",
    "text": "Convolutional Neural Networks\nConvolutional Neural Networks (CNNs) address these limitations through specialized layers designed for processing grid-like data, particularly images.\n\nConvolution Operations\nThe core operation in CNNs is the convolution, which slides a small weight matrix (kernel or filter) over the input image, computing the dot product at each position:\n\\[(\\mathbf{A} \\circledast \\mathbf{W})_{ij} = \\sum_{m} \\sum_{n} \\mathbf{A}_{i+m,j+n} \\mathbf{W}_{m,n}\\]\nFor a 2D input with a 2×2 filter example:\n\\[c_{11} = a_{11}w_{11} + a_{12}w_{12} + a_{21}w_{21} + a_{22}w_{22}\\]\nThis operation produces a feature map that highlights regions where patterns match the filter.\n\n\nOutput Dimensions\nFor an input of size \\(n_h \\times n_w\\) and a filter of size \\(f_h \\times f_w\\), the output dimensions are:\n\\[\\text{Output Size} = (n_h - f_h + 1) \\times (n_w - f_w + 1)\\]\nWith zero-padding of \\(p\\) and stride \\(s\\), the output dimensions become:\n\\[\\text{Output Size} = \\left(\\frac{n_h + 2p - f_h + s}{s}\\right) \\times \\left(\\frac{n_w + 2p - f_w + s}{s}\\right)\\]\n\n\nKey Properties\nConvolution operations provide several advantages:\n\nParameter Sharing: The same filter is applied across the entire image, dramatically reducing the number of parameters. A 5×5 filter requires only 25 parameters compared to thousands in FCNNs.\nTranslational Invariance: Patterns are detected regardless of their position in the image.\nHierarchical Feature Learning: Early layers detect simple features (edges, corners), while deeper layers combine these to recognize complex patterns."
  },
  {
    "objectID": "lectures/lecture12.html#cnn-architecture-components",
    "href": "lectures/lecture12.html#cnn-architecture-components",
    "title": "Lecture 12: Convolutional Neural Networks for Image Classification",
    "section": "CNN Architecture Components",
    "text": "CNN Architecture Components\n\nConvolutional Layers\nEach convolutional layer applies a set of learnable filters to produce multiple feature maps:\n\\[\\text{Feature Map}_k = \\text{ReLU}(\\mathbf{W}_k \\circledast \\mathbf{X} + b_k)\\]\nWhere:\n\n\\(\\mathbf{W}_k\\) is the \\(k\\)-th filter\n\\(b_k\\) is the bias term\nReLU is the activation function\n\n\n\nPooling Layers\nPooling reduces the spatial dimensions of feature maps while preserving important information:\n\nMax Pooling: Takes the maximum value in each window\n\\[\\text{MaxPool}(A)_{ij} = \\max_{m,n \\in \\text{window}} A_{i+m,j+n}\\]\nAverage Pooling: Takes the average of values in each window\n\nFor a pooling operation with filter size \\(K \\times K\\) and stride \\(s\\), the output size is:\n\\[\\text{Output Size} = \\left(\\frac{H - K}{s} + 1\\right) \\times \\left(\\frac{W - K}{s} + 1\\right)\\]\nPooling provides:\n\nFurther translational invariance\nReduced computational complexity\nResistance to small distortions\n\n\n\nMulti-Channel Convolutions\nFor RGB images or multi-channel feature maps, filters extend through all input channels:\n\nInput: \\(C_{in} \\times H \\times W\\)\nFilter: \\(C_{in} \\times K_h \\times K_w\\)\nOutput: Single channel feature map\n\nWith \\(C_{out}\\) filters, the output becomes a \\(C_{out} \\times H_{out} \\times W_{out}\\) feature map."
  },
  {
    "objectID": "lectures/lecture12.html#standard-cnn-architecture",
    "href": "lectures/lecture12.html#standard-cnn-architecture",
    "title": "Lecture 12: Convolutional Neural Networks for Image Classification",
    "section": "Standard CNN Architecture",
    "text": "Standard CNN Architecture\nThe LeNet-5 architecture (1998) exemplifies the standard CNN structure:\n\nInput Layer: Raw image (e.g., 1×28×28 for MNIST)\nConvolutional Layer: Apply filters to detect features\nActivation Layer: Apply nonlinearity (typically ReLU)\nPooling Layer: Reduce spatial dimensions\nRepeat Steps 2-4: Build hierarchical representations\nFully Connected Layers: Combine features for classification\nOutput Layer: Final classification (e.g., softmax for multi-class)\n\nThis architecture progressively transforms the input from pixel values to high-level features:\n\nFirst-layer filters typically detect edges and simple textures\nMiddle layers combine these to form parts of objects\nDeeper layers represent complete objects or scenes"
  },
  {
    "objectID": "lectures/lecture12.html#advantages-of-cnns",
    "href": "lectures/lecture12.html#advantages-of-cnns",
    "title": "Lecture 12: Convolutional Neural Networks for Image Classification",
    "section": "Advantages of CNNs",
    "text": "Advantages of CNNs\nCNNs offer significant benefits for image processing:\n\nEfficiency: Despite performing many operations, convolutions are computationally efficient, especially on GPUs.\nReduced Parameters: A CNN with 6 filters of size 5×5 requires only 150 parameters, compared to 4,704 for an equivalent FCNN.\nPerformance: CNNs consistently outperform FCNNs on image tasks in both accuracy and training speed.\nFeature Interpretability: Convolutional filters often correspond to human-interpretable image features, unlike the abstract representations in FCNNs.\n\nThese advantages have made CNNs the dominant architecture for computer vision tasks, including image classification, object detection, and segmentation."
  },
  {
    "objectID": "lectures/lecture14.html",
    "href": "lectures/lecture14.html",
    "title": "Lecture 14: Object Detection in Images",
    "section": "",
    "text": "← Previous: Lecture 13\n\n\nNext: Lecture 15 →"
  },
  {
    "objectID": "lectures/lecture14.html#computer-vision-tasks-beyond-classification",
    "href": "lectures/lecture14.html#computer-vision-tasks-beyond-classification",
    "title": "Lecture 14: Object Detection in Images",
    "section": "Computer Vision Tasks Beyond Classification",
    "text": "Computer Vision Tasks Beyond Classification\nImage classification—assigning a single label to an entire image—is just one of many computer vision tasks. More advanced applications require models that can:\n\nLocate multiple objects within images\nIdentify the class of each object\nPrecisely mark boundaries around detected objects\n\nObject detection addresses the first two tasks, providing both the classification (what) and localization (where) of objects in a scene."
  },
  {
    "objectID": "lectures/lecture14.html#transfer-learning-for-advanced-vision-tasks",
    "href": "lectures/lecture14.html#transfer-learning-for-advanced-vision-tasks",
    "title": "Lecture 14: Object Detection in Images",
    "section": "Transfer Learning for Advanced Vision Tasks",
    "text": "Transfer Learning for Advanced Vision Tasks\nWhen approaching complex vision tasks, transfer learning offers significant advantages:\n\nFeature Extraction Approach\n\nUse pre-trained CNN backbone (often from ImageNet) as a feature extractor\nFreeze the convolutional layers to maintain learned representations\nAdd task-specific heads for new objectives\nTrain only the new heads while keeping the feature extractor fixed\n\n\n\nFine-Tuning Approach\n\nInitialize model with pre-trained weights\nAdd task-specific layers for the desired task\nTrain in stages:\n\nTrain only the new layers with backbone frozen\nUnfreeze deeper layers gradually (“progressive unfreezing”)\nTrain the entire network with a very small learning rate\n\n\nBest practices for fine-tuning include:\n\nUse significantly lower learning rates (10-100× smaller)\nRequire substantial task-specific data\nOnly unfreeze deeper layers when necessary"
  },
  {
    "objectID": "lectures/lecture14.html#object-detection-fundamentals",
    "href": "lectures/lecture14.html#object-detection-fundamentals",
    "title": "Lecture 14: Object Detection in Images",
    "section": "Object Detection Fundamentals",
    "text": "Object Detection Fundamentals\nObject detection combines classification with localization to identify multiple objects in images.\n\nTask Definition\nFor each object in an image, predict:\n\nClass label (from a predefined set of categories)\nBounding box coordinates, typically represented as:\n\n\\((x_{min}, y_{min}, width, height)\\) or\n\\((x_{min}, y_{min}, x_{max}, y_{max})\\)\n\n\nThese bounding boxes are usually axis-aligned (parallel to image edges) for simplicity and computational efficiency.\n\n\nEvaluation Metrics\nThe primary metric for measuring bounding box accuracy is Intersection over Union (IoU):\n\\[IoU = \\frac{\\text{Area of Intersection}}{\\text{Area of Union}}\\]\nIoU ranges from 0 (no overlap) to 1 (perfect overlap) and provides a geometric measure of detection quality."
  },
  {
    "objectID": "lectures/lecture14.html#two-stage-detectors-r-cnn-family",
    "href": "lectures/lecture14.html#two-stage-detectors-r-cnn-family",
    "title": "Lecture 14: Object Detection in Images",
    "section": "Two-Stage Detectors: R-CNN Family",
    "text": "Two-Stage Detectors: R-CNN Family\nTwo-stage detectors divide the detection process into region proposal and classification/refinement stages.\n\nR-CNN (Regions with CNN)\nThe original R-CNN approach follows these steps:\n\nRegion Proposal: Use selective search to identify ~2,000 potential object regions\n\nSelective search merges similar regions based on color, texture, and other visual cues\nMore efficient than exhaustive sliding windows (which would require billions of evaluations)\n\nFeature Extraction: Pass each proposed region through a CNN\n\nResize each region to fit CNN input dimensions\nExtract fixed-length feature vectors\n\nClassification: Apply SVM classifiers to categorize each region\n\nOne classifier per object category\nIncludes a background class for non-objects\n\nBounding Box Refinement: Apply regression to improve bounding box precision\n\n\n\nTraining Process\nThe training procedure for R-CNN requires defining positive and negative examples:\n\nPositive examples: Region proposals with IoU &gt; 0.5 with any ground truth box\nNegative examples (hard negatives): Region proposals with IoU &lt; 0.3 with all ground truth boxes\nIgnored examples: Region proposals with IoU between 0.3 and 0.5\n\n\n\nPrediction and Post-Processing\nAfter obtaining raw detections, Non-Maximum Suppression (NMS) removes redundant predictions:\n\nSelect the highest-scoring detection\nEliminate overlapping detections with IoU &gt; threshold (typically 0.7)\nRepeat until no redundant detections remain\n\n\n\nLimitations of Two-Stage Detectors\nDespite their accuracy, R-CNN and its variants (Fast R-CNN, Faster R-CNN) suffer from:\n\nHigh computational cost (~0.5-5 frames per second)\nComplex multi-stage pipeline\nImpractical for real-time applications"
  },
  {
    "objectID": "lectures/lecture14.html#single-shot-detectors",
    "href": "lectures/lecture14.html#single-shot-detectors",
    "title": "Lecture 14: Object Detection in Images",
    "section": "Single-Shot Detectors",
    "text": "Single-Shot Detectors\nSingle-shot detectors perform object detection in a single forward pass through the network, dramatically improving speed.\n\nYOLO (You Only Look Once)\nYOLO revolutionized object detection by reformulating it as a single regression problem:\n\nGrid Division: Divide the image into an S×S grid\nPrediction Structure: For each grid cell, predict:\n\nObject presence probability\nB bounding boxes with confidence scores\nClass probabilities\n\nOutput Representation: The output is a tensor of shape: \\[S \\times S \\times (B \\times 5 + C)\\] Where B is the number of boxes per cell and C is the number of classes\nUnified Architecture: Uses a fully convolutional network without separate proposal and classification stages\nJoint Loss Function: Combines localization, confidence, and classification losses:\n\nBounding box coordinate error (weighted higher for boxes containing objects)\nObjectness score error (presence/absence of objects)\nClassification error (only for cells containing objects)\n\n\n\n\nAdvantages of Single-Shot Detectors\nYOLO and similar models (SSD, RetinaNet) offer:\n\nReal-time detection speeds (45-155 FPS for YOLOv3-v9)\nEnd-to-end training\nReasonable accuracy, though historically slightly lower than two-stage detectors\nPractical deployment in resource-constrained environments\n\n\n\nEvolution of Detectors\nModern object detection has seen a convergence in performance:\n\nTwo-stage detectors offer higher accuracy but lower speed\nSingle-shot detectors offer higher speed but lower accuracy\nRecent advances (YOLOv4-v9, EfficientDet) have narrowed this gap\n\nThe choice between architectures depends on application requirements:\n\nSafety-critical applications may prioritize accuracy (two-stage)\nReal-time applications may prioritize speed (single-shot)\nMobile applications may prioritize efficiency (specialized single-shot)\n\nThis progression reflects a fundamental engineering tradeoff between speed and accuracy that continues to drive innovation in the field."
  },
  {
    "objectID": "lectures/lecture16.html",
    "href": "lectures/lecture16.html",
    "title": "Lecture 16: A Very Fast Lecture on Recurrent Neural Networks and LSTMs",
    "section": "",
    "text": "← Previous: Lecture 15\n\n\nNext: Lecture 17 →"
  },
  {
    "objectID": "lectures/lecture16.html#sequence-modeling-paradigms",
    "href": "lectures/lecture16.html#sequence-modeling-paradigms",
    "title": "Lecture 16: A Very Fast Lecture on Recurrent Neural Networks and LSTMs",
    "section": "Sequence Modeling Paradigms",
    "text": "Sequence Modeling Paradigms\nSequence modeling represents a fundamental shift from previous neural network architectures by introducing temporal dependencies. While feedforward networks map individual inputs to individual outputs, sequence models handle data where order and context matter.\n\nTypes of Sequence-to-Sequence Relationships\nSequence models can accommodate various input-output relationships:\n\nOne-to-Many: A single input generates a sequence of outputs\n\nExample: Image captioning (one image → sequence of words)\n\nMany-to-One: A sequence of inputs produces a single output\n\nExample: Sentiment analysis (sequence of words → sentiment class)\nExample: Video classification (sequence of frames → video category)\n\nMany-to-Many (Unaligned): Sequence of inputs yields a different-length sequence of outputs\n\nExample: Machine translation (English sentence → French sentence)\nExample: Time series forecasting (historical data → future predictions)\n\nMany-to-Many (Aligned): Each input element corresponds to an output element\n\nExample: Part-of-speech tagging (each word → grammatical tag)\nExample: Per-frame video annotation (each frame → frame label)"
  },
  {
    "objectID": "lectures/lecture16.html#recurrent-neural-networks-rnns",
    "href": "lectures/lecture16.html#recurrent-neural-networks-rnns",
    "title": "Lecture 16: A Very Fast Lecture on Recurrent Neural Networks and LSTMs",
    "section": "Recurrent Neural Networks (RNNs)",
    "text": "Recurrent Neural Networks (RNNs)\nRecurrent Neural Networks extend feedforward architectures by introducing connections between hidden states across time steps.\n\nCore Concept: Hidden State Recurrence\nThe defining feature of RNNs is the recurrence relation:\n\\[h_t = f_w(h_{t-1}, x_t)\\]\nWhere:\n\n\\(h_{t-1}\\) is the previous hidden state vector\n\\(x_t\\) is the current input vector\n\\(f_w\\) is a parameterized function\n\\(h_t\\) is the new hidden state vector\n\n\n\nVanilla RNN Architecture\nThe standard RNN update function is:\n\\[\\mathbf{h}_t = \\tanh(\\mathbf{W}_{hh}\\mathbf{h}_{t-1} + \\mathbf{W}_{xh}\\mathbf{x}_t + \\mathbf{b}_h)\\]\nWhere:\n\n\\(\\mathbf{W}_{hh}\\) is the hidden-to-hidden weight matrix (dimension \\(m \\times m\\))\n\\(\\mathbf{W}_{xh}\\) is the input-to-hidden weight matrix (dimension \\(m \\times P\\))\n\\(\\mathbf{b}_h\\) is a bias vector\n\\(\\tanh\\) provides non-linearity while maintaining gradient flow\n\nThe output at each time step is computed as:\n\\[\\mathbf{y}_t = h(\\mathbf{W}_{hy}\\mathbf{h}_t + \\mathbf{b}_y)\\]\nWhere:\n\n\\(\\mathbf{W}_{hy}\\) is the hidden-to-output weight matrix\n\\(h()\\) is an appropriate activation function (e.g., softmax for classification)\n\n\n\nParameter Sharing\nA critical insight of RNNs is that the same weight matrices (\\(\\mathbf{W}_{hh}\\), \\(\\mathbf{W}_{xh}\\), \\(\\mathbf{W}_{hy}\\)) are used across all time steps, making the model:\n\nComputationally efficient\nAble to handle variable-length sequences\nCapable of learning patterns independent of their position in the sequence\n\n\n\nLanguage Modeling Example\nA character-level language model demonstrates the RNN’s ability to predict the next element in a sequence:\n\nInput: “h”, “e”, “l”, “l”\nTarget output: “e”, “l”, “l”, “o”\n\nAt inference time, the model can generate text by recursively feeding its own predictions back as inputs."
  },
  {
    "objectID": "lectures/lecture16.html#the-vanishing-gradient-problem",
    "href": "lectures/lecture16.html#the-vanishing-gradient-problem",
    "title": "Lecture 16: A Very Fast Lecture on Recurrent Neural Networks and LSTMs",
    "section": "The Vanishing Gradient Problem",
    "text": "The Vanishing Gradient Problem\nDespite their elegant design, vanilla RNNs suffer from a critical limitation when modeling long-range dependencies.\n\nBackpropagation Through Time\nTraining RNNs requires computing gradients through the entire sequence using backpropagation through time. For the weight matrix \\(\\mathbf{W}_{hh}\\), this gives:\n\\[\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}_{hh}} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}_T} \\frac{\\partial \\mathbf{h}_{1}}{\\partial \\mathbf{W}_{hh}} \\prod_{t=2}^T \\frac{\\partial \\mathbf{h}_t}{\\partial \\mathbf{h}_{t-1}}\\]\nFor the tanh activation, each partial derivative includes:\n\\[\\frac{\\partial \\mathbf{h}_{t}}{\\partial \\mathbf{h}_{t-1}} = \\tanh'(\\mathbf{W}_{hh}\\mathbf{h}_{t-1} + \\mathbf{W}_{xh}\\mathbf{x}_t)\\mathbf{W}_{hh}\\]\n\n\nMathematical Analysis of Gradient Flow\nTwo factors affect the gradient magnitude across time steps:\n\nThe derivative of tanh is always between 0 and 1\nRepeated multiplication by \\(\\mathbf{W}_{hh}\\) has compounding effects:\n\nIf the largest singular value of \\(\\mathbf{W}_{hh}\\) is &gt; 1: exploding gradients\nIf the largest singular value of \\(\\mathbf{W}_{hh}\\) is &lt; 1: vanishing gradients\n\n\nThis means that for long sequences, earlier inputs have diminishing influence on later outputs, making it difficult to learn long-range dependencies."
  },
  {
    "objectID": "lectures/lecture16.html#long-short-term-memory-lstm",
    "href": "lectures/lecture16.html#long-short-term-memory-lstm",
    "title": "Lecture 16: A Very Fast Lecture on Recurrent Neural Networks and LSTMs",
    "section": "Long Short-Term Memory (LSTM)",
    "text": "Long Short-Term Memory (LSTM)\nLSTM networks address the vanishing gradient problem by introducing a more complex hidden state architecture with controlled information flow.\n\nCell State and Gating Mechanism\nLSTMs maintain two types of hidden state vectors:\n\nCell state (\\(\\mathbf{C}_t\\)): A memory vector that can maintain information across many time steps\nHidden state (\\(\\mathbf{h}_t\\)): The output vector used for predictions\n\nInformation flow between states is regulated by three gating mechanisms:\n\nForget Gate (\\(\\mathbf{f}_t\\)): Controls what information from the previous cell state to discard \\[\\mathbf{f}_t = \\sigma(\\mathbf{W}_{f}[\\mathbf{h}_{t-1}, \\mathbf{x}_t] + \\mathbf{b}_f)\\]\nInput Gate (\\(\\mathbf{i}_t\\)): Determines what new information will be stored in the cell state \\[\\mathbf{i}_t = \\sigma(\\mathbf{W}_{i}[\\mathbf{h}_{t-1}, \\mathbf{x}_t] + \\mathbf{b}_i)\\] \\[\\tilde{\\mathbf{C}}_t = \\tanh(\\mathbf{W}_{C}[\\mathbf{h}_{t-1}, \\mathbf{x}_t] + \\mathbf{b}_C)\\]\nOutput Gate (\\(\\mathbf{o}_t\\)): Determines what parts of the cell state to output \\[\\mathbf{o}_t = \\sigma(\\mathbf{W}_{o}[\\mathbf{h}_{t-1}, \\mathbf{x}_t] + \\mathbf{b}_o)\\]\n\n\n\nCell State Update\nThe cell state update follows a linear path with controlled modifications:\n\\[\\mathbf{C}_t = \\mathbf{f}_t \\odot \\mathbf{C}_{t-1} + \\mathbf{i}_t \\odot \\tilde{\\mathbf{C}}_t\\]\nWhere \\(\\odot\\) represents element-wise multiplication.\n\n\nHidden State Calculation\nThe hidden state is derived from the cell state:\n\\[\\mathbf{h}_t = \\mathbf{o}_t \\odot \\tanh(\\mathbf{C}_t)\\]\n\n\nGradient Flow in LSTMs\nThe key innovation of LSTMs is that gradients can flow through the cell state with minimal attenuation:\n\\[\\frac{\\partial \\mathbf{C}_t}{\\partial \\mathbf{C}_{t-1}} = \\mathbf{f}_t\\]\nSince the forget gate values are typically close to 1 for relevant information, the gradient doesn’t vanish exponentially across time steps, allowing LSTMs to learn long-range dependencies effectively."
  },
  {
    "objectID": "lectures/lecture16.html#practical-implications",
    "href": "lectures/lecture16.html#practical-implications",
    "title": "Lecture 16: A Very Fast Lecture on Recurrent Neural Networks and LSTMs",
    "section": "Practical Implications",
    "text": "Practical Implications\nLSTMs have become the standard building block for sequence modeling tasks due to their:\n\nAbility to learn long-range dependencies\nRobustness to vanishing gradients\nFlexibility in controlling information flow\n\nWhile LSTMs trade some expressiveness for stability in training, the architecture has proven remarkably effective for a wide range of sequence tasks including language modeling, machine translation, and time series forecasting.\nThe next evolution in sequence modeling—transformer architectures with multi-headed self-attention—builds on these foundations while addressing additional limitations of recurrent models."
  },
  {
    "objectID": "lectures/lecture18.html",
    "href": "lectures/lecture18.html",
    "title": "Lecture 18: Transformers: RNNs are so last decade",
    "section": "",
    "text": "← Previous: Lecture 17\n\n\nNext: Lecture 19 →"
  },
  {
    "objectID": "lectures/lecture18.html#from-seq2seq-to-transformers",
    "href": "lectures/lecture18.html#from-seq2seq-to-transformers",
    "title": "Lecture 18: Transformers: RNNs are so last decade",
    "section": "From Seq2Seq to Transformers",
    "text": "From Seq2Seq to Transformers\nSequence-to-sequence models with attention represented a significant advancement in handling sequential data, but they still relied on recurrent structures that limited their efficiency and capacity to handle long-range dependencies.\n\nLimitations of Recurrent Seq2Seq Models\nRecurrent seq2seq models suffer from several fundamental constraints:\n\nSequential processing introduces computational bottlenecks\nInformation must flow through multiple time steps, leading to vanishing gradients\nTraining requires time-sequential operations that cannot be parallelized\nDeep networks (one layer per token) become increasingly difficult to train\n\nFor a sequence with hundreds of tokens, these limitations severely restrict model capacity and training efficiency, particularly when handling complex tasks like language translation or question answering."
  },
  {
    "objectID": "lectures/lecture18.html#self-attention-the-core-innovation",
    "href": "lectures/lecture18.html#self-attention-the-core-innovation",
    "title": "Lecture 18: Transformers: RNNs are so last decade",
    "section": "Self-Attention: The Core Innovation",
    "text": "Self-Attention: The Core Innovation\nSelf-attention represents the key breakthrough that enabled the transformer architecture, allowing models to directly connect any position in a sequence to any other position with constant computational complexity.\n\nSelf-Attention Mechanism\nThe self-attention operation computes weighted representations of the entire input sequence for each position:\n\nFor each input token, compute three vectors through learned linear projections:\n\nQuery vector (\\(\\mathbf{q}_i\\)): Represents what information the token is “looking for”\nKey vector (\\(\\mathbf{k}_i\\)): Represents what information the token “contains”\nValue vector (\\(\\mathbf{v}_i\\)): Represents the actual content of the token\n\nFor each position \\(i\\), calculate attention weights by comparing its query to all keys: \\[e_{ij} = \\frac{\\mathbf{q}_i^T\\mathbf{k}_j}{\\sqrt{d_k}}\\]\nNormalize weights using softmax: \\[a_{ij} = \\frac{\\exp(e_{ij})}{\\sum_{m=1}^n \\exp(e_{im})}\\]\nCompute a weighted sum of all value vectors: \\[\\mathbf{o}_i = \\sum_{j=1}^n a_{ij}\\mathbf{v}_j\\]\n\nThis operation allows each token to attend to all other tokens, capturing contextual relationships regardless of their distance in the sequence.\n\n\nMulti-Head Attention\nTo capture different types of relationships simultaneously, the transformer employs multi-head attention:\n\nApply multiple sets of query/key/value projections in parallel\nEach “head” can attend to different aspects of the input (e.g., syntactic vs. semantic relationships)\nConcatenate the outputs from all heads and project to the original dimension\n\nMulti-head attention enables the model to jointly attend to information from different representational subspaces, enhancing the model’s ability to capture complex patterns."
  },
  {
    "objectID": "lectures/lecture18.html#positional-encodings",
    "href": "lectures/lecture18.html#positional-encodings",
    "title": "Lecture 18: Transformers: RNNs are so last decade",
    "section": "Positional Encodings",
    "text": "Positional Encodings\nA critical insight in transformers is addressing the lack of inherent sequence ordering in self-attention. Unlike RNNs, self-attention has no built-in notion of token position.\n\nSinusoidal Position Embeddings\nThe transformer architecture incorporates position information through additive sinusoidal encodings:\n\\[PE_{(pos,2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)\\] \\[PE_{(pos,2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)\\]\nWhere:\n\n\\(pos\\) is the position in the sequence\n\\(i\\) is the dimension index\n\\(d_{model}\\) is the embedding dimension\n\nThese encodings have important properties:\n\nThey provide unique patterns for each position\nThey capture relative distances between positions\nThey allow the model to extrapolate to sequence lengths not seen in training\nThey don’t require additional parameters to be learned"
  },
  {
    "objectID": "lectures/lecture18.html#the-transformer-architecture",
    "href": "lectures/lecture18.html#the-transformer-architecture",
    "title": "Lecture 18: Transformers: RNNs are so last decade",
    "section": "The Transformer Architecture",
    "text": "The Transformer Architecture\nThe complete transformer architecture combines multiple components into an encoder-decoder structure that entirely replaces recurrence with attention.\n\nEncoder Block\nEach encoder block consists of:\n\nMulti-head self-attention layer\nPosition-wise feed-forward network (FFN)\nLayer normalization and residual connections\n\nThe encoder processes the entire input sequence in parallel, generating contextual representations for each token that incorporate information from the entire sequence.\n\n\nDecoder Block\nEach decoder block contains:\n\nMasked multi-head self-attention (prevents attending to future positions)\nMulti-head cross-attention over encoder outputs\nPosition-wise feed-forward network\nLayer normalization and residual connections\n\nThe masking in the self-attention layer ensures that predictions for position \\(i\\) can only depend on known outputs at positions less than \\(i\\), preserving the autoregressive property needed for generation.\n\n\nParallelization Advantage\nThe transformer architecture offers significant computational advantages:\n\nSelf-attention operations can be parallelized across all sequence positions\nMultiple attention heads can be computed simultaneously\nFor a sequence of length \\(n\\), complexity reduces from \\(O(n)\\) sequential operations to \\(O(1)\\)\nGPU acceleration becomes dramatically more effective\n\nThis parallelization enables training on substantially larger datasets and with more parameters than was previously feasible with recurrent architectures."
  },
  {
    "objectID": "lectures/lecture18.html#the-generative-pre-trained-transformer-gpt",
    "href": "lectures/lecture18.html#the-generative-pre-trained-transformer-gpt",
    "title": "Lecture 18: Transformers: RNNs are so last decade",
    "section": "The Generative Pre-trained Transformer (GPT)",
    "text": "The Generative Pre-trained Transformer (GPT)\nGPT represents a decoder-only variant of the transformer architecture, optimized for autoregressive language modeling.\n\nArchitecture and Training\nGPT consists of:\n\nToken and position embeddings\nMultiple layers of masked self-attention blocks\nA final linear layer with softmax activation\n\nThe model is trained to predict the next token given all previous tokens in the sequence, using a language modeling objective:\n\\[L(\\theta) = \\sum_i \\log P(x_i | x_{&lt;i}; \\theta)\\]\nDuring training, all tokens are processed in parallel with appropriate masking, while generation remains sequential.\n\n\nAutoregressive Generation\nAt inference time, GPT generates text one token at a time:\n\nStart with a prompt sequence\nFeed the sequence through the model to get a probability distribution over the next token\nSample from this distribution (using techniques like top-k or nucleus sampling)\nAppend the sampled token to the sequence\nRepeat until an end condition is met\n\nThis approach allows GPT to generate coherent, contextually relevant text that follows the statistical patterns observed in its training data.\n\n\nScaling Properties\nGPT models have demonstrated remarkable scaling properties:\n\nPerformance improves predictably with more parameters and training data\nLarger models exhibit emergent capabilities not present in smaller variants\nThe same architecture can be applied across diverse language tasks\n\nModern implementations like GPT-3 and GPT-4 have scaled to hundreds of billions of parameters, enabling unprecedented capabilities in language understanding and generation."
  },
  {
    "objectID": "lectures/lecture18.html#bidirectional-encoder-representations-from-transformers-bert",
    "href": "lectures/lecture18.html#bidirectional-encoder-representations-from-transformers-bert",
    "title": "Lecture 18: Transformers: RNNs are so last decade",
    "section": "Bidirectional Encoder Representations from Transformers (BERT)",
    "text": "Bidirectional Encoder Representations from Transformers (BERT)\nWhile GPT focuses on generative capabilities, BERT represents an encoder-only approach optimized for representational learning.\n\nBidirectional Context\nUnlike the unidirectional context of GPT, BERT:\n\nProcesses the entire input sequence simultaneously\nAttends to both left and right context for each token\nDevelops rich bidirectional representations\n\nThis bidirectional approach is particularly suited for tasks that require deep understanding of context, such as question answering and natural language inference."
  },
  {
    "objectID": "lectures/lecture18.html#the-transformer-revolution",
    "href": "lectures/lecture18.html#the-transformer-revolution",
    "title": "Lecture 18: Transformers: RNNs are so last decade",
    "section": "The Transformer Revolution",
    "text": "The Transformer Revolution\nThe introduction of transformer architectures marked a paradigm shift in machine learning for sequential data:\n\nEliminated the sequential bottleneck of recurrent models\nEnabled efficient training of much larger models\nEstablished a fundamental architecture that scales effectively with compute\nProvided a unified approach to diverse language and sequence modeling tasks\n\nThis architectural innovation directly enabled the development of today’s large language models and continues to drive advancements across machine learning domains from natural language processing to computer vision."
  },
  {
    "objectID": "lectures/lecture2.html",
    "href": "lectures/lecture2.html",
    "title": "Lecture 2: Machine Learning Basics",
    "section": "",
    "text": "← Previous: Lecture 1\n\n\nNext: Lecture 3 →"
  },
  {
    "objectID": "lectures/lecture2.html#what-is-machine-learning",
    "href": "lectures/lecture2.html#what-is-machine-learning",
    "title": "Lecture 2: Machine Learning Basics",
    "section": "1. What is Machine Learning?",
    "text": "1. What is Machine Learning?\nAt its heart, machine learning involves algorithms that learn from data. &gt; “A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.” (Mitchell, 1997)\nLet’s break down these components:\n\nTasks (T): The objectives the algorithm aims to achieve.\n\nPrediction: Forecasting future or unknown values (e.g., stock prices, disease likelihood).\nDescription: Uncovering patterns or structures in data (e.g., customer segmentation via clustering, topic modeling in texts). This often involves dimensionality reduction or density estimation.\nInference/Explanation: Understanding the causal relationships and the impact of variables on one another.\n\nLoss focus: Accuracy of parameter estimates, e.g., for regression coefficients \\(\\boldsymbol{\\beta}\\), a common loss is Mean Squared Error of the estimate: \\(E[(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})^2]\\).\n\n\nExperience (E): The data that fuels the learning process.\n\nSupervised Learning: Uses labeled data, where each data point has features and a known outcome (e.g., images labeled with “cat” or “dog”).\nUnsupervised Learning: Uses unlabeled data, focusing on finding inherent structure in the features themselves (e.g., grouping similar news articles).\nSemi-supervised Learning: A hybrid approach using a small amount of labeled data and a large amount of unlabeled data.\nReinforcement Learning: The algorithm learns by interacting with an environment and receiving feedback (rewards or penalties) for its actions (e.g., training a robot to navigate a maze).\n\nPerformance Measure (P): A metric to quantify how well the algorithm is performing its task. This is typically achieved through a loss function."
  },
  {
    "objectID": "lectures/lecture2.html#predictive-performance-loss-functions",
    "href": "lectures/lecture2.html#predictive-performance-loss-functions",
    "title": "Lecture 2: Machine Learning Basics",
    "section": "2. Predictive Performance & Loss Functions",
    "text": "2. Predictive Performance & Loss Functions\nIn predictive modeling, we often assume an underlying true relationship: \\(y = f(\\mathbf{x}) + \\epsilon\\) where:\n\n\\(y\\) is the outcome variable.\n\\(f(\\mathbf{x})\\) represents the systematic information (the “signal”) that features \\(\\mathbf{x}\\) provide about \\(y\\).\n\\(\\epsilon\\) is irreducible random error or “noise.”\n\nThe goal is to learn an approximation, \\(\\hat{f}(\\mathbf{x})\\), that accurately captures \\(f(\\mathbf{x})\\) from the training data.\nCommon Loss Functions:\n\nFor Continuous Outcomes:\n\nMean Squared Error (MSE): Measures the average squared difference between predicted and actual values. \\[\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{f}(\\mathbf{x}_i))^2\\]\n\nFor Discrete/Categorical Outcomes:\n\n0/1 Loss: Assigns a loss of 1 for an incorrect classification and 0 for a correct one. \\[\\text{0/1 Loss} = \\frac{1}{N} \\sum_{i=1}^{N} I(\\hat{f}(\\mathbf{x}_i) \\neq y_i)\\] where \\(I(\\cdot)\\) is the indicator function. In probability terms for a single instance: \\(1 - P(\\hat{f}(\\mathbf{x}) = y)\\).\nCross-Entropy Loss (or Log Loss): Penalizes confident but incorrect predictions more heavily. For a multi-class problem with K classes: \\[\\text{Cross-Entropy} = - \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{k=1}^{K} I(y_i = k) \\log P(\\hat{f}(\\mathbf{x}_i) = k)\\]"
  },
  {
    "objectID": "lectures/lecture2.html#generalization-error-the-true-test-of-a-model",
    "href": "lectures/lecture2.html#generalization-error-the-true-test-of-a-model",
    "title": "Lecture 2: Machine Learning Basics",
    "section": "3. Generalization Error: The True Test of a Model",
    "text": "3. Generalization Error: The True Test of a Model\nWhile performance on training data is informative, the ultimate goal is a model that performs well on unseen data. This is captured by generalization error.\n3.1. Defining Generalization Error\nAssuming our data samples \\(\\lbrace\\mathbf{x}_i, y_i\\rbrace\\) are independent and identically distributed (i.i.d.) draws from a true, underlying data-generating distribution \\(\\mathcal{T}\\), the generalization error is the expected loss of our model \\(\\hat{f}(\\mathbf{x})\\) over this entire distribution: \\[\\text{Generalization Error} = E_{\\mathcal{T}}[\\mathcal{L}(\\hat{f}(\\mathbf{x}), y)]\\]\n3.2. Training Error vs. Generalization Error\nIn practice, we can only directly compute the training error using our observed training dataset \\(\\mathcal{D} = \\{(\\mathbf{x}_i, y_i)\\}_{i=1}^N\\): \\[\\text{Training Error} = E_{\\mathcal{D}}[\\mathcal{L}(\\mathbf{y}, \\hat{\\mathbf{y}})] = \\frac{1}{N} \\sum_{i=1}^{N} \\mathcal{L}(y_i, \\hat{f}(\\mathbf{x}_i))\\]\n3.3. The Relationship: The Role of Covariance\nA key insight connects these two error measures. Under certain assumptions (e.g., fixed features \\(\\mathbf{X}\\), and outcomes \\(y\\) and \\(y'\\) differing only by noise for training and test sets respectively), for Mean Squared Error, the relationship is: \\[\\text{Expected Test Error} \\approx \\text{Training Error} + \\frac{2}{N} \\sum_{i=1}^{N} \\text{Cov}(y_i, \\hat{y}_i)\\] Here, \\(\\text{Cov}(y_i, \\hat{y}_i)\\) is the covariance between the true training outcomes and the model’s predictions at those training points.\n\nInterpretation: The gap between generalization (test) error and training error increases with the average covariance between the training outcomes and their predictions. A high covariance suggests the model is fitting the noise in the training data (overfitting).\n\n3.4. Implications for Linear Models\nFor linear models of the form \\(\\hat{\\mathbf{y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\), where \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\\), the sum of covariances can be further analyzed.\n\nThe term \\(\\sum_{i=1}^{N} \\text{Cov}(y_i, \\hat{y}_i)\\) simplifies to \\(\\sigma^2 \\text{tr}(\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T)\\).\nThe trace of the “hat matrix” \\(\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\) is equal to \\(P\\), the number of parameters (features) in the model.\nThus, the generalization error offset becomes: \\[\\frac{2P\\sigma^2}{N}\\]\n\n3.5. Key Factors Influencing Generalization Error:\nThis leads to a crucial understanding of what drives generalization:\n\nTraining Error: The baseline error on the data used to build the model.\nModel Complexity (\\(P\\)): More complex models (larger \\(P\\)) tend to increase the covariance term, widening the gap between training and generalization error.\nInherent Noise Variance (\\(\\sigma^2\\)): Higher irreducible error in the data-generating process naturally leads to a larger error gap.\nSample Size (\\(N\\)): Increasing the training sample size \\(N\\) helps to reduce the impact of the covariance term, thereby shrinking the gap and improving generalization.\n\nConclusion:\nThe lecture demonstrates through examples (decision trees, polynomial regression) that finding the right model involves balancing its flexibility against the amount of training data available. An overly complex model might achieve low training error but generalize poorly (overfitting), while an overly simple model might not capture the underlying patterns even in the training data (underfitting). The optimal model complexity often scales with the size of the training dataset."
  },
  {
    "objectID": "lectures/lecture21.html",
    "href": "lectures/lecture21.html",
    "title": "Lecture 21: Deterministic Autoencoders (and why they fail for image generation); Intro to Bayesian Machine Learning",
    "section": "",
    "text": "← Previous: Lecture 20\n\n\nNext: Lecture 22 →"
  },
  {
    "objectID": "lectures/lecture21.html#generative-models",
    "href": "lectures/lecture21.html#generative-models",
    "title": "Lecture 21: Deterministic Autoencoders (and why they fail for image generation); Intro to Bayesian Machine Learning",
    "section": "Generative Models",
    "text": "Generative Models\n\nGoals of Generation\n\nDensity estimation: Determine the probability \\(P(\\mathbf{x})\\) of observing data points\nSampling: Generate novel data from the model distribution\nRepresentation: Learn meaningful feature representations and manipulate specific features\n\n\n\nAutoregressive Generation\n\nBased on the probability chain rule: \\(f(x_1,x_2,x_3,...,x_P) = f(x_1)f(x_2 | x_1)f(x_3 | x_1,x_2)...f(x_P | x_{P-1}, x_{P-2},...)\\)\nImplementation approaches:\n\nPixelCNN: Uses CNN-style windowing instead of recurrence\nImageGPT: Uses masked self-attention instead of recurrence\n\nLimitations:\n\nExtremely slow for high-dimensional data like images\nLimited practical resolution (e.g., 64×64 for ImageGPT)\nLacks effective feature representation"
  },
  {
    "objectID": "lectures/lecture21.html#dimensionality-reduction-and-autoencoders",
    "href": "lectures/lecture21.html#dimensionality-reduction-and-autoencoders",
    "title": "Lecture 21: Deterministic Autoencoders (and why they fail for image generation); Intro to Bayesian Machine Learning",
    "section": "Dimensionality Reduction and Autoencoders",
    "text": "Dimensionality Reduction and Autoencoders\n\nPrincipal Component Analysis (PCA)\n\nFinds a low-dimensional representation that minimizes reconstruction error: \\(\\frac{1}{N} \\sum \\limits_{i = 1}^N \\| \\mathbf{x}_i - d(e(\\mathbf{x}_i))\\|^2_2\\)\nFunctions:\n\n\\(e(\\mathbf{x}_i) = \\mathbf{z}_i\\) is the encoder mapping input to latent space\n\\(d(\\mathbf{z}_i)\\) is the decoder mapping latent variables back to original feature space\n\nFormulation with weight matrix \\(\\mathbf{W}\\): \\(\\frac{1}{N} \\sum \\limits_{i = 1}^N \\| \\mathbf{x}_i - \\mathbf{W} \\mathbf{W}^T \\mathbf{x}_i\\|^2_2\\)\nConstraints:\n\nColumns of \\(\\mathbf{W}\\) must be orthogonal\nColumns of \\(\\mathbf{W}\\) must be normalized\n\nImplementation using eigendecomposition:\n\n\\(\\mathbf{X}^T \\mathbf{X} = \\mathbf{Q} \\mathbf{D} \\mathbf{Q}^{-1}\\)\n\\(\\mathbf{W} = \\mathbf{Q}_{1:K}\\) (first \\(K\\) eigenvectors)\n\n\n\n\nDeterministic Autoencoders\n\nGeneralization of PCA using neural networks\nArchitecture:\n\nEncoder network maps inputs to lower-dimensional latent space\nBottleneck forces the model to learn efficient representations\nDecoder network reconstructs original input from latent representation\n\nAdvantages over PCA:\n\nCan capture non-linear relationships\nCNN backbones help induce structure in recovered images\nPerformance strictly dominates PCA for image data\n\nApplications:\n\nDimensionality reduction\nFeature extraction for downstream tasks\nExamination of feature maps to understand data variation\n\nLimitations as generative models:\n\nNo probabilistic formulation (no \\(P(\\mathbf{X})\\))\nCannot properly sample from gaps between training examples\nNo density estimation capability"
  },
  {
    "objectID": "lectures/lecture21.html#generative-probabilistic-models",
    "href": "lectures/lecture21.html#generative-probabilistic-models",
    "title": "Lecture 21: Deterministic Autoencoders (and why they fail for image generation); Intro to Bayesian Machine Learning",
    "section": "Generative Probabilistic Models",
    "text": "Generative Probabilistic Models\n\nFactor Analysis\n\nProbabilistic version of PCA with a defined prior distribution\nGaussian factor analysis:\n\nPrior: \\(f(\\mathbf{z}) = \\mathcal{N}_K(\\mathbf{z} | \\mathbf{0}, \\mathcal{I}_K)\\)\nLikelihood: \\(f(\\mathbf{x} | \\mathbf{z}, \\boldsymbol{\\Theta}) = \\mathcal{N}_P(\\mathbf{x} | \\mathbf{W} \\mathbf{z} + \\boldsymbol{\\alpha}, \\boldsymbol{\\Psi})\\)\n\n\n\n\nProperties of Generative Models\n\nMust provide a probability density \\(P(\\mathbf{x}|\\boldsymbol{\\Theta})\\) for any viable input \\(\\mathbf{x}\\)\nComparison with discriminative models:\n\nDiscriminative models (e.g., logistic regression): \\(P(y=1|\\mathbf{x},\\hat{\\boldsymbol{\\beta}}) = \\sigma(\\mathbf{x}^T \\hat{\\boldsymbol{\\beta}})\\)\nGenerative models (e.g., QDA): \\(\\mathbf{x}|y=c \\sim \\mathcal{N}_P(\\mathbf{x}|\\boldsymbol{\\mu}_c, \\boldsymbol{\\Sigma}_c)\\)\n\nTrade-off: Generative power comes at the cost of making assumptions about data structure"
  },
  {
    "objectID": "lectures/lecture21.html#bayesian-methods",
    "href": "lectures/lecture21.html#bayesian-methods",
    "title": "Lecture 21: Deterministic Autoencoders (and why they fail for image generation); Intro to Bayesian Machine Learning",
    "section": "Bayesian Methods",
    "text": "Bayesian Methods\n\nBayesian Framework\n\nUpdate prior beliefs based on observed data using Bayes’ theorem: \\(f(\\boldsymbol{\\theta} | \\mathcal{D}) = \\frac{f(\\mathcal{D} | \\boldsymbol{\\theta})f(\\boldsymbol{\\theta})}{f(\\mathcal{D})}\\)\nComponents:\n\nLikelihood: \\(f(\\mathcal{D} | \\boldsymbol{\\theta})\\) - probability of observing data given parameters\nPrior: \\(f(\\boldsymbol{\\theta})\\) - beliefs about parameters before observing data\nMarginal likelihood: \\(f(\\mathcal{D}) = \\int f(\\mathcal{D} | \\boldsymbol{\\theta})f(\\boldsymbol{\\theta}) d\\theta\\)\nPosterior: \\(f(\\boldsymbol{\\theta} | \\mathcal{D})\\) - updated beliefs after observing data\n\n\n\n\nNormal Distribution with Known Variance\n\nLikelihood: \\(f(\\mathbf{y} | \\mu, \\sigma^2) = \\prod_{i=1}^N \\mathcal{N}(y_i | \\mu, \\sigma^2)\\)\nPrior: \\(f(\\mu) \\sim \\mathcal{N}(\\mu | \\mu_0, \\sigma^2_0)\\)\nPosterior: \\(f(\\mu | \\mathbf{y}) \\sim \\mathcal{N}(\\mu | \\hat{\\mu}, \\hat{\\sigma}^2)\\)\n\n\\(\\hat{\\sigma}^2 = \\left(\\frac{N}{\\sigma^2} + \\frac{1}{\\sigma^2_0}\\right)^{-1}\\)\n\\(\\hat{\\mu} = \\hat{\\sigma}^2\\left(\\frac{1}{\\sigma^2}\\sum y_i + \\frac{1}{\\sigma^2_0}\\mu_0\\right)\\)\n\nProperties:\n\nWith diffuse prior (\\(\\sigma^2_0 \\to \\infty\\)), posterior mean converges to sample mean\nAs \\(N \\to \\infty\\), prior influence diminishes and posterior approaches MLE\n\n\n\n\nBayesian Linear Regression\n\nLikelihood: \\(f(\\mathbf{y} | \\mathbf{X}, \\boldsymbol{\\beta}, \\sigma^2) \\sim \\mathcal{N}_N(\\mathbf{y} | \\mathbf{X}\\boldsymbol{\\beta}, \\sigma^2\\mathcal{I}_N)\\)\nPrior: \\(f(\\boldsymbol{\\beta}) \\sim \\mathcal{N}_P(\\boldsymbol{\\beta} | \\boldsymbol{\\mu}_0, \\boldsymbol{\\Sigma}_0)\\)\nPosterior: \\(f(\\boldsymbol{\\beta} | \\mathbf{X}, \\mathbf{y}, \\sigma^2) \\sim \\mathcal{N}_P(\\boldsymbol{\\beta} | \\hat{\\boldsymbol{\\mu}}, \\hat{\\boldsymbol{\\Sigma}})\\)\n\n\\(\\hat{\\boldsymbol{\\Sigma}} = \\left(\\frac{1}{\\sigma^2}\\mathbf{X}^T\\mathbf{X} + \\boldsymbol{\\Sigma}_0^{-1}\\right)^{-1}\\)\n\\(\\hat{\\boldsymbol{\\mu}} = \\hat{\\boldsymbol{\\Sigma}}\\left(\\frac{1}{\\sigma^2}\\mathbf{X}^T\\mathbf{y} + \\boldsymbol{\\Sigma}^{-1}_0\\boldsymbol{\\mu}_0\\right)\\)\n\nConnections to regularization:\n\nWith \\(\\boldsymbol{\\mu}_0 = \\mathbf{0}\\) and \\(\\boldsymbol{\\Sigma}_0 = \\tau^2\\mathcal{I}_P\\): \\(\\hat{\\boldsymbol{\\mu}} = (\\mathbf{X}^T\\mathbf{X} + \\lambda\\mathcal{I}_P)^{-1}\\mathbf{X}^T\\mathbf{y}\\)\nThis is equivalent to ridge regression with \\(\\lambda = \\frac{\\sigma^2}{\\tau^2}\\)\nL2 regularization (weight decay) can be interpreted as a Bayesian prior favoring simpler models"
  },
  {
    "objectID": "lectures/lecture23.html",
    "href": "lectures/lecture23.html",
    "title": "Lecture 23: Variational Autoencoders",
    "section": "",
    "text": "← Previous: Lecture 22\n\n\nNext: Lecture 24 →"
  },
  {
    "objectID": "lectures/lecture23.html#generative-models",
    "href": "lectures/lecture23.html#generative-models",
    "title": "Lecture 23: Variational Autoencoders",
    "section": "Generative Models",
    "text": "Generative Models\n\nGoals of Generative Modeling\n\nDensity estimation: Determine probability \\(P(\\mathbf{x})\\) of observing data points\nSampling: Generate novel data from the model distribution\nRepresentation learning: Extract meaningful feature representations and manipulate specific features\n\n\n\nLimitations of Deterministic Models\n\nPCA and deterministic autoencoders can learn mappings between input space and latent space\nBoth fail to compute \\(P(\\mathbf{x}|\\boldsymbol{\\Theta}) = \\int P(\\mathbf{x}|\\boldsymbol{\\Theta}, \\mathbf{z})P(\\mathbf{z})d\\mathbf{z}\\)\nCannot properly generate new samples from latent space"
  },
  {
    "objectID": "lectures/lecture23.html#factor-analysis",
    "href": "lectures/lecture23.html#factor-analysis",
    "title": "Lecture 23: Variational Autoencoders",
    "section": "Factor Analysis",
    "text": "Factor Analysis\n\nProbabilistic Approach\n\nAssumes each input follows a distribution: \\(P(\\mathbf{x}|\\mathbf{z}) = \\mathcal{P}(\\mathbf{x}|f(\\mathbf{z},\\boldsymbol{\\Theta}))\\)\n\\(f(\\mathbf{z})\\) maps latent variables to original feature space\n\\(\\boldsymbol{\\Theta}\\) contains parameters that define the mapping\n\n\n\nOptimization Challenge\n\nGoal: maximize likelihood \\(\\hat{\\boldsymbol{\\Theta}} = \\underset{\\boldsymbol{\\Theta}}{\\text{argmax}} \\prod_{i=1}^N P(\\mathbf{x}_i|\\mathbf{z}_i,\\boldsymbol{\\Theta})\\)\nSince \\(\\mathbf{z}\\) is unknown, must integrate: \\(\\hat{\\boldsymbol{\\Theta}} = \\underset{\\boldsymbol{\\Theta}}{\\text{argmax}} \\prod_{i=1}^N \\int P(\\mathbf{x}_i|\\mathbf{z},\\boldsymbol{\\Theta})P(\\mathbf{z}_i)d\\mathbf{z}\\)\nLog-likelihood involves intractable log-integral: \\(\\hat{\\boldsymbol{\\Theta}} = \\underset{\\boldsymbol{\\Theta}}{\\text{argmax}} \\sum_{i=1}^N \\log \\int P(\\mathbf{x}_i|\\mathbf{z},\\boldsymbol{\\Theta})P(\\mathbf{z}_i)d\\mathbf{z}\\)"
  },
  {
    "objectID": "lectures/lecture23.html#variational-inference",
    "href": "lectures/lecture23.html#variational-inference",
    "title": "Lecture 23: Variational Autoencoders",
    "section": "Variational Inference",
    "text": "Variational Inference\n\nBayesian Framework\n\nRearranging Bayes’ rule: \\(P(\\mathbf{x}_i|\\boldsymbol{\\Theta}) = \\frac{P(\\mathbf{x}_i|\\mathbf{z}_i,\\boldsymbol{\\Theta})P(\\mathbf{z}_i)}{P(\\mathbf{z}_i|\\mathbf{x}_i,\\boldsymbol{\\Theta})}\\)\nKnown components:\n\nDecoder (likelihood): \\(P(\\mathbf{x}_i|\\mathbf{z}_i,\\boldsymbol{\\Theta})\\)\nPrior: \\(P(\\mathbf{z}_i)\\)\n\nUnknown components:\n\nMarginal likelihood: \\(P(\\mathbf{x}_i|\\boldsymbol{\\Theta})\\)\nEncoder (posterior): \\(P(\\mathbf{z}_i|\\mathbf{x}_i,\\boldsymbol{\\Theta})\\)\n\n\n\n\nApproximation Strategy\n\nIntroduce approximate posterior: \\(Q(\\mathbf{z}_i|\\mathbf{x}_i,\\boldsymbol{\\Phi}) \\approx P(\\mathbf{z}_i|\\mathbf{x}_i,\\boldsymbol{\\Theta})\\)\nRearrange log-likelihood: \\(\\log P(\\mathbf{x}_i|\\boldsymbol{\\Theta}) = \\log P(\\mathbf{x}_i|\\mathbf{z}_i,\\boldsymbol{\\Theta}) - \\log \\frac{Q(\\mathbf{z}_i|\\mathbf{x}_i,\\boldsymbol{\\Phi})}{P(\\mathbf{z}_i)} + \\log \\frac{Q(\\mathbf{z}_i|\\mathbf{x}_i,\\boldsymbol{\\Phi})}{P(\\mathbf{z}_i|\\mathbf{x}_i,\\boldsymbol{\\Theta})}\\)\nTaking expectation with respect to \\(Q\\): \\(E_Q[\\log P(\\mathbf{x}_i|\\boldsymbol{\\Theta})] = E_Q[\\log P(\\mathbf{x}_i|\\mathbf{z}_i,\\boldsymbol{\\Theta})] - E_Q[\\log \\frac{Q(\\mathbf{z}_i|\\mathbf{x}_i,\\boldsymbol{\\Phi})}{P(\\mathbf{z}_i)}] + E_Q[\\log \\frac{Q(\\mathbf{z}_i|\\mathbf{x}_i,\\boldsymbol{\\Phi})}{P(\\mathbf{z}_i|\\mathbf{x}_i,\\boldsymbol{\\Theta})}]\\)\n\n\n\nEvidence Lower Bound (ELBO)\n\nRecognize KL divergence terms: \\(D_{KL}(Q(\\mathbf{z}_i|\\mathbf{x}_i) || P(\\mathbf{z}_i)) = \\int Q(\\mathbf{z}_i|\\mathbf{x}_i,\\boldsymbol{\\Phi}) \\log \\frac{Q(\\mathbf{z}_i|\\mathbf{x}_i,\\boldsymbol{\\Phi})}{P(\\mathbf{z}_i)} d\\mathbf{z}_i\\)\nSince \\(D_{KL}(Q(\\mathbf{z}_i|\\mathbf{x}_i) || P(\\mathbf{z}_i|\\mathbf{x}_i)) \\geq 0\\): \\(\\log P(\\mathbf{x}_i|\\boldsymbol{\\Theta}) \\geq E_Q[\\log P(\\mathbf{x}_i|\\mathbf{z}_i,\\boldsymbol{\\Theta})] - D_{KL}(Q(\\mathbf{z}_i|\\mathbf{x}_i) || P(\\mathbf{z}_i))\\)\nELBO interpretations:\n\n\\(E_Q[\\log P(\\mathbf{x}_i|\\mathbf{z}_i,\\boldsymbol{\\Theta})]\\): Expected reconstruction error\n\\(D_{KL}(Q(\\mathbf{z}_i|\\mathbf{x}_i) || P(\\mathbf{z}_i))\\): Regularization forcing approximate posterior to stay close to prior"
  },
  {
    "objectID": "lectures/lecture23.html#variational-autoencoders-vaes",
    "href": "lectures/lecture23.html#variational-autoencoders-vaes",
    "title": "Lecture 23: Variational Autoencoders",
    "section": "Variational Autoencoders (VAEs)",
    "text": "Variational Autoencoders (VAEs)\n\nModel Architecture\n\nPrior on latent variables: \\(P(\\mathbf{z}) \\sim \\mathcal{N}_K(\\mathbf{0},\\mathcal{I}_K)\\)\nDecoder (likelihood): \\(P(\\mathbf{x}|\\mathbf{z}) \\sim \\mathcal{N}_P(f(\\mathbf{z}),\\boldsymbol{\\Sigma}_x)\\)\nEncoder (approximate posterior): \\(Q(\\mathbf{z}|\\mathbf{x}) \\sim \\mathcal{N}_K(g(\\mathbf{x}),\\boldsymbol{\\Sigma}_z)\\)\nFunctions \\(f(\\mathbf{z})\\) and \\(g(\\mathbf{x})\\) implemented using neural networks\n\n\n\nImplementation Details\n\nEncoder network maps input to parameters of latent distribution (mean and variance)\nDecoder network maps latent samples to parameters of output distribution\nTraining objective: Maximize ELBO (minimize negative ELBO) \\(-E_Q[\\log P(\\mathbf{x}|\\mathbf{z})] + D_{KL}(Q(\\mathbf{z}|\\mathbf{x}) || P(\\mathbf{z}))\\)\nFor standard normal prior and diagonal normal proposal: \\(D_{KL}(Q(\\mathbf{z}_i|\\mathbf{x}_i) || P(\\mathbf{z}_i)) = \\frac{1}{2} \\sum_{k=1}^K [\\sigma^2_k + \\mu^2_k - 1 - \\log(\\sigma^2_k)]\\)\n\n\n\nPractical Considerations\n\nOften restrict covariance matrices to be diagonal for computational efficiency\nUse multivariate normal distributions for tractable KL divergence computation\nAmortized inference: Rather than learning separate posteriors for each data point, learn functions mapping inputs to distribution parameters\n\n\n\nAdvantages\n\nCreates probabilistic mappings between input and latent space\nFills gaps between training examples in a principled way\nEnables sampling new coherent images from the prior\nSupports image editing through latent space manipulation"
  },
  {
    "objectID": "lectures/lecture25.html",
    "href": "lectures/lecture25.html",
    "title": "Lecture 25: Generative Adversarial Networks",
    "section": "",
    "text": "← Previous: Lecture 24\n\n\nNext: Lecture 26 →"
  },
  {
    "objectID": "lectures/lecture25.html#gans-overview",
    "href": "lectures/lecture25.html#gans-overview",
    "title": "Lecture 25: Generative Adversarial Networks",
    "section": "GANs Overview",
    "text": "GANs Overview\n\nMotivation and Philosophy\n\nGoal: Learn \\(P(\\mathbf{x})\\) without explicitly modeling the probability distribution\nGives up on explicit density estimation but maintains ability to generate high-quality samples\nAddresses limitations of previous approaches:\n\nAutoregressive models: Slow training and generation, no explicit latent code\nVAEs: Lower bound optimization, often produces blurry images due to averaging\n\n\n\n\nCore Approach\n\nIntroduce latent variable \\(\\mathbf{z}\\) with simple prior \\(P(\\mathbf{z})\\)\nGenerator network: \\(\\hat{\\mathbf{x}} = g(\\mathbf{z})\\) maps from latent space to data space\nLearn transformation from easy-to-sample distribution to complex data distribution"
  },
  {
    "objectID": "lectures/lecture25.html#theoretical-foundation",
    "href": "lectures/lecture25.html#theoretical-foundation",
    "title": "Lecture 25: Generative Adversarial Networks",
    "section": "Theoretical Foundation",
    "text": "Theoretical Foundation\n\nDensity Ratio Perspective\n\nCompare two distributions: true data distribution \\(P(\\mathbf{x})\\) and model distribution \\(Q(\\mathbf{x})\\)\nDistributions are identical when: \\(\\frac{P(\\mathbf{x})}{Q(\\mathbf{x})} = 1\\) for all \\(\\mathbf{x}\\)\nBinary classification setup:\n\n\\(y_i = 1\\) for samples from \\(P(\\mathbf{x})\\) (real data)\n\\(y_i = 0\\) for samples from \\(Q(\\mathbf{x})\\) (fake data)\n\n\n\n\nDensity Ratio via Classification\n\nDensity ratio expressed as: \\(\\frac{P(\\mathbf{x})}{Q(\\mathbf{x})} = \\frac{p(\\mathbf{x}|y=1)}{p(\\mathbf{x}|y=0)}\\)\nUsing Bayes’ rule: \\(\\frac{P(\\mathbf{x})}{Q(\\mathbf{x})} = \\frac{p(y=1|\\mathbf{x})}{p(y=0|\\mathbf{x})} \\frac{p(y=0)}{p(y=1)}\\)\nFor equal sampling: \\(\\frac{P(\\mathbf{x})}{Q(\\mathbf{x})} = \\frac{p(y=1|\\mathbf{x})}{1-p(y=1|\\mathbf{x})}\\)\n\n\n\nOptimal Discriminator\n\nBinary classification objective: \\(\\boldsymbol{\\Phi} = \\underset{\\phi}{\\text{argmax}} E[y \\log D_{\\phi}(\\mathbf{x}) + (1-y) \\log(1-D_{\\phi}(\\mathbf{x}))]\\)\nOptimal discriminator: \\(D_{\\Phi}(\\mathbf{x}) = \\frac{P(\\mathbf{x})}{P(\\mathbf{x}) + Q(\\mathbf{x})}\\)\nMaximum value function: \\(V(P,Q) = \\underset{\\phi}{\\text{max}} E[y \\log D_{\\phi}(\\mathbf{x}) + (1-y) \\log(1-D_{\\phi}(\\mathbf{x}))]\\)\n\n\n\nJensen-Shannon Divergence\n\nOptimal classifier performance relates to Jensen-Shannon divergence: \\(V(P,Q) = \\frac{1}{2} D_{KL}\\left(P(\\mathbf{x}) || \\frac{1}{2}(Q(\\mathbf{x}) + P(\\mathbf{x}))\\right) + \\frac{1}{2} D_{KL}\\left(Q(\\mathbf{x}) || \\frac{1}{2}(Q(\\mathbf{x}) + P(\\mathbf{x}))\\right) - \\log 2\\)\nJensen-Shannon divergence is symmetric (unlike KL divergence)\nMeasures distance between distributions via common “middle point”"
  },
  {
    "objectID": "lectures/lecture25.html#gan-architecture-and-training",
    "href": "lectures/lecture25.html#gan-architecture-and-training",
    "title": "Lecture 25: Generative Adversarial Networks",
    "section": "GAN Architecture and Training",
    "text": "GAN Architecture and Training\n\nMinimax Game Formulation\n\nGenerator objective: Minimize Jensen-Shannon divergence between \\(P(\\mathbf{x})\\) and \\(Q(\\mathbf{x})\\)\nDiscriminator objective: Maximize classification accuracy between real and fake samples\nCombined objective: \\(\\underset{\\theta}{\\text{min}} \\underset{\\phi}{\\text{max}} \\frac{1}{2} E_{P(\\mathbf{x})}[\\log D_{\\phi}(\\mathbf{x})] + E_{Q(\\mathbf{z})}[\\log(1 - D_{\\phi}(g_{\\theta}(\\mathbf{z})))]\\)\n\n\n\nTraining Strategy\n\nDiscriminator update: Minimize detection loss (maximize ability to distinguish real from fake)\n\nCompute discriminator loss on real and fake images\nUpdate to minimize classification error\n\nGenerator update: Fool discriminator\n\nGenerate fake instances\nCompute discriminator loss treating all instances as real\nUpdate to minimize this loss\n\n\n\n\nTraining Challenges\n\nMode collapse: Generator converges to single mode even with multimodal true distribution\nMode hopping: Generator jumps between modes without convergence\nOptimization difficulties: No traditional loss function to optimize toward\nNash equilibrium: Theoretical convergence in zero-sum game setting"
  },
  {
    "objectID": "lectures/lecture25.html#practical-considerations",
    "href": "lectures/lecture25.html#practical-considerations",
    "title": "Lecture 25: Generative Adversarial Networks",
    "section": "Practical Considerations",
    "text": "Practical Considerations\n\nTraining Improvements\n\nLearning rate strategies: Careful tuning to prevent mode collapse and hopping\nGradient penalties: Clip gradients or add penalties to prevent large moves\nIterative training: Train discriminator multiple times before generator update\nSGD over ADAM: Sometimes momentum-based methods struggle without clear loss target\n\n\n\nWasserstein GAN Improvements\n\nLinear discriminator outputs: Predict “realness” scores \\(D(\\mathbf{x}) \\in \\mathbb{R}\\) instead of probabilities\nModified losses:\n\nDiscriminator: \\(D(\\mathbf{x}) - D(g(\\mathbf{z}))\\) (maximize realness difference)\nGenerator: \\(D(g(\\mathbf{z}))\\) (maximize realness of fakes)"
  },
  {
    "objectID": "lectures/lecture25.html#extensions-and-applications",
    "href": "lectures/lecture25.html#extensions-and-applications",
    "title": "Lecture 25: Generative Adversarial Networks",
    "section": "Extensions and Applications",
    "text": "Extensions and Applications\n\nConditional GANs\n\nGoal: Control generation with additional information (class labels)\nArchitecture modifications:\n\nGenerator: \\(g(\\mathbf{z}, \\mathbf{c})\\) where \\(\\mathbf{c}\\) is conditioning information\nDiscriminator: \\(D(\\mathbf{x}, \\mathbf{c})\\) considers both image and condition\n\n\n\n\nConditional Batch Normalization\n\nStandard batch normalization: \\(h_{ij}^* = \\gamma_j g_{ij} + \\beta_j\\)\nConditional version: \\(h_{ij}^* = \\gamma^{(y)}_j g_{ij} + \\beta^{(y)}_j\\)\nLearn separate normalization parameters for each class\nForces both networks to perform well across all classes\nApproximates \\(P(\\mathbf{x}|y)\\) instead of \\(P(\\mathbf{x})\\)\n\n\n\nStyleGAN\n\nApproach: Split latent space into initial space and style space\nFeature control: Learn explicit mappings from features to style vectors\nApplications: Particularly effective for generating realistic human faces"
  },
  {
    "objectID": "lectures/lecture25.html#advantages-and-limitations",
    "href": "lectures/lecture25.html#advantages-and-limitations",
    "title": "Lecture 25: Generative Adversarial Networks",
    "section": "Advantages and Limitations",
    "text": "Advantages and Limitations\n\nAdvantages\n\nHigh-quality generation: Can produce very realistic samples\nFast sampling: No iterative generation process required\nAssumption-free: Minimal distributional assumptions\nMathematically sound: Grounded in game theory and divergence minimization\n\n\n\nLimitations\n\nNo density estimation: Cannot compute \\(P(\\mathbf{x})\\) directly\nNo encoding capability: Difficult to map from data space back to latent space\nTraining instability: Sensitive to hyperparameters and training procedures\nLimited control: Basic GANs provide limited control over generated content"
  },
  {
    "objectID": "lectures/lecture27.html",
    "href": "lectures/lecture27.html",
    "title": "Lecture 27: Interpretable Deep Learning: Post-Hoc Methods and Glass-Box Models",
    "section": "",
    "text": "← Previous: Lecture 26\n\n\nNext: Lecture 28 →"
  },
  {
    "objectID": "lectures/lecture27.html#motivation-for-interpretability",
    "href": "lectures/lecture27.html#motivation-for-interpretability",
    "title": "Lecture 27: Interpretable Deep Learning: Post-Hoc Methods and Glass-Box Models",
    "section": "Motivation for Interpretability",
    "text": "Motivation for Interpretability\n\nThe Importance of Interpretability\n\nModel performance \\(\\neq\\) success: Classifiers can perform well for incorrect reasons\nSafety: Detect and prevent critical failures (e.g., autonomous driving misclassifications)\nFairness: Reveal and mitigate biases across different groups\nPrivacy/Security: Ensure protection of sensitive information\nLegal requirements: Compliance with regulations like GDPR’s “right to explanation”\n\n\n\nThe Interpretability Challenge\n\nNeural networks are complicated “black-box” algorithms: \\(\\hat{y} \\propto \\mathbf{W} \\varphi(\\mathbf{W} \\varphi (...))\\)\nDefinition (Biran and Cotton, 2017): “Interpretability is the degree to which a human can understand the cause of a decision”\nAlternative definition (Kim et al., 2016): “… a user can correctly and efficiently predict the method’s results”"
  },
  {
    "objectID": "lectures/lecture27.html#glass-box-models",
    "href": "lectures/lecture27.html#glass-box-models",
    "title": "Lecture 27: Interpretable Deep Learning: Post-Hoc Methods and Glass-Box Models",
    "section": "Glass-Box Models",
    "text": "Glass-Box Models\n\nInherently Interpretable Models\n\nLinear models: \\(\\hat{y} = g(\\mathbf{X} \\hat{\\boldsymbol{\\beta}})\\) - Changes in inputs directly relate to changes in predictions\nSimple decision trees: Transparent decision paths\nRidge/LASSO: Regularized linear models with variable selection\nGeneralized Additive Models: Flexible but transparent functional forms\n\n\n\nTransparency-Performance Trade-off\n\nAs transparency increases, bias typically increases\nInterpretable models require functional specification, which means making assumptions\nModern machine learning needs universal approximators to model complex dependencies\nUniversal approximators (RFs, NNs, SVMs) tend to be local in nature and difficult to interpret globally"
  },
  {
    "objectID": "lectures/lecture27.html#post-hoc-interpretability-methods",
    "href": "lectures/lecture27.html#post-hoc-interpretability-methods",
    "title": "Lecture 27: Interpretable Deep Learning: Post-Hoc Methods and Glass-Box Models",
    "section": "Post-Hoc Interpretability Methods",
    "text": "Post-Hoc Interpretability Methods\n\nLIME (Local Interpretable Model-agnostic Explanations)\n\nGoal: Understand how changes in feature values affect predictions for individual instances\nProcess:\n\nCreate perturbed samples around target instance \\(\\mathbf{x}\\)\nGet predictions for perturbed samples from black box\nCompute proximity weights using kernel function\nFit weighted LASSO to create local linear approximation: \\(\\hat{\\boldsymbol{\\beta}} = \\underset{\\{\\beta_0, \\boldsymbol{\\beta}\\}}{\\text{argmin}} \\sum_{i=1}^N w_i(\\hat{y}_i - \\beta_0 - \\mathbf{x}_i' \\boldsymbol{\\beta})^2 + \\lambda \\sum_{j=1}^P |\\beta_j|\\)\n\nStrengths: Short, understandable explanations; easy implementation\nWeaknesses: Sensitive to neighborhood choice; unrealistic data points; limited to single-point analysis\n\n\n\nPermutation Importance\n\nMeasures feature importance as impact on model performance\nProcedure:\n\nFit model and compute baseline performance on validation set\nFor each feature, randomly permute its values and measure performance drop\nRank features by performance decrease\n\nAdvantage: Global measure of feature importance without refitting model\nApplication: Standard method for random forests, gradient boosting, tabular neural nets"
  },
  {
    "objectID": "lectures/lecture27.html#image-and-text-interpretability",
    "href": "lectures/lecture27.html#image-and-text-interpretability",
    "title": "Lecture 27: Interpretable Deep Learning: Post-Hoc Methods and Glass-Box Models",
    "section": "Image and Text Interpretability",
    "text": "Image and Text Interpretability\n\nSaliency Maps for Images\n\nVanilla Gradient Method:\n\nPerform forward pass to get class score\nCompute gradient of score with respect to input pixels: \\(\\frac{\\partial s_c}{\\partial \\mathbf{x}}\\)\nVisualize gradient as heatmap\n\n\nLocal linear approximation: \\(s_c(\\mathbf{x}) \\approx \\mathbf{w}^T \\mathbf{x} + b\\) where \\(\\mathbf{w} = \\frac{\\partial s_c}{\\partial \\mathbf{x}}|_{\\mathbf{x}_0}\\)\n\nSmoothGrad: Average gradients over multiple noise-added versions of image to reduce sensitivity\nGradient-weighted Class Activation Mapping (Grad-CAM):\n\nCompute gradient of score w.r.t. final convolutional layer activations\nCompute weights \\(\\alpha_d\\) by global average pooling gradients for each filter\nCreate coarse heatmap: \\(\\mathbf{x}_{ij} = \\text{ReLU}\\left(\\sum_{d=1}^D \\alpha_d x_{i,j,d}\\right)\\)\nUpscale to original image size\n\nGuided Grad-CAM: Combine vanilla gradients with Grad-CAM for finer detail\n\n\n\nAttention Visualization for Text\n\nGoal: Understand how words relate to each other and contribute to classification\nChallenge: Multiple attention heads and layers in transformer models\nAttention matrices: Visualization of attention weights between tokens\nAggregation methods:\n\nLayer-wise averaging across attention heads\nModel-wide averaging across layers\nRoll-out: Account for residual connections by adding identity matrix to attention matrices\n\nAugmented layer-wise attention: \\(\\tilde{\\mathbf{A}}^\\ell = \\mathbf{A}^{\\ell} + \\mathcal{I}\\)\nRolled-out attention: \\(\\mathbf{R} = \\hat{\\mathbf{A}}^{(1)} \\times \\hat{\\mathbf{A}}^{(2)} \\times ... \\times \\hat{\\mathbf{A}}^{(L)}\\)\n\n\nLimitation: Aggregation loses specificity and causal importance"
  },
  {
    "objectID": "lectures/lecture27.html#intrinsic-interpretability",
    "href": "lectures/lecture27.html#intrinsic-interpretability",
    "title": "Lecture 27: Interpretable Deep Learning: Post-Hoc Methods and Glass-Box Models",
    "section": "Intrinsic Interpretability",
    "text": "Intrinsic Interpretability\n\nLimitations of Post-Hoc Methods\n\nCannot expect models to learn interpretable structures without designing for interpretability\nPost-hoc explanation may not reflect actual decision process\nAnalogy: Training a puppy is more effective with immediate reinforcement than delayed feedback\n\n\n\nMixture of Experts (MoE)\n\nApproach: Divide hidden units into expert blocks, each specializing in specific input patterns\nImplementation:\n\nBreak feedforward layer into K equally sized expert blocks\nPass input through routing function that selects k &lt; K experts\nProcess input only through selected experts\n\nBenefits:\n\nEach expert specializes in meaningful aspects of inputs\nEnsemble learning advantage: reduced model variance\nEfficiency: Significant decrease in training time for large models\nApplications: Found in Switch Transformers, LLaMA, DeepSeek models\n\n\n\n\nOther Glass-Box Approaches\n\nConcept Bottleneck Models: Predict human-interpretable concepts as intermediary step\nKNN Retrieval Models: Retrieve similar training examples to justify predictions"
  },
  {
    "objectID": "lectures/lecture27.html#future-directions",
    "href": "lectures/lecture27.html#future-directions",
    "title": "Lecture 27: Interpretable Deep Learning: Post-Hoc Methods and Glass-Box Models",
    "section": "Future Directions",
    "text": "Future Directions\n\nResearch Priorities\n\nDesigning models with built-in interpretability without sacrificing performance\nIntegration of interpretability considerations into model training\nImproving efficiency and scalability of interpretable architectures\n\n\n\nIndustry Impact\n\nGrowing demand for interpretable AI in high-stakes applications\nCompanies developing specialized architectures with transparency built in\nCompliance with emerging regulatory requirements driving innovation"
  },
  {
    "objectID": "lectures/lecture3.html",
    "href": "lectures/lecture3.html",
    "title": "Lecture 3: Generalization: The Predictive Goal",
    "section": "",
    "text": "← Previous: Lecture 2\n\n\nNext: Lecture 4 →"
  },
  {
    "objectID": "lectures/lecture3.html#understanding-generalization-error",
    "href": "lectures/lecture3.html#understanding-generalization-error",
    "title": "Lecture 3: Generalization: The Predictive Goal",
    "section": "1. Understanding Generalization Error",
    "text": "1. Understanding Generalization Error\nThe lecture initially breaks down generalization error with a foundational formula: \\[Generalization Error = Training Error + \\frac{2}{N} \\sum Cov(y_i, \\hat{y}_i)\\] This equation highlights that the discrepancy between generalization error and training error widens as the covariance between the actual outcomes (\\(y_i\\)) and the model’s predictions (\\(\\hat{y}_i\\)) increases.\nKey introductory concepts include:\n\nFixed X Assumption: A simplifying assumption in early analysis where training features (X) are considered constant, while outcomes vary due to different noise draws (e.g., \\(y = f(x) + \\epsilon\\) vs. \\(y' = f(x) + \\epsilon'\\)).\nPolynomial Regression Example: This serves as an illustration of how increasing model complexity (e.g., the degree of a polynomial) can lead to the model becoming overly “wiggly” and fitting to noise, a phenomenon known as overfitting. The lecture uses Python code and visualizations to demonstrate how polynomials of varying degrees fit noisy data.\n\nA core idea presented is that the prediction at any given point is a weighted combination of the training outcomes: \\[\\hat{y}_0 = \\mathbf{x}_0^T \\hat{\\beta} = \\mathbf{x}_0^T (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}\\]\nFor linear models, the sum of self-covariances is shown to be \\(P\\sigma^2\\), which results in a training error offset of \\(\\frac{2P\\sigma^2}{N}\\). This implies that a model with more features (a higher \\(P\\)) will have increased covariance, thereby enlarging the gap between training and generalization error. This effect is mitigated by a larger sample size \\(N\\).\n\nPrimary Factors Affecting Generalization:\n\nThe magnitude of the training error.\nThe covariance between training outcomes and the model’s predictions.\nThe size of the training sample."
  },
  {
    "objectID": "lectures/lecture3.html#bias-variance-decomposition-for-mse",
    "href": "lectures/lecture3.html#bias-variance-decomposition-for-mse",
    "title": "Lecture 3: Generalization: The Predictive Goal",
    "section": "2. Bias-Variance Decomposition (for MSE)",
    "text": "2. Bias-Variance Decomposition (for MSE)\nWhen the fixed X assumption is relaxed, the generalization error under Mean Squared Error (MSE) can be decomposed into three fundamental components: \\[E_\\mathcal{T}[(y - \\hat{y})^2] = V_{y|x}[y] (Irreducible Error) + V_D[\\hat{y}] (Model Variance) + (E_D[\\hat{y}] - f(x))^2 (Bias^2)\\] Let’s examine each term:\n\nBias (\\(Bias^2\\)): Defined as \\((E_D[\\hat{y}] - f(x))^2\\). This term quantifies the difference between the average prediction of the model (if trained on many different datasets D) and the true underlying function \\(f(x)\\). Bias tends to decrease as model flexibility increases, but it is bounded. Underfitting is often characterized by high bias.\nModel Variance (\\(V_D[\\hat{y}]\\)): This term measures the variability of the model’s predictions if it were trained on different datasets D. Model variance typically increases with model flexibility (complexity) and decreases with a larger sample size. Overfitting is often characterized by high model variance.\nIrreducible Error (\\(V_{y|x}[y]\\)): This component represents the inherent noise or randomness in the data generating process that no model can eliminate.\n\nPython examples are used to visualize how bias and model variance change for polynomial regression models of different degrees and with varying sample sizes, effectively illustrating the bias-variance tradeoff.\nAn approximation for Generalization Error encapsulates this tradeoff: \\[\\sigma^2 + \\mathcal{O}(\\frac{1}{C}) + \\mathcal{O}(\\frac{C}{N})\\] Here, \\(C\\) represents model complexity. This formula underscores that if \\(C\\) is too high, model variance can dominate (overfitting), and if \\(C\\) is too low, bias can dominate (underfitting)."
  },
  {
    "objectID": "lectures/lecture3.html#generalization-in-classification-01-loss",
    "href": "lectures/lecture3.html#generalization-in-classification-01-loss",
    "title": "Lecture 3: Generalization: The Predictive Goal",
    "section": "3. Generalization in Classification (0/1 Loss)",
    "text": "3. Generalization in Classification (0/1 Loss)\nFor classification tasks, where 0/1 loss is common, the generalization error is the expected probability of misclassification: \\[E_\\mathcal{T}[I(y \\neq \\hat{y})]\\]\n\nVapnik-Chervonenkis (VC) Bound: This provides a theoretical upper bound on the generalization error for classification models: \\[GenError \\le TrainError + \\sqrt{\\frac{1}{N} \\left[ d \\left(\\log \\frac{2N}{d} + 1\\right) - \\log \\frac{\\delta}{4} \\right]}\\] In this bound, \\(d\\) is the VC dimension of the classifier.\nVC Dimension (\\(d\\)): This is a measure of a classifier’s complexity or capacity. It’s defined as the size of the largest set of points that the classifier can “shatter” (i.e., correctly classify for all \\(2^d\\) possible binary labelings).\n\nFor instance, linear classifiers in \\(P\\) dimensions have a VC dimension of \\(P+1\\).\nSome complex models, like 1-Nearest Neighbor (1NN) or Support Vector Machines (SVMs) with Radial Basis Function (RBF) kernels, can have an infinite VC dimension, although the VC bound represents a worst-case scenario.\n\nThe training offset term in the VC bound (the part added to Training Error) increases with the VC dimension \\(d\\) and decreases with the sample size \\(N\\).\nAnalogous to the MSE case, the generalization error for 0/1 loss can also be thought of in terms of a complexity tradeoff: \\[GenError \\approx \\mathcal{O}(\\frac{1}{C}) + \\mathcal{O}(\\frac{C}{N})\\]"
  },
  {
    "objectID": "lectures/lecture3.html#practical-measurement-of-generalization-error",
    "href": "lectures/lecture3.html#practical-measurement-of-generalization-error",
    "title": "Lecture 3: Generalization: The Predictive Goal",
    "section": "4. Practical Measurement of Generalization Error",
    "text": "4. Practical Measurement of Generalization Error\nTheoretical bounds are insightful, but practical estimation of generalization error is crucial for model selection and assessment.\n\nHeldout Test Set: This is a portion of data that is never used during the training or tuning phases. It provides an unbiased estimate of how the model will perform on new, unseen data.\nValidation Set: This dataset is used to tune model hyperparameters (e.g., selecting the value of K in K-Nearest Neighbors, or determining the strength of a regularization penalty).\nData Splitting Strategies:\n\nFor large datasets, a common approach is a three-way split: training set, validation set, and test set.\nFor smaller datasets, K-fold cross-validation is often preferred for hyperparameter tuning to make more efficient use of limited data, followed by a final evaluation on a heldout test set.\n\n\nThe lecture strongly emphasizes that models should never be chosen based solely on their performance on the training error, as this is not indicative of real-world performance."
  },
  {
    "objectID": "lectures/lecture3.html#conclusion",
    "href": "lectures/lecture3.html#conclusion",
    "title": "Lecture 3: Generalization: The Predictive Goal",
    "section": "Conclusion",
    "text": "Conclusion\nThe lecture wraps up by highlighting that the next topics will focus on specific regression methodologies, including linear and logistic regression, the concept of likelihoods, and the use of regularization techniques to improve generalization. A detailed derivation of the MSE decomposition is also noted as being available at the end of the original lecture slides."
  },
  {
    "objectID": "lectures/lecture5.html",
    "href": "lectures/lecture5.html",
    "title": "Lecture 5: Loss Minimization and Optimization",
    "section": "",
    "text": "← Previous: Lecture 4\n\n\nNext: Lecture 6 →"
  },
  {
    "objectID": "lectures/lecture5.html#the-empirical-risk-minimization-framework",
    "href": "lectures/lecture5.html#the-empirical-risk-minimization-framework",
    "title": "Lecture 5: Loss Minimization and Optimization",
    "section": "1. The Empirical Risk Minimization Framework",
    "text": "1. The Empirical Risk Minimization Framework\nMachine learning problems with parameters can generally be expressed as minimizing empirical risk:\n\\[\\hat{\\boldsymbol{\\beta}} = \\underset{\\boldsymbol{\\beta}}{\\text{argmin}} \\frac{1}{N} \\sum_{i=1}^{N} L(y_i, f(\\boldsymbol{\\beta}, \\mathbf{x}_i)) + \\lambda C(\\boldsymbol{\\beta})\\]\nThis framework encompasses numerous methods including:\n\nLinear regression with MSE loss\nLogistic regression with cross-entropy loss\nSupport vector machines with hinge loss\n\nEach requires finding coefficients that minimize their respective loss functions."
  },
  {
    "objectID": "lectures/lecture5.html#mathematical-foundations-of-optimization",
    "href": "lectures/lecture5.html#mathematical-foundations-of-optimization",
    "title": "Lecture 5: Loss Minimization and Optimization",
    "section": "2. Mathematical Foundations of Optimization",
    "text": "2. Mathematical Foundations of Optimization\nFinding a minimum in the parameter space involves understanding:\n2.1. Gradients and Critical Points\nThe gradient vector represents the direction of steepest ascent at any point:\n\\[\\mathbf{g}(\\boldsymbol{\\theta}) = \\nabla \\mathcal{L}(\\boldsymbol{\\theta}) = \\begin{bmatrix}\n\\frac{\\partial \\mathcal{L}(\\boldsymbol{\\theta})}{\\partial \\theta_1} \\\\\n\\frac{\\partial \\mathcal{L}(\\boldsymbol{\\theta})}{\\partial \\theta_2} \\\\\n\\vdots\n\\end{bmatrix}\\]\nA critical point occurs when \\(\\mathbf{g}(\\boldsymbol{\\theta}) = \\mathbf{0}\\).\n2.2. Hessians and Convexity\nThe Hessian matrix of second derivatives determines whether a critical point is a minimum:\n\\[\\mathbf{H}(\\boldsymbol{\\theta}) = \\nabla^2 \\mathcal{L}(\\boldsymbol{\\theta})\\]\nA function is strictly convex if its Hessian is positive definite everywhere, guaranteeing a unique global minimum.\nFor regularized linear regression, the Hessian: \\[\\mathcal{H}(\\boldsymbol{\\beta}) = \\frac{2}{N}[\\mathbf{X}^T\\mathbf{X} + \\lambda\\mathcal{I}_P]\\] is always positive definite, ensuring a single global minimum.\nFor logistic regression, the Hessian can be expressed as \\(\\mathbf{X}^T\\mathbf{S}\\mathbf{X}\\), where \\(\\mathbf{S}\\) is a diagonal matrix with positive elements, also ensuring convexity."
  },
  {
    "objectID": "lectures/lecture5.html#gradient-descent-methods",
    "href": "lectures/lecture5.html#gradient-descent-methods",
    "title": "Lecture 5: Loss Minimization and Optimization",
    "section": "3. Gradient Descent Methods",
    "text": "3. Gradient Descent Methods\n3.1. Basic Gradient Descent\nThe simplest optimization approach follows the direction of steepest descent:\n\\[\\boldsymbol{\\beta}_{t+1} = \\boldsymbol{\\beta}_t - \\eta \\mathbf{g}(\\boldsymbol{\\beta}_t)\\]\nWhere:\n\n\\(\\eta\\) is the step size (learning rate)\n\\(\\mathbf{g}(\\boldsymbol{\\beta}_t)\\) is the gradient evaluated at the current position\n\nThis approach guarantees convergence to a minimum with a sufficiently small step size, but the trade-off between convergence speed and precision poses practical challenges.\n3.2. Stochastic Gradient Descent (SGD)\nRather than using all N training examples to compute the gradient, SGD uses a small batch B (often just one example):\n\\[\\mathbf{g}(\\boldsymbol{\\beta}; \\mathcal{B}) = \\frac{1}{B} \\sum_{j \\in \\mathcal{B}} \\mathbf{g}(\\boldsymbol{\\beta}; \\mathbf{x}_j, y_j)\\]\nKey advantages:\n\nReduced computational complexity from \\(\\mathcal{O}(NP)\\) to \\(\\mathcal{O}(P)\\) per step\nAvoids wasting time on redundant examples\nAllows quick initial movement away from poor starting points\n\nChallenges:\n\nNoisy gradient estimates lead to a “noise ball” around the minimum\nSensitivity to learning rate selection\nDifficulty determining convergence"
  },
  {
    "objectID": "lectures/lecture5.html#improving-sgd-performance",
    "href": "lectures/lecture5.html#improving-sgd-performance",
    "title": "Lecture 5: Loss Minimization and Optimization",
    "section": "4. Improving SGD Performance",
    "text": "4. Improving SGD Performance\n4.1. Learning Rate Schedules\nInstead of a fixed learning rate, polynomial decay schedules reduce step size over time:\n\\[\\eta_t = \\eta_0(bt + 1)^{-a}\\]\nThis approach allows:\n\nLarge initial steps to quickly move away from poor starting positions\nProgressively smaller steps as the algorithm approaches the minimum\nReduced final noise ball size\n\n4.2. Iterate Averaging\nComputing a running average of parameter values during SGD:\n\\[\\bar{\\boldsymbol{\\beta}}_{t} = \\frac{1}{t} \\sum \\boldsymbol{\\beta}_t = \\frac{1}{t} \\boldsymbol{\\beta}_t + \\frac{t-1}{t} \\bar{\\boldsymbol{\\beta}}_{t-1}\\]\nThis technique:\n\nProvides theoretical guarantees for optimal variance\nReduces the impact of random fluctuations around the minimum\nCan work effectively even without learning rate schedules"
  },
  {
    "objectID": "lectures/lecture5.html#practical-implementations",
    "href": "lectures/lecture5.html#practical-implementations",
    "title": "Lecture 5: Loss Minimization and Optimization",
    "section": "5. Practical Implementations",
    "text": "5. Practical Implementations\nEmpirical demonstrations with logistic regression reveal:\n\nThe critical impact of step size selection on convergence\nHow SGD with just one example per step can match full gradient descent with proper tuning\nThe effectiveness of combining SGD with learning rate schedules or iterate averaging\nThe challenge of balancing convergence speed with final accuracy\n\nConclusion:\nOptimization methods like SGD make machine learning feasible for large datasets by dramatically reducing computational complexity, particularly for models without closed-form solutions. The trade-offs between computational efficiency, convergence speed, and final accuracy can be managed through techniques like learning rate scheduling and iterate averaging. Modern improvements to SGD, including momentum and adaptive methods, further enhance efficiency by reducing the “random walk” behavior characteristic of basic SGD implementations."
  },
  {
    "objectID": "lectures/lecture7.html",
    "href": "lectures/lecture7.html",
    "title": "Lecture 7: Nonlinearities and Universal Approximators",
    "section": "",
    "text": "← Previous: Lecture 6\n\n\nNext: Lecture 8 →"
  },
  {
    "objectID": "lectures/lecture7.html#the-need-for-expressive-models",
    "href": "lectures/lecture7.html#the-need-for-expressive-models",
    "title": "Lecture 7: Nonlinearities and Universal Approximators",
    "section": "1. The Need for Expressive Models",
    "text": "1. The Need for Expressive Models\nIn the machine learning framework, the goal is to learn a function that maps inputs to outputs:\n\\[y = f(\\mathbf{x}) + \\epsilon\\]\nWhile we aim to minimize generalization error, this depends on three components:\n\nBias: How far the proposed function deviates from the truth on average\nIrreducible Error: Variation unexplainable by available features\nComplexity: Model capacity that can lead to overfitting\n\nAs training data grows, complexity concerns diminish, but bias persists. For complex problems with nonlinear decision boundaries, standard linear models cannot capture the true functional form, resulting in persistent bias regardless of data size."
  },
  {
    "objectID": "lectures/lecture7.html#universal-approximators",
    "href": "lectures/lecture7.html#universal-approximators",
    "title": "Lecture 7: Nonlinearities and Universal Approximators",
    "section": "2. Universal Approximators",
    "text": "2. Universal Approximators\nUniversal approximators are learning methods that can theoretically approximate any Borel measurable function from one finite-dimensional space to another with arbitrary precision. In practical terms, these methods can learn any reasonable continuous function given sufficient resources.\n2.1. Theoretical vs. Practical Capability\nWhile universal approximators can theoretically represent any function, two factors limit practical success:\n\nOptimization challenges: Non-convex loss landscapes with local minima\nOverfitting: The training algorithm may learn noise rather than the true function\n\nWhen evaluating methods as universal approximators, a key indicator is their theoretical ability to achieve zero training error for any dataset—though this capability alone doesn’t guarantee good generalization."
  },
  {
    "objectID": "lectures/lecture7.html#k-nearest-neighbors-knn",
    "href": "lectures/lecture7.html#k-nearest-neighbors-knn",
    "title": "Lecture 7: Nonlinearities and Universal Approximators",
    "section": "3. K-Nearest Neighbors (KNN)",
    "text": "3. K-Nearest Neighbors (KNN)\nKNN can represent arbitrarily complex functions as training data grows, but suffers severely from the curse of dimensionality.\n3.1. Characteristics and Limitations\n\nWith 1-NN, the method partitions the feature space into Voronoi cells\nTo effectively cover a space with \\(P\\) dimensions, approximately \\(10^P\\) training points are needed\nThis exponential growth in parameter requirements creates generalization challenges\nThe generalization error grows according to: \\(E_\\mathcal{T}[\\mathcal{L}(y - \\hat{y})] = \\frac{1}{N} \\sum_{i=1}^N \\mathcal{L}(y_i - \\hat{y}_i) + \\mathcal{O} \\left(\\frac{P}{N} \\right)\\)\nAs dimensionality increases, an exponentially larger training set is needed to maintain generalization performance"
  },
  {
    "objectID": "lectures/lecture7.html#polynomial-expansions-and-feature-transformations",
    "href": "lectures/lecture7.html#polynomial-expansions-and-feature-transformations",
    "title": "Lecture 7: Nonlinearities and Universal Approximators",
    "section": "4. Polynomial Expansions and Feature Transformations",
    "text": "4. Polynomial Expansions and Feature Transformations\nAdding polynomial terms to linear models creates more expressive models capable of capturing nonlinear relationships.\n4.1. Explicit Polynomial Features\n\nA full polynomial expansion of degree \\(d\\) with \\(P\\) features results in \\(\\binom{P+d}{d} \\approx \\frac{P^d}{d!}\\) terms\nThis approach suffers from:\n\nExponential parameter growth with increasing degree or dimensionality\nDegraded generalization with generalization gap scaling as \\(\\mathcal{O} \\left( \\frac{P^d}{Nd!} \\right)\\)\nComputational complexity for training scaling as \\(\\mathcal{O} \\left(\\frac{NP^{2d}}{d!} \\right)\\)\n\n\n4.2. Kernel Methods\nKernel methods implicitly map data to higher-dimensional spaces without explicitly computing the transformation:\n\nThe Radial Basis Function (RBF) kernel: \\(K(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp(-\\gamma \\|\\mathbf{x}_i - \\mathbf{x}_j\\|^2)\\)\nThe RBF kernel is a universal approximator where the \\(\\gamma\\) parameter controls flexibility\nWhile powerful for complex relationships, RBF kernels face challenges:\n\nPotential for poor generalization without proper regularization\nComputational scaling issues as training size grows (\\(\\mathcal{O}(N^3)\\) for matrix inversion)\nMemory requirements for storing the \\(N \\times N\\) kernel matrix"
  },
  {
    "objectID": "lectures/lecture7.html#local-approximation-methods",
    "href": "lectures/lecture7.html#local-approximation-methods",
    "title": "Lecture 7: Nonlinearities and Universal Approximators",
    "section": "5. Local Approximation Methods",
    "text": "5. Local Approximation Methods\nThese methods divide the feature space into regions and fit local polynomial models within each region.\n5.1. Regression Splines\n\nSplit the feature space into \\(K\\) regions with division points \\(\\boldsymbol{\\xi}\\)\nFit local polynomials within each region\nEnsure continuity at region boundaries\nEffective for low-dimensional problems (\\(P &lt; 5\\)), but suffer from the curse of dimensionality as feature dimensions increase\n\n5.2. Tree-Based Methods\nDecision trees partition the feature space through recursive binary splitting:\n\nSingle decision trees can theoretically approximate any function when grown to full depth\nHowever, they suffer from poor generalization due to overfitting\nTraining complexity for a single tree to full depth is \\(\\mathcal{O}(P N \\log N)\\)\n\n5.3. Random Forests\nRandom Forests address the generalization issues of single trees:\n\nLimit feature consideration at each split (random feature subset of size \\(P'\\))\nGrow multiple trees to full depth\nAverage predictions through bagging\nComputational complexity scales as \\(\\mathcal{O}(B P' N \\log N)\\) where \\(B\\) is the number of trees\nEffectively combat overfitting while maintaining universal approximation properties\nParticularly effective for tabular data but less optimal for complex data types like images, speech, and text"
  },
  {
    "objectID": "lectures/lecture7.html#approaching-neural-networks",
    "href": "lectures/lecture7.html#approaching-neural-networks",
    "title": "Lecture 7: Nonlinearities and Universal Approximators",
    "section": "6. Approaching Neural Networks",
    "text": "6. Approaching Neural Networks\nThe limitations of these methods point toward neural networks as another powerful universal approximator. Neural networks address the curse of dimensionality through:\n\nEngineering low-dimensional representations of high-dimensional spaces\nHierarchical feature extraction\nParameter sharing to reduce effective parameter count\nFlexible architectures adaptable to various data types\n\nThese characteristics make neural networks particularly effective for complex data types where other universal approximators struggle with either generalization or computational scaling."
  },
  {
    "objectID": "lectures/lecture9.html",
    "href": "lectures/lecture9.html",
    "title": "Lecture 9: Deep Neural Networks: What do they learn? Do they learn things?? Let’s find out!",
    "section": "",
    "text": "← Previous: Lecture 8\n\n\nNext: Lecture 10 →"
  },
  {
    "objectID": "lectures/lecture9.html#from-single-layer-to-deep-networks",
    "href": "lectures/lecture9.html#from-single-layer-to-deep-networks",
    "title": "Lecture 9: Deep Neural Networks: What do they learn? Do they learn things?? Let’s find out!",
    "section": "1. From Single-Layer to Deep Networks",
    "text": "1. From Single-Layer to Deep Networks\nSingle-layer neural networks can theoretically approximate any function with sufficient hidden units, but may require an impractically large number of parameters. A standard single-layer, fully-connected neural network is defined as:\n\\[\\theta_i = g(\\boldsymbol{\\beta}^T \\varphi(\\mathbf{W}^T \\mathbf{x}_i + \\mathbf{c}) + b)\\]\nWhere:\n\n\\(\\mathbf{W}\\) is a \\(P \\times K\\) weight matrix connecting inputs to hidden units\n\\(\\mathbf{c}\\) is a vector of bias terms for the hidden layer\n\\(\\varphi()\\) is a nonlinear activation function (typically ReLU)\n\\(\\boldsymbol{\\beta}\\) is a vector of weights connecting hidden units to the output\n\\(b\\) is an output bias term\n\nDeep neural networks extend this architecture by incorporating multiple hidden layers, transforming the model to:\n\\[\\theta_i = g(\\boldsymbol{\\beta}^T \\varphi(\\mathbf{W}_2^T \\varphi(\\mathbf{W}_1^T \\mathbf{x}_i + \\mathbf{c}_1) + \\mathbf{c}_2) + b)\\]\nThis nested structure allows for more complex transformations of the input data through successive nonlinear mappings."
  },
  {
    "objectID": "lectures/lecture9.html#the-power-of-hierarchical-representations",
    "href": "lectures/lecture9.html#the-power-of-hierarchical-representations",
    "title": "Lecture 9: Deep Neural Networks: What do they learn? Do they learn things?? Let’s find out!",
    "section": "2. The Power of Hierarchical Representations",
    "text": "2. The Power of Hierarchical Representations\nDeep neural networks derive their effectiveness from learning hierarchical representations of data, with each layer building upon the features extracted by the previous layer.\n2.1. Feature Learning in Practice\nUsing the MNIST dataset of handwritten digits (focusing on 3s and 8s) demonstrates how neural networks learn meaningful representations:\n\nFirst-layer units learn to identify primitive features like edges, curves, and regions of the digit\nSubsequent layers combine these primitive features into more complex patterns\nThe final layers create abstract representations that enable effective classification\n\nThis hierarchical approach follows a similar process to human visual perception, where complex recognition emerges from the combination of simpler features.\n2.2. Latent Space Transformations\nEach layer of a deep network creates a transformed representation of the data:\n\\[\\mathbf{Z}^{(l)} = \\varphi(\\mathbf{Z}^{(l-1)}\\mathbf{W}^{(l)} + \\mathbf{c}^{(l)})\\]\nWhere \\(\\mathbf{Z}^{(l)}\\) represents the activations at layer \\(l\\). These transformations progressively map the data into spaces where:\n\nClass separation becomes more apparent\nRelevant features are emphasized\nIrrelevant variations are suppressed\n\nThis can be viewed as a nonlinear generalization of dimensionality reduction techniques like PCA, where each layer creates increasingly abstract representations of the data."
  },
  {
    "objectID": "lectures/lecture9.html#advantages-of-depth-over-width",
    "href": "lectures/lecture9.html#advantages-of-depth-over-width",
    "title": "Lecture 9: Deep Neural Networks: What do they learn? Do they learn things?? Let’s find out!",
    "section": "3. Advantages of Depth Over Width",
    "text": "3. Advantages of Depth Over Width\nDeep networks with multiple layers offer several advantages over equivalent-sized wide networks with a single hidden layer:\n3.1. Parameter Efficiency\nDeep networks can represent complex functions with fewer parameters than wide networks. For example, representing an XOR-like decision boundary may require:\n\nA single hidden layer with dozens of units\nA deep network with just a few units per layer\n\nThis efficiency stems from the ability to reuse and combine features across layers, allowing complex functions to be decomposed into simpler hierarchical components.\n3.2. Compositional Structure\nDeep networks naturally model compositional relationships, where complex concepts are built from simpler ones. This aligns with the hierarchical structure found in many real-world data types:\n\nIn images: edges → shapes → parts → objects\nIn language: characters → words → phrases → sentences\nIn audio: waveforms → phonemes → words → speech\n\nThis compositional representation provides an inductive bias that helps the network generalize better to unseen examples."
  },
  {
    "objectID": "lectures/lecture9.html#training-deep-neural-networks",
    "href": "lectures/lecture9.html#training-deep-neural-networks",
    "title": "Lecture 9: Deep Neural Networks: What do they learn? Do they learn things?? Let’s find out!",
    "section": "4. Training Deep Neural Networks",
    "text": "4. Training Deep Neural Networks\nWhile deep networks offer greater representational power, they also introduce training challenges that require specialized techniques.\n4.1. Loss Landscape Complexity\nThe loss landscapes of deep neural networks contain:\n\nMultiple local minima of varying quality\nFlat minima that may generalize better than sharp minima\nSaddle points that can slow optimization\nPlateaus where gradients provide little guidance\n\nStochastic gradient descent (SGD) with appropriate learning rate schedules often converges to flat minima, which can lead to better generalization performance compared to exact optimization methods.\n4.2. Gradient Challenges\nDeep networks face several gradient-related challenges:\n\nVanishing gradients: When gradients become extremely small in early layers\nExploding gradients: When gradients become extremely large\nDiscontinuities from ReLU activations\nHigh-dimensional Jacobian matrices that are computationally intensive\n\nThese issues are addressed through techniques like careful initialization, gradient clipping, batch normalization, and alternative activation functions."
  },
  {
    "objectID": "lectures/lecture9.html#geometric-interpretation-of-deep-networks",
    "href": "lectures/lecture9.html#geometric-interpretation-of-deep-networks",
    "title": "Lecture 9: Deep Neural Networks: What do they learn? Do they learn things?? Let’s find out!",
    "section": "5. Geometric Interpretation of Deep Networks",
    "text": "5. Geometric Interpretation of Deep Networks\nDeep networks create a sequence of nonlinear transformations that progressively reshape the input space:\n5.1. Visualization of Hidden Representations\nBy visualizing the activations of hidden layers, we can observe how the network:\n\nClusters similar examples together\nSeparates different classes\nCreates increasingly linear decision boundaries in the transformed spaces\n\nFor the MNIST digits 3 and 8, early layers learn to identify distinctive features such as the empty space on the left side of a 3 or the double circle structure of an 8.\n5.2. Decision Boundaries\nThrough successive nonlinear transformations, deep networks create complex decision boundaries that would be difficult to achieve with simpler models:\n\nEarly layers fold and warp the input space\nMiddle layers further transform the data to enhance separability\nLater layers perform the final classification in the transformed space\n\nThis process allows deep networks to learn complex patterns while maintaining computational efficiency."
  },
  {
    "objectID": "lectures/lecture9.html#the-role-of-optimization-in-generalization",
    "href": "lectures/lecture9.html#the-role-of-optimization-in-generalization",
    "title": "Lecture 9: Deep Neural Networks: What do they learn? Do they learn things?? Let’s find out!",
    "section": "6. The Role of Optimization in Generalization",
    "text": "6. The Role of Optimization in Generalization\nThe optimization process itself plays a critical role in how well deep networks generalize:\n6.1. Implicit Regularization of SGD\nStochastic gradient descent provides an implicit form of regularization:\n\nThe noise in gradient estimates helps escape sharp minima\nSGD tends to converge to flatter regions of the loss landscape\nThese flat minima often correspond to simpler models that generalize better\n\nThis phenomenon helps explain why deep networks can generalize well despite being highly overparameterized.\n6.2. The Backpropagation Algorithm\nTraining deep networks efficiently requires the backpropagation algorithm, which:\n\nApplies the chain rule to compute gradients across multiple layers\nReuses intermediate computations to avoid redundant calculations\nAllows for end-to-end training of all network parameters\n\nDespite its effectiveness, backpropagation still faces challenges with very deep networks, necessitating careful initialization and normalization techniques."
  },
  {
    "objectID": "lectures/search.html",
    "href": "lectures/search.html",
    "title": "Search Course Materials",
    "section": "",
    "text": "Search"
  },
  {
    "objectID": "lectures/search.html#search-tips",
    "href": "lectures/search.html#search-tips",
    "title": "Search Course Materials",
    "section": "Search Tips",
    "text": "Search Tips\n\nUse specific terms from lectures when searching\nTry both general concepts (e.g., “neural networks”) and specific techniques (e.g., “backpropagation”)\nCourse materials are fully searchable, including lecture summaries and slide content"
  },
  {
    "objectID": "lectures/summaries/Lecture11Summary.html",
    "href": "lectures/summaries/Lecture11Summary.html",
    "title": "Lecture 11: Vanishing Gradients and Generalization",
    "section": "",
    "text": "Backpropagation is the primary algorithm for training deep neural networks through a two-step process:\n\nForward Pass: Compute intermediate values through the network based on current parameter values\nBackward Pass: Compute gradients of the loss with respect to parameters by propagating derivatives backward\n\nFor a neural network with multiple layers, the computational graph allows systematic derivative calculation using the chain rule:\n\\[\\text{Downstream Gradient} = \\text{Local Gradient} \\times \\text{Upstream Gradient}\\]\nAt each layer, gradients follow predictable patterns, with the output layer gradients for mean squared error:\n\\[\\frac{\\partial \\mathcal{L}_i}{\\partial z} = -2(y_i - z_i)\\]\n\\[\\frac{\\partial \\mathcal{L}_i}{\\partial \\boldsymbol{\\beta}} = -2(y_i - z_i) \\otimes \\mathbf{h}_2\\]\n\\[\\frac{\\partial \\mathcal{L}_i}{\\partial \\mathbf{h}_2} = -2(y_i - z_i) \\otimes \\boldsymbol{\\beta}^T\\]\nThe gradient at any layer depends on all upstream derivatives, creating chains of multiplication across layers:\n\\[\\mathbf{u} = \\frac{\\partial \\mathcal{L}}{\\partial z} \\otimes \\boldsymbol{\\beta}^T \\odot \\varphi'(\\mathbf{q}_D) \\otimes \\mathbf{W}_D^T \\odot \\varphi'(\\mathbf{q}_{D-1}) \\otimes \\mathbf{W}_{D-1}^T \\odot \\cdots\\]"
  },
  {
    "objectID": "lectures/summaries/Lecture11Summary.html#backpropagation-review",
    "href": "lectures/summaries/Lecture11Summary.html#backpropagation-review",
    "title": "Lecture 11: Vanishing Gradients and Generalization",
    "section": "",
    "text": "Backpropagation is the primary algorithm for training deep neural networks through a two-step process:\n\nForward Pass: Compute intermediate values through the network based on current parameter values\nBackward Pass: Compute gradients of the loss with respect to parameters by propagating derivatives backward\n\nFor a neural network with multiple layers, the computational graph allows systematic derivative calculation using the chain rule:\n\\[\\text{Downstream Gradient} = \\text{Local Gradient} \\times \\text{Upstream Gradient}\\]\nAt each layer, gradients follow predictable patterns, with the output layer gradients for mean squared error:\n\\[\\frac{\\partial \\mathcal{L}_i}{\\partial z} = -2(y_i - z_i)\\]\n\\[\\frac{\\partial \\mathcal{L}_i}{\\partial \\boldsymbol{\\beta}} = -2(y_i - z_i) \\otimes \\mathbf{h}_2\\]\n\\[\\frac{\\partial \\mathcal{L}_i}{\\partial \\mathbf{h}_2} = -2(y_i - z_i) \\otimes \\boldsymbol{\\beta}^T\\]\nThe gradient at any layer depends on all upstream derivatives, creating chains of multiplication across layers:\n\\[\\mathbf{u} = \\frac{\\partial \\mathcal{L}}{\\partial z} \\otimes \\boldsymbol{\\beta}^T \\odot \\varphi'(\\mathbf{q}_D) \\otimes \\mathbf{W}_D^T \\odot \\varphi'(\\mathbf{q}_{D-1}) \\otimes \\mathbf{W}_{D-1}^T \\odot \\cdots\\]"
  },
  {
    "objectID": "lectures/summaries/Lecture11Summary.html#vanishing-gradients",
    "href": "lectures/summaries/Lecture11Summary.html#vanishing-gradients",
    "title": "Lecture 11: Vanishing Gradients and Generalization",
    "section": "Vanishing Gradients",
    "text": "Vanishing Gradients\nVanishing gradients occur when the product chain of derivatives approaches zero as it propagates backward through the network. This creates two significant problems:\n\nParameters in early layers receive minimal updates\nThe network cannot learn long-range dependencies\n\n\nSigmoid Activation Problems\nThe sigmoid activation function is a primary culprit in vanishing gradients:\n\\[\\varphi(q) = \\sigma(q) = \\frac{1}{1 + \\exp[-q]}\\]\nIts derivative:\n\\[\\varphi'(q) = \\sigma(q)(1 - \\sigma(q))\\]\nThis derivative approaches zero when inputs are either very large (&gt;3) or very small (&lt;-3), causing gradients to effectively vanish during backpropagation.\n\n\nActivation Function Solutions\nSeveral activation functions address the vanishing gradient problem:\n\nReLU (Rectified Linear Unit): \\[\\varphi(q) = \\max(0, q)\\]\n\nNon-saturating for positive values\nInduces sparsity in hidden representations\nSimple and computationally efficient\nBut can suffer from “dead ReLUs” when units persistently output zero\n\nLeaky ReLU: \\[\\varphi(q) = \\max(\\alpha q, q), \\text{ where } 0 &lt; \\alpha &lt; 1\\]\n\nPrevents complete deactivation of neurons\nSmall slope for negative values keeps gradient flowing\nTypically α = 0.01 or 0.1\n\nELU (Exponential Linear Unit): \\[\\varphi(q) = \\begin{cases}\nq & \\text{if } q &gt; 0 \\\\\n\\alpha(e^q - 1) & \\text{if } q \\leq 0\n\\end{cases}\n\\]\n\nSmooth transition at q=0\nNegative saturation reduces noise influence\nCan help with faster convergence\n\n\n\n\nBatch Normalization\nBatch normalization provides an architectural solution to gradient problems by normalizing activations:\n\\[\\tilde{\\mathbf{q}}_k = \\gamma \\odot \\hat{\\mathbf{q}}_k + \\delta\\]\n\\[\\hat{\\mathbf{q}}_k = \\frac{\\mathbf{q}_k - \\mu_k}{\\sqrt{\\sigma^2_k + \\epsilon}}\\]\nWhere μ₍ and σ₍² are the batch mean and variance. This technique:\n\nStandardizes activations at each layer\nPrevents weights from becoming too large or too small\nAllows training with higher learning rates\nActs as a form of regularization\nReduces internal covariate shift"
  },
  {
    "objectID": "lectures/summaries/Lecture11Summary.html#generalization-in-neural-networks",
    "href": "lectures/summaries/Lecture11Summary.html#generalization-in-neural-networks",
    "title": "Lecture 11: Vanishing Gradients and Generalization",
    "section": "Generalization in Neural Networks",
    "text": "Generalization in Neural Networks\nNeural networks can achieve near-zero training error but often struggle with overfitting due to their expressive capacity. Several techniques address this challenge:\n\nEarly Stopping\n\nMonitor validation loss during training\nStop when validation loss begins to increase\nPrevents learning noise in the training data\nCan be viewed as controlling the effective number of parameters\n\n\n\nWeight Decay (L2 Regularization)\nAdds a penalty term to the loss function:\n\\[\\frac{1}{N} \\sum_{i=1}^N \\mathcal{L}(\\boldsymbol{\\theta} | \\mathbf{x}_i, y_i) + \\lambda \\sum_{d=1}^D \\|\\mathbf{W}_d\\|^2_F\\]\nWhere the Frobenius norm is:\n\\[\\|\\mathbf{W}_d\\|^2_F = \\sum_{i=1}^{K_{d-1}} \\sum_{j=1}^{K_d} w_{ij}^2\\]\nThis regularization:\n\nPenalizes large weights\nSlows learning to find better generalizable solutions\nHelps close the gap between training and generalization error\nEffectively controls the number of hidden units without retraining\n\n\n\nDropout\nDuring training, randomly set weights to zero with probability p:\n\\[\\theta_{dij} = \\mathbf{W}_{dij} \\epsilon_{di}\\]\nWhere ε_di = 0 with probability p and ε_di = 1 with probability (1-p).\nAt inference time, weights are scaled by (1-p).\nDropout:\n\nForces the network to learn robust feature dependencies\nFunctions like an ensemble of multiple network configurations\nPrevents co-adaptation of hidden units\nProvides implicit regularization similar to ridge regression\nTypically p = 0.5, but 0.2-0.25 often works well in practice\n\nThese techniques together—proper activation functions, batch normalization, early stopping, weight decay, and dropout—form a powerful toolkit for training deep neural networks that generalize well to unseen data."
  },
  {
    "objectID": "lectures/summaries/Lecture13Summary.html",
    "href": "lectures/summaries/Lecture13Summary.html",
    "title": "Lecture 13: Advanced CNN Architectures and Transfer Learning",
    "section": "",
    "text": "Convolutional Neural Networks (CNNs) require careful architectural design to achieve optimal performance on complex image classification tasks. Several key design principles have emerged from empirical research:\n\n\nEffective CNN architectures follow a consistent pattern:\n\nStart with rich input representations (e.g., 3×32×32 for RGB images)\nApply successive convolution and pooling layers to downsample spatial dimensions\nCompensate for downsampling by increasing the number of filters\nPreserve approximate volume through the network (H×W×C remains roughly constant)\n\nFor example, a typical progression might transform:\n\nInput: 3×32×32 (3,072 values)\nEarly layers: 64×16×16 (16,384 values)\nMiddle layers: 128×8×8 (8,192 values)\nDeep layers: 256×4×4 (4,096 values)\n\n\n\n\nBatch normalization is essential for training deep CNNs, helping to address vanishing gradients. For each channel in the feature maps:\n\\[\\mu_c = \\frac{1}{N \\times H \\times W} \\sum_{i,j,k} x_{i,j,k}\\]\n\\[\\sigma^2_c = \\frac{1}{N \\times H \\times W} \\sum_{i,j,k} (x_{i,j,k} - \\mu_c)^2\\]\n\\[\\hat{x}_{i,j,k} = \\frac{x_{i,j,k} - \\mu_c}{\\sqrt{\\sigma^2_c + \\epsilon}}\\]\n\\[y_{i,j,k} = \\gamma_c \\hat{x}_{i,j,k} + \\delta_c\\]\nWhere:\n\n\\(\\gamma_c\\) and \\(\\delta_c\\) are learnable parameters\nNormalization layers typically follow convolution layers\nThis process stabilizes and accelerates training"
  },
  {
    "objectID": "lectures/summaries/Lecture13Summary.html#cnn-architecture-design-principles",
    "href": "lectures/summaries/Lecture13Summary.html#cnn-architecture-design-principles",
    "title": "Lecture 13: Advanced CNN Architectures and Transfer Learning",
    "section": "",
    "text": "Convolutional Neural Networks (CNNs) require careful architectural design to achieve optimal performance on complex image classification tasks. Several key design principles have emerged from empirical research:\n\n\nEffective CNN architectures follow a consistent pattern:\n\nStart with rich input representations (e.g., 3×32×32 for RGB images)\nApply successive convolution and pooling layers to downsample spatial dimensions\nCompensate for downsampling by increasing the number of filters\nPreserve approximate volume through the network (H×W×C remains roughly constant)\n\nFor example, a typical progression might transform:\n\nInput: 3×32×32 (3,072 values)\nEarly layers: 64×16×16 (16,384 values)\nMiddle layers: 128×8×8 (8,192 values)\nDeep layers: 256×4×4 (4,096 values)\n\n\n\n\nBatch normalization is essential for training deep CNNs, helping to address vanishing gradients. For each channel in the feature maps:\n\\[\\mu_c = \\frac{1}{N \\times H \\times W} \\sum_{i,j,k} x_{i,j,k}\\]\n\\[\\sigma^2_c = \\frac{1}{N \\times H \\times W} \\sum_{i,j,k} (x_{i,j,k} - \\mu_c)^2\\]\n\\[\\hat{x}_{i,j,k} = \\frac{x_{i,j,k} - \\mu_c}{\\sqrt{\\sigma^2_c + \\epsilon}}\\]\n\\[y_{i,j,k} = \\gamma_c \\hat{x}_{i,j,k} + \\delta_c\\]\nWhere:\n\n\\(\\gamma_c\\) and \\(\\delta_c\\) are learnable parameters\nNormalization layers typically follow convolution layers\nThis process stabilizes and accelerates training"
  },
  {
    "objectID": "lectures/summaries/Lecture13Summary.html#advanced-cnn-architectures",
    "href": "lectures/summaries/Lecture13Summary.html#advanced-cnn-architectures",
    "title": "Lecture 13: Advanced CNN Architectures and Transfer Learning",
    "section": "Advanced CNN Architectures",
    "text": "Advanced CNN Architectures\nAs image recognition tasks became increasingly complex, CNN architectures evolved significantly to address various challenges.\n\nVGG Architecture\nThe VGG architecture introduced systematic design principles:\n\nUse consistent 3×3 convolutions with stride 1\nApply 2×2 max pooling with stride 2\nDouble the number of filters after each pooling operation\n\nThis systematic approach made networks deeper while maintaining computational efficiency, with improved performance over earlier architectures like AlexNet.\n\n\nAddressing Overfitting\nTwo primary techniques help CNNs generalize better:\n\nDropout: Randomly deactivating connections during training:\n\nForces the network to learn redundant representations\nCreates an implicit ensemble of networks\nSignificantly improves generalization performance\n\nData Augmentation: Systematically creating variations of training images:\n\nFlipping (horizontal/vertical)\nRotation and scaling\nColor jittering\nRandom cropping\n\nThese transformations help the model learn invariant features and generalize to unseen images.\n\n\n\nResidual Networks (ResNets)\nA breakthrough in deep CNN architecture, ResNets introduced skip connections that bypass layers:\n\\[H(x) = F(x) + x\\]\nThis simple modification has profound effects on gradient flow:\nTraditional CNN gradient: \\[\\frac{\\partial \\mathcal{L}}{\\partial x} = \\frac{\\partial \\mathcal{L}}{\\partial x_k} \\cdot \\prod_{i=1}^{k} \\left( \\phi_i'(z_i) \\, W_i \\right)\\]\nResNet gradient: \\[\\frac{\\partial \\mathcal{L}}{\\partial x} = \\frac{\\partial \\mathcal{L}}{\\partial x_n} \\cdot \\prod_{i=1}^{k} \\left( \\mathcal{I} + J_{F_i}(x_i) \\right)\\]\nThe identity matrix term prevents gradient vanishing, allowing for much deeper networks:\n\nEven if \\(J_{F_i}(x_i)\\) has small eigenvalues, gradients can still flow\nEach residual block learns an incremental transformation\nNetworks can reach hundreds of layers while still training effectively\n\nResNets also employ global average pooling instead of fully connected layers, where each feature map is reduced to a single value by averaging, significantly reducing parameters while maintaining performance."
  },
  {
    "objectID": "lectures/summaries/Lecture13Summary.html#visualizing-cnn-features",
    "href": "lectures/summaries/Lecture13Summary.html#visualizing-cnn-features",
    "title": "Lecture 13: Advanced CNN Architectures and Transfer Learning",
    "section": "Visualizing CNN Features",
    "text": "Visualizing CNN Features\nUnderstanding what features CNNs extract helps interpret their behavior:\n\nExemplar Approach: Finding images that maximally activate specific filters\n\nCompute activations for all images at a given filter\nIdentify images with the highest activation values\nLook for common patterns across these high-activation images\n\nActivation Maximization: Synthesizing images that maximize specific activations\n\nStart with random noise\nIteratively modify the image to increase activation at a target filter\nAscend the gradient to visualize what patterns the filter detects"
  },
  {
    "objectID": "lectures/summaries/Lecture13Summary.html#transfer-learning-for-cnns",
    "href": "lectures/summaries/Lecture13Summary.html#transfer-learning-for-cnns",
    "title": "Lecture 13: Advanced CNN Architectures and Transfer Learning",
    "section": "Transfer Learning for CNNs",
    "text": "Transfer Learning for CNNs\nTransfer learning has become the standard approach for applying CNNs to new image tasks. This approach leverages pre-trained feature extractors:\n\nMotivation\n\nCNNs trained on ImageNet (14+ million images, 20,000+ categories) learn general-purpose visual features\nLower layers detect universal patterns (edges, textures, simple shapes)\nThese features are transferable across most visual tasks\nTraining such networks requires enormous computational resources\n\n\n\nImplementation Process\n\nFeature Extraction:\n\nTake a pre-trained CNN (e.g., ResNet-50)\nRemove the final classification layer\nPass your images through this truncated network\nExtract the feature maps from the last layer\n\nNew Classifier Training:\n\nUse these extracted features as inputs to a new classifier\nTrain only this new classifier on your specific task\nOptionally fine-tune some of the later layers of the pre-trained network\n\n\n\n\nAdvantages\n\nDramatically reduces training time and computational requirements\nImproves performance on small datasets\nLeverages knowledge from millions of images\nEnables state-of-the-art performance with modest resources\n\nTransfer learning has democratized advanced computer vision, making sophisticated image analysis accessible without requiring massive computational resources or enormous labeled datasets."
  },
  {
    "objectID": "lectures/summaries/Lecture15Summary.html",
    "href": "lectures/summaries/Lecture15Summary.html",
    "title": "Lecture 15: Semantic Segmentation and Advanced CNN Applications",
    "section": "",
    "text": "Semantic segmentation represents a pixel-level classification task in computer vision, where each pixel in an image is assigned a semantic class label.\n\n\nUnlike object detection that provides bounding boxes around objects, semantic segmentation:\n\nClassifies every pixel in the image\nDoes not differentiate between instances of the same class (all cows are labeled as “cow”)\nIncludes both foreground objects and background elements\n\nThe output is a label map of the same dimensions as the input image, where each pixel value corresponds to a class index.\n\n\n\nA straightforward approach might attempt to:\n\nApply a standard CNN backbone\nUse fully connected layers to classify each pixel independently\n\nThis approach presents critical limitations:\n\nComputationally expensive when processing high-resolution images\nFails to effectively utilize spatial context and neighborhood relationships\nLoses localization information during downsampling"
  },
  {
    "objectID": "lectures/summaries/Lecture15Summary.html#semantic-segmentation",
    "href": "lectures/summaries/Lecture15Summary.html#semantic-segmentation",
    "title": "Lecture 15: Semantic Segmentation and Advanced CNN Applications",
    "section": "",
    "text": "Semantic segmentation represents a pixel-level classification task in computer vision, where each pixel in an image is assigned a semantic class label.\n\n\nUnlike object detection that provides bounding boxes around objects, semantic segmentation:\n\nClassifies every pixel in the image\nDoes not differentiate between instances of the same class (all cows are labeled as “cow”)\nIncludes both foreground objects and background elements\n\nThe output is a label map of the same dimensions as the input image, where each pixel value corresponds to a class index.\n\n\n\nA straightforward approach might attempt to:\n\nApply a standard CNN backbone\nUse fully connected layers to classify each pixel independently\n\nThis approach presents critical limitations:\n\nComputationally expensive when processing high-resolution images\nFails to effectively utilize spatial context and neighborhood relationships\nLoses localization information during downsampling"
  },
  {
    "objectID": "lectures/summaries/Lecture15Summary.html#u-net-architecture",
    "href": "lectures/summaries/Lecture15Summary.html#u-net-architecture",
    "title": "Lecture 15: Semantic Segmentation and Advanced CNN Applications",
    "section": "U-Net Architecture",
    "text": "U-Net Architecture\nU-Net represents a specialized encoder-decoder architecture that addresses the challenges of semantic segmentation.\n\nEncoder-Decoder Framework\nThe U-Net architecture consists of two primary components:\nEncoder Path (Contracting):\n\nProgressively reduces spatial dimensions through downsampling\nIncreases feature channels to capture abstract representations\nFollows traditional CNN pattern of convolution + pooling\n\nDecoder Path (Expanding):\n\nProgressively increases spatial dimensions through upsampling\nDecreases feature channels to approach segmentation map\nUses transposed convolutions to recover spatial information\n\n\n\nSkip Connections\nThe defining feature of U-Net is its skip connections that:\n\nConnect corresponding layers between encoder and decoder paths\nPreserve high-resolution spatial information lost during downsampling\nConcatenate feature maps from encoding path to decoding path\nForm the characteristic “U” shape in the architecture diagram\n\n\n\nTransposed Convolution for Upsampling\nTransposed convolution (sometimes called deconvolution) enables learnable upsampling:\n\\[\\mathbf{X} \\circledast^{-1} \\mathbf{K} \\rightarrow \\text{larger output}\\]\nKey properties include:\n\nInverse operation to standard convolution with stride &gt; 1\nExpands input dimensions rather than contracting them\nWeights are learned during training, unlike fixed interpolation methods\nStride in transposed convolution refers to output matrix spacing\n\nConceptually, transposed convolution can be viewed as a learnable interpolation method that reconstructs higher-resolution features from compressed representations.\n\n\nTraining and Loss Functions\nU-Net training typically employs:\n\nPixel-wise cross-entropy loss for classification accuracy\nIntersection over Union (IoU) metrics for evaluation: \\[IoU = \\frac{\\text{True Positives}}{\\text{True Positives + False Positives + False Negatives}}\\]"
  },
  {
    "objectID": "lectures/summaries/Lecture15Summary.html#instance-segmentation",
    "href": "lectures/summaries/Lecture15Summary.html#instance-segmentation",
    "title": "Lecture 15: Semantic Segmentation and Advanced CNN Applications",
    "section": "Instance Segmentation",
    "text": "Instance Segmentation\nInstance segmentation extends semantic segmentation by distinguishing individual instances of objects.\n\nTask Definition\nFor each object instance in the image:\n\nIdentify the class label (as in object detection)\nGenerate a pixel-perfect mask (as in semantic segmentation)\nAssign a unique identifier to each instance\n\nUnlike semantic segmentation, instance segmentation:\n\nDifferentiates between multiple instances of the same class\nFocuses primarily on foreground objects rather than background\nRequires both classification and boundary delineation\n\n\n\nMask R-CNN\nMask R-CNN extends Faster R-CNN by adding a branch for predicting segmentation masks:\n\nUses a standard object detection backbone to identify regions of interest\nFor each region, generates both class predictions and bounding boxes\nAdds a parallel branch that predicts a binary mask for each detected object\n\n\n\nSegment Anything Model (SAM)\nMeta’s Segment Anything Model (SAM) represents a recent advancement in instance segmentation:\n\nTrained on an unprecedented dataset of 11 million images with 1.1 billion masks\nFunctions as a foundation model for segmentation tasks\nCan identify object “blobs” without specific class labels\nSupports interactive prompting (e.g., point-based queries)\nDesigned for zero-shot transfer to new segmentation tasks\n\nSAM demonstrates how large-scale pretraining can create universal segmentation capabilities across diverse visual domains."
  },
  {
    "objectID": "lectures/summaries/Lecture15Summary.html#generative-applications-of-cnns",
    "href": "lectures/summaries/Lecture15Summary.html#generative-applications-of-cnns",
    "title": "Lecture 15: Semantic Segmentation and Advanced CNN Applications",
    "section": "Generative Applications of CNNs",
    "text": "Generative Applications of CNNs\nCNNs can be inverted to generate images rather than classify them, shifting from discriminative models \\(P(y|x)\\) to generative models \\(P(x|y)\\).\n\nBayesian Framework\nThis inversion uses Bayes’ theorem:\n\\[P(x|y=c) = \\frac{1}{Z}P(y=c|x)P(x)\\]\nWhere:\n\n\\(P(y=c|x)\\) is provided by the CNN classifier (likelihood)\n\\(P(x)\\) is an image prior\n\\(Z\\) is a normalizing constant\n\n\n\nLangevin Dynamics for Image Generation\nThe unadjusted Langevin algorithm enables sampling from the posterior by iteratively updating:\n\\[x_{t+1} = x_t + \\epsilon_1 \\frac{\\partial \\log P(x_t)}{\\partial x_t} + \\epsilon_2 \\frac{\\partial \\log P(y=c|x_t)}{\\partial x_t}\\]\nKey components include:\n\nGradient of the CNN classifier with respect to input\nGradient of the image prior\nStep sizes \\(\\epsilon_1\\) and \\(\\epsilon_2\\) to balance these influences\n\n\n\nTotal Variation Prior\nA common differentiable image prior is the total variation (TV) prior:\n\\[TV(x) = \\sum_{i,j,k} (x_{i,j,k} - x_{i+1,j,k})^2 + (x_{i,j,k} - x_{i,j+1,k})^2\\]\nThis prior:\n\nEncourages smoothness between adjacent pixels\nPenalizes sharp transitions and noise\nBalances the classifier’s tendency to generate exaggerated features\n\n\n\nCreative Applications\nWhile computationally intensive, this approach has enabled creative applications:\n\nDeepDream: Amplifying patterns recognized by CNN layers\nNeural Style Transfer: Combining content from one image with style from another\n\nThese techniques demonstrate how CNNs can not only analyze visual data but also generate novel visual content by leveraging learned representations."
  },
  {
    "objectID": "lectures/summaries/Lecture17Summary.html",
    "href": "lectures/summaries/Lecture17Summary.html",
    "title": "Lecture 17: Sequence-to-Sequence Models and Attention Mechanisms",
    "section": "",
    "text": "Sequence-to-sequence (Seq2Seq) models represent a powerful framework for transforming one sequence into another, with applications across numerous domains.\n\n\nThe standard Seq2Seq architecture consists of two primary components:\n\nEncoder: Processes the input sequence and compresses its information into context vectors\n\nTypically implemented as an RNN (often LSTM) that reads the input sequence\nProduces hidden states at each step, with the final hidden state serving as the “context” vector\n\nDecoder: Generates the output sequence based on the encoded information\n\nAlso implemented as an RNN that generates one output element at a time\nTakes the previous output element and previous hidden state as inputs\nInitialized with the final encoder hidden state (context vector)\n\n\nThis encoder-decoder framework provides a flexible approach for mapping arbitrary-length input sequences to arbitrary-length output sequences.\n\n\n\nDespite its elegance, the standard Seq2Seq architecture suffers from a fundamental limitation:\n\nAll information from the input sequence must be compressed into a fixed-length context vector\nFor long sequences, this creates an information bottleneck\nThe model struggles to retain details from early parts of the input sequence\nPerformance degrades as sequence length increases\n\nFor example, when translating a long sentence, information about the first few words may be lost by the time the encoder processes the entire input."
  },
  {
    "objectID": "lectures/summaries/Lecture17Summary.html#sequence-to-sequence-modeling",
    "href": "lectures/summaries/Lecture17Summary.html#sequence-to-sequence-modeling",
    "title": "Lecture 17: Sequence-to-Sequence Models and Attention Mechanisms",
    "section": "",
    "text": "Sequence-to-sequence (Seq2Seq) models represent a powerful framework for transforming one sequence into another, with applications across numerous domains.\n\n\nThe standard Seq2Seq architecture consists of two primary components:\n\nEncoder: Processes the input sequence and compresses its information into context vectors\n\nTypically implemented as an RNN (often LSTM) that reads the input sequence\nProduces hidden states at each step, with the final hidden state serving as the “context” vector\n\nDecoder: Generates the output sequence based on the encoded information\n\nAlso implemented as an RNN that generates one output element at a time\nTakes the previous output element and previous hidden state as inputs\nInitialized with the final encoder hidden state (context vector)\n\n\nThis encoder-decoder framework provides a flexible approach for mapping arbitrary-length input sequences to arbitrary-length output sequences.\n\n\n\nDespite its elegance, the standard Seq2Seq architecture suffers from a fundamental limitation:\n\nAll information from the input sequence must be compressed into a fixed-length context vector\nFor long sequences, this creates an information bottleneck\nThe model struggles to retain details from early parts of the input sequence\nPerformance degrades as sequence length increases\n\nFor example, when translating a long sentence, information about the first few words may be lost by the time the encoder processes the entire input."
  },
  {
    "objectID": "lectures/summaries/Lecture17Summary.html#attention-mechanisms",
    "href": "lectures/summaries/Lecture17Summary.html#attention-mechanisms",
    "title": "Lecture 17: Sequence-to-Sequence Models and Attention Mechanisms",
    "section": "Attention Mechanisms",
    "text": "Attention Mechanisms\nAttention mechanisms address the bottleneck problem by allowing the decoder to focus on different parts of the input sequence at each decoding step.\n\nIntuition and Motivation\nAttention draws inspiration from human cognition:\n\nWhen translating a sentence, humans don’t process the entire source sentence at once\nInstead, they focus on specific words or phrases relevant to the current word being translated\nAttention allows neural networks to implement a similar focusing mechanism\n\n\n\nAttention in Seq2Seq Models\nIn attention-augmented Seq2Seq models:\n\nThe encoder still processes the entire input sequence\nHowever, instead of using only the final hidden state, all encoder hidden states are preserved\nAt each decoding step, the decoder computes a weighted sum of these encoder states\nThe weights determine how much “attention” to pay to each input position\n\nThis creates direct pathways between each decoder step and all encoder positions, effectively bypassing the bottleneck.\n\n\nComputing Attention Weights\nAttention scores between a decoder state and encoder states are computed through various mechanisms:\n\nBahdanau Attention (Additive Attention): \\[e_{ti} = \\mathbf{w}_2^T \\odot \\tanh(\\mathbf{W}_1 [\\mathbf{h}_i, \\mathbf{s}_{t-1}])\\]\n\nConcatenates encoder and decoder states\nPasses them through a small neural network\nLearns alignment through trainable parameters \\(\\mathbf{W}_1\\) and \\(\\mathbf{w}_2\\)\n\nScaled Dot-Product Attention: \\[e_{ti} = \\frac{\\mathbf{h}_i^T \\mathbf{W}_g \\mathbf{s}_{t-1}}{\\sqrt{m}}\\]\n\nComputes similarity through dot product of transformed vectors\nScaling by \\(\\sqrt{m}\\) prevents dot products from growing too large in magnitude\nMore computationally efficient than additive attention\n\n\nThe attention weights are then normalized using softmax: \\[a_{ti} = \\frac{\\exp(e_{ti})}{\\sum_j \\exp(e_{tj})}\\]\n\n\nContext Vector Computation\nThe context vector for each decoder step is computed as a weighted sum of encoder hidden states:\n\\[\\mathbf{c}_t = \\sum_{i=1}^{T_e} a_{ti} \\mathbf{h}_i\\]\nThis context vector is then used as an additional input to the decoder:\n\\[\\mathbf{s}_t = f(\\mathbf{y}_{t-1}, \\mathbf{s}_{t-1}, \\mathbf{c}_t)\\] \\[\\mathbf{y}_t = g(\\mathbf{s}_t, \\mathbf{y}_{t-1}, \\mathbf{c}_t)\\]\nWhere \\(f\\) and \\(g\\) are the state update and output functions, respectively.\n\n\nVisualizing Attention\nAttention weights can be visualized as a matrix where each cell represents how much the decoder focuses on a particular encoder position when generating a specific output:\n\nRows correspond to output positions\nColumns correspond to input positions\nBrighter cells indicate higher attention weights\n\nIn language translation, attention weights often reveal alignment between source and target words, capturing:\n\nWord reordering across languages\nOne-to-many and many-to-one word correspondences\nRelevant contextual information beyond direct translations"
  },
  {
    "objectID": "lectures/summaries/Lecture17Summary.html#query-key-value-framework",
    "href": "lectures/summaries/Lecture17Summary.html#query-key-value-framework",
    "title": "Lecture 17: Sequence-to-Sequence Models and Attention Mechanisms",
    "section": "Query-Key-Value Framework",
    "text": "Query-Key-Value Framework\nThe attention mechanism can be generalized using the Query-Key-Value (QKV) framework, which provides a more flexible way to think about attention.\n\nCore Components\n\nQuery (Q): The vector representing the current decoder state\nKey (K): The vectors against which the query is compared (encoder states)\nValue (V): The vectors that are weighted to produce the context (typically the same as keys)\n\n\n\nAttention Layer Computation\nThe attention layer performs the following operations:\n\nTransform input vectors into keys and values: \\[\\mathbf{K} = \\mathbf{X}\\mathbf{W}_K\\] \\[\\mathbf{V} = \\mathbf{X}\\mathbf{W}_V\\]\nCompute similarity scores between queries and keys: \\[\\mathbf{E} = \\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{m}}\\]\nNormalize scores to obtain attention weights: \\[\\mathbf{A} = \\text{softmax}(\\mathbf{E})\\]\nCompute weighted values: \\[\\mathbf{Y} = \\mathbf{A}\\mathbf{V}\\]\n\nThis framework generalizes attention beyond the encoder-decoder context, allowing for applications in various network architectures."
  },
  {
    "objectID": "lectures/summaries/Lecture17Summary.html#beyond-sequential-processing",
    "href": "lectures/summaries/Lecture17Summary.html#beyond-sequential-processing",
    "title": "Lecture 17: Sequence-to-Sequence Models and Attention Mechanisms",
    "section": "Beyond Sequential Processing",
    "text": "Beyond Sequential Processing\nThe QKV framework suggests that attention can operate without requiring sequential processing of inputs:\n\nTraditional RNNs must process inputs sequentially due to their recurrent nature\nAttention can theoretically compute weights for all positions simultaneously\nHowever, standard attention still lacks positional information\n\nThis insight leads to several questions:\n\nCan we replace sequential encoding entirely with attention-based mechanisms?\nHow do we incorporate positional information without sequential processing?\nCan attention look at relationships between input elements themselves?\n\nThese questions set the stage for self-attention mechanisms and transformer architectures, where attention becomes the primary computational mechanism rather than an augmentation of recurrent networks."
  },
  {
    "objectID": "lectures/summaries/Lecture19Summary.html",
    "href": "lectures/summaries/Lecture19Summary.html",
    "title": "Lecture 19: Representation Learning, Vision Transformers, and Autoregressive Generation",
    "section": "",
    "text": "While generative transformer models like GPT focus on predicting the next token in a sequence, representation learning approaches the problem from a fundamentally different perspective.\n\n\nRepresentation learning models like BERT (Bidirectional Encoder Representations from Transformers) aim to develop rich contextual understanding of inputs rather than generating continuations:\n\nFocus on capturing the meaning and relationships within data\nLeverage bidirectional context rather than unidirectional prediction\nCreate versatile representations that can be applied to multiple downstream tasks\nEmphasize understanding over generation\n\nThis approach is particularly valuable for tasks where complete context is available at inference time, such as text classification, entity recognition, or question answering.\n\n\n\nBERT uses the encoder portion of the transformer architecture with several key modifications:\n\nInput Structure: BERT uses special tokens to structure inputs:\n\n&lt;CLS&gt; token at the beginning, which aggregates sequence-level representations\n&lt;SEP&gt; tokens to separate different sentences within an input\n&lt;MASK&gt; tokens that replace words to be predicted during training\n\nPre-training Objectives: BERT is trained with two simultaneous tasks:\n\nMasked Language Modeling (MLM): Randomly mask 15% of input tokens and train the model to predict them based on bidirectional context\nNext Sentence Prediction (NSP): Predict whether two sentences appear consecutively in text\n\nInput Representation: Each token receives three types of embeddings:\n\nToken embeddings representing the word identity\nSegment embeddings indicating which sentence the token belongs to\nPosition embeddings encoding sequential position\n\nModel Variants:\n\nBERT-base: 12 layers, 768-dimensional hidden states, 12 attention heads (110M parameters)\nBERT-large: 24 layers, 1024-dimensional hidden states, 16 attention heads (340M parameters)\n\n\n\n\n\nBERT’s power stems from its ability to transfer contextual language understanding to various downstream tasks:\n\nPre-train on massive text corpora (Wikipedia + BooksCorpus)\nFine-tune for specific tasks by:\n\nAdding task-specific layers on top of the pre-trained representations\nUpdating all parameters end-to-end with a small learning rate\nAchieving state-of-the-art performance with minimal task-specific data\n\n\nThis transfer learning approach dramatically reduces the computational resources needed for specific NLP tasks, as the general language understanding is already encoded in the pre-trained model."
  },
  {
    "objectID": "lectures/summaries/Lecture19Summary.html#representation-learning-with-bert",
    "href": "lectures/summaries/Lecture19Summary.html#representation-learning-with-bert",
    "title": "Lecture 19: Representation Learning, Vision Transformers, and Autoregressive Generation",
    "section": "",
    "text": "While generative transformer models like GPT focus on predicting the next token in a sequence, representation learning approaches the problem from a fundamentally different perspective.\n\n\nRepresentation learning models like BERT (Bidirectional Encoder Representations from Transformers) aim to develop rich contextual understanding of inputs rather than generating continuations:\n\nFocus on capturing the meaning and relationships within data\nLeverage bidirectional context rather than unidirectional prediction\nCreate versatile representations that can be applied to multiple downstream tasks\nEmphasize understanding over generation\n\nThis approach is particularly valuable for tasks where complete context is available at inference time, such as text classification, entity recognition, or question answering.\n\n\n\nBERT uses the encoder portion of the transformer architecture with several key modifications:\n\nInput Structure: BERT uses special tokens to structure inputs:\n\n&lt;CLS&gt; token at the beginning, which aggregates sequence-level representations\n&lt;SEP&gt; tokens to separate different sentences within an input\n&lt;MASK&gt; tokens that replace words to be predicted during training\n\nPre-training Objectives: BERT is trained with two simultaneous tasks:\n\nMasked Language Modeling (MLM): Randomly mask 15% of input tokens and train the model to predict them based on bidirectional context\nNext Sentence Prediction (NSP): Predict whether two sentences appear consecutively in text\n\nInput Representation: Each token receives three types of embeddings:\n\nToken embeddings representing the word identity\nSegment embeddings indicating which sentence the token belongs to\nPosition embeddings encoding sequential position\n\nModel Variants:\n\nBERT-base: 12 layers, 768-dimensional hidden states, 12 attention heads (110M parameters)\nBERT-large: 24 layers, 1024-dimensional hidden states, 16 attention heads (340M parameters)\n\n\n\n\n\nBERT’s power stems from its ability to transfer contextual language understanding to various downstream tasks:\n\nPre-train on massive text corpora (Wikipedia + BooksCorpus)\nFine-tune for specific tasks by:\n\nAdding task-specific layers on top of the pre-trained representations\nUpdating all parameters end-to-end with a small learning rate\nAchieving state-of-the-art performance with minimal task-specific data\n\n\nThis transfer learning approach dramatically reduces the computational resources needed for specific NLP tasks, as the general language understanding is already encoded in the pre-trained model."
  },
  {
    "objectID": "lectures/summaries/Lecture19Summary.html#vision-transformers",
    "href": "lectures/summaries/Lecture19Summary.html#vision-transformers",
    "title": "Lecture 19: Representation Learning, Vision Transformers, and Autoregressive Generation",
    "section": "Vision Transformers",
    "text": "Vision Transformers\nThe success of transformers in NLP has inspired their application to computer vision, challenging the dominance of convolutional neural networks.\n\nAdapting Transformers for Images\nVision Transformers (ViT) apply self-attention mechanisms to image data, but face a fundamental challenge: images contain significantly more “tokens” than text, with each pixel potentially representing an individual token.\nThe key innovation in ViT addresses this challenge through:\n\nPatch-based Tokenization: Instead of treating individual pixels as tokens:\n\nDivide the image into fixed-size patches (typically 16×16 pixels)\nFlatten each patch into a vector\nProject these vectors to the transformer’s working dimension\n\nClass Token: Similar to BERT’s &lt;CLS&gt; token, a learnable embedding is prepended to the sequence of patch embeddings to aggregate information for classification.\nPosition Embeddings: Learnable position embeddings are added to patch embeddings to retain spatial information lost in the flattening process.\n\n\n\nViT Architecture\nThe ViT architecture closely follows the transformer encoder:\n\nInput image is divided into patches and linearly embedded\nPosition embeddings are added to patch embeddings\nThe resulting sequence is processed through multiple transformer encoder blocks:\n\nMulti-head self-attention\nMLP blocks\nLayer normalization and residual connections\n\nThe final representation of the class token is fed into a classification head\n\n\n\nPerformance and Scaling Properties\nVision Transformers demonstrate several important properties:\n\nThey perform competitively with CNNs only when trained on large datasets\nThey scale very efficiently with more data and model size\nThey require fewer computational resources at comparable performance levels\nThey lack the inductive biases of CNNs (locality, translation equivariance)\nThey can capture long-range dependencies more efficiently than CNNs\n\nRecent developments like Swin Transformers combine the strengths of both approaches by incorporating local attention windows that progressively merge, similar to how CNNs hierarchically process visual information."
  },
  {
    "objectID": "lectures/summaries/Lecture19Summary.html#autoregressive-generation",
    "href": "lectures/summaries/Lecture19Summary.html#autoregressive-generation",
    "title": "Lecture 19: Representation Learning, Vision Transformers, and Autoregressive Generation",
    "section": "Autoregressive Generation",
    "text": "Autoregressive Generation\nAutoregressive generation represents a powerful approach to modeling complex data distributions by factorizing them as products of conditional probabilities.\n\nPrinciples of Autoregressive Modeling\nThe core insight of autoregressive models stems from the chain rule of probability:\n\\[P(\\mathbf{x}) = P(x_1) \\cdot P(x_2|x_1) \\cdot P(x_3|x_1,x_2) \\cdot ... \\cdot P(x_n|x_1,...,x_{n-1})\\]\nThis allows modeling a joint distribution as a sequence of conditional distributions, where each element depends on all previous elements. For generation:\n\nSample first element from \\(P(x_1)\\)\nSample second element from \\(P(x_2|x_1)\\)\nContinue until the entire sequence is generated\n\nThis approach provides explicit likelihood modeling and allows direct sampling from the learned distribution.\n\n\nLanguage Generation with GPT\nGPT (Generative Pre-trained Transformer) implements autoregressive modeling for text:\n\nUses masked self-attention to prevent looking at future tokens\nPredicts probability distribution over next token given all previous tokens\nScales effectively with more data and parameters\nCan be primed with initial text to generate contextually relevant continuations\n\nGPT’s masked self-attention mechanism is perfectly suited for autoregressive modeling, as it naturally implements the conditional probability structure required.\n\n\nImage Generation with PixelRNN/PixelCNN\nAutoregressive models can also generate images by treating them as sequences of pixels:\n\nSequence Definition: Define an ordering of pixels (typically row by row, from top-left)\nContext Modeling: Model each pixel as dependent on all previous pixels\nNetwork Architecture: Use RNNs (PixelRNN) or masked convolutions (PixelCNN) to capture dependencies\n\nPixelRNN specifically uses LSTM units to model the conditional distribution of each pixel:\n\\[\\mathbf{h}_{x,y} = f(\\mathbf{h}_{x-1,y}, \\mathbf{h}_{x,y-1})\\] \\[P(\\mathbf{x}_{x,y}|\\mathbf{x}_{&lt;(x,y)}) = g(\\mathbf{h}_{x,y})\\]\nWhere \\(\\mathbf{h}_{x,y}\\) is the hidden state for position \\((x,y)\\) and \\(g\\) is a function that outputs a distribution over pixel values.\n\n\nLimitations of Autoregressive Generation\nWhile theoretically elegant, autoregressive models face practical challenges:\n\nSequential Generation: Generating samples is inherently sequential and cannot be parallelized\nComputational Complexity: For high-dimensional data like images, generation becomes extremely slow\nLimited Resolution: Practical applications for images are often limited to small resolutions (e.g., 64×64)\nLong-range Dependencies: Capturing dependencies between distant elements remains challenging\n\nThese limitations have motivated alternative approaches to generative modeling, such as variational autoencoders and generative adversarial networks, which offer different tradeoffs between likelihood modeling, sampling efficiency, and generation quality."
  },
  {
    "objectID": "lectures/summaries/Lecture21Summary.html",
    "href": "lectures/summaries/Lecture21Summary.html",
    "title": "Lecture 21: Autoencoders and Generation",
    "section": "",
    "text": "Density estimation: Determine the probability \\(P(\\mathbf{x})\\) of observing data points\nSampling: Generate novel data from the model distribution\nRepresentation: Learn meaningful feature representations and manipulate specific features\n\n\n\n\n\nBased on the probability chain rule: \\(f(x_1,x_2,x_3,...,x_P) = f(x_1)f(x_2 | x_1)f(x_3 | x_1,x_2)...f(x_P | x_{P-1}, x_{P-2},...)\\)\nImplementation approaches:\n\nPixelCNN: Uses CNN-style windowing instead of recurrence\nImageGPT: Uses masked self-attention instead of recurrence\n\nLimitations:\n\nExtremely slow for high-dimensional data like images\nLimited practical resolution (e.g., 64×64 for ImageGPT)\nLacks effective feature representation"
  },
  {
    "objectID": "lectures/summaries/Lecture21Summary.html#generative-models",
    "href": "lectures/summaries/Lecture21Summary.html#generative-models",
    "title": "Lecture 21: Autoencoders and Generation",
    "section": "",
    "text": "Density estimation: Determine the probability \\(P(\\mathbf{x})\\) of observing data points\nSampling: Generate novel data from the model distribution\nRepresentation: Learn meaningful feature representations and manipulate specific features\n\n\n\n\n\nBased on the probability chain rule: \\(f(x_1,x_2,x_3,...,x_P) = f(x_1)f(x_2 | x_1)f(x_3 | x_1,x_2)...f(x_P | x_{P-1}, x_{P-2},...)\\)\nImplementation approaches:\n\nPixelCNN: Uses CNN-style windowing instead of recurrence\nImageGPT: Uses masked self-attention instead of recurrence\n\nLimitations:\n\nExtremely slow for high-dimensional data like images\nLimited practical resolution (e.g., 64×64 for ImageGPT)\nLacks effective feature representation"
  },
  {
    "objectID": "lectures/summaries/Lecture21Summary.html#dimensionality-reduction-and-autoencoders",
    "href": "lectures/summaries/Lecture21Summary.html#dimensionality-reduction-and-autoencoders",
    "title": "Lecture 21: Autoencoders and Generation",
    "section": "Dimensionality Reduction and Autoencoders",
    "text": "Dimensionality Reduction and Autoencoders\n\nPrincipal Component Analysis (PCA)\n\nFinds a low-dimensional representation that minimizes reconstruction error: \\(\\frac{1}{N} \\sum \\limits_{i = 1}^N \\| \\mathbf{x}_i - d(e(\\mathbf{x}_i))\\|^2_2\\)\nFunctions:\n\n\\(e(\\mathbf{x}_i) = \\mathbf{z}_i\\) is the encoder mapping input to latent space\n\\(d(\\mathbf{z}_i)\\) is the decoder mapping latent variables back to original feature space\n\nFormulation with weight matrix \\(\\mathbf{W}\\): \\(\\frac{1}{N} \\sum \\limits_{i = 1}^N \\| \\mathbf{x}_i - \\mathbf{W} \\mathbf{W}^T \\mathbf{x}_i\\|^2_2\\)\nConstraints:\n\nColumns of \\(\\mathbf{W}\\) must be orthogonal\nColumns of \\(\\mathbf{W}\\) must be normalized\n\nImplementation using eigendecomposition:\n\n\\(\\mathbf{X}^T \\mathbf{X} = \\mathbf{Q} \\mathbf{D} \\mathbf{Q}^{-1}\\)\n\\(\\mathbf{W} = \\mathbf{Q}_{1:K}\\) (first \\(K\\) eigenvectors)\n\n\n\n\nDeterministic Autoencoders\n\nGeneralization of PCA using neural networks\nArchitecture:\n\nEncoder network maps inputs to lower-dimensional latent space\nBottleneck forces the model to learn efficient representations\nDecoder network reconstructs original input from latent representation\n\nAdvantages over PCA:\n\nCan capture non-linear relationships\nCNN backbones help induce structure in recovered images\nPerformance strictly dominates PCA for image data\n\nApplications:\n\nDimensionality reduction\nFeature extraction for downstream tasks\nExamination of feature maps to understand data variation\n\nLimitations as generative models:\n\nNo probabilistic formulation (no \\(P(\\mathbf{X})\\))\nCannot properly sample from gaps between training examples\nNo density estimation capability"
  },
  {
    "objectID": "lectures/summaries/Lecture21Summary.html#generative-probabilistic-models",
    "href": "lectures/summaries/Lecture21Summary.html#generative-probabilistic-models",
    "title": "Lecture 21: Autoencoders and Generation",
    "section": "Generative Probabilistic Models",
    "text": "Generative Probabilistic Models\n\nFactor Analysis\n\nProbabilistic version of PCA with a defined prior distribution\nGaussian factor analysis:\n\nPrior: \\(f(\\mathbf{z}) = \\mathcal{N}_K(\\mathbf{z} | \\mathbf{0}, \\mathcal{I}_K)\\)\nLikelihood: \\(f(\\mathbf{x} | \\mathbf{z}, \\boldsymbol{\\Theta}) = \\mathcal{N}_P(\\mathbf{x} | \\mathbf{W} \\mathbf{z} + \\boldsymbol{\\alpha}, \\boldsymbol{\\Psi})\\)\n\n\n\n\nProperties of Generative Models\n\nMust provide a probability density \\(P(\\mathbf{x}|\\boldsymbol{\\Theta})\\) for any viable input \\(\\mathbf{x}\\)\nComparison with discriminative models:\n\nDiscriminative models (e.g., logistic regression): \\(P(y=1|\\mathbf{x},\\hat{\\boldsymbol{\\beta}}) = \\sigma(\\mathbf{x}^T \\hat{\\boldsymbol{\\beta}})\\)\nGenerative models (e.g., QDA): \\(\\mathbf{x}|y=c \\sim \\mathcal{N}_P(\\mathbf{x}|\\boldsymbol{\\mu}_c, \\boldsymbol{\\Sigma}_c)\\)\n\nTrade-off: Generative power comes at the cost of making assumptions about data structure"
  },
  {
    "objectID": "lectures/summaries/Lecture21Summary.html#bayesian-methods",
    "href": "lectures/summaries/Lecture21Summary.html#bayesian-methods",
    "title": "Lecture 21: Autoencoders and Generation",
    "section": "Bayesian Methods",
    "text": "Bayesian Methods\n\nBayesian Framework\n\nUpdate prior beliefs based on observed data using Bayes’ theorem: \\(f(\\boldsymbol{\\theta} | \\mathcal{D}) = \\frac{f(\\mathcal{D} | \\boldsymbol{\\theta})f(\\boldsymbol{\\theta})}{f(\\mathcal{D})}\\)\nComponents:\n\nLikelihood: \\(f(\\mathcal{D} | \\boldsymbol{\\theta})\\) - probability of observing data given parameters\nPrior: \\(f(\\boldsymbol{\\theta})\\) - beliefs about parameters before observing data\nMarginal likelihood: \\(f(\\mathcal{D}) = \\int f(\\mathcal{D} | \\boldsymbol{\\theta})f(\\boldsymbol{\\theta}) d\\theta\\)\nPosterior: \\(f(\\boldsymbol{\\theta} | \\mathcal{D})\\) - updated beliefs after observing data\n\n\n\n\nNormal Distribution with Known Variance\n\nLikelihood: \\(f(\\mathbf{y} | \\mu, \\sigma^2) = \\prod_{i=1}^N \\mathcal{N}(y_i | \\mu, \\sigma^2)\\)\nPrior: \\(f(\\mu) \\sim \\mathcal{N}(\\mu | \\mu_0, \\sigma^2_0)\\)\nPosterior: \\(f(\\mu | \\mathbf{y}) \\sim \\mathcal{N}(\\mu | \\hat{\\mu}, \\hat{\\sigma}^2)\\)\n\n\\(\\hat{\\sigma}^2 = \\left(\\frac{N}{\\sigma^2} + \\frac{1}{\\sigma^2_0}\\right)^{-1}\\)\n\\(\\hat{\\mu} = \\hat{\\sigma}^2\\left(\\frac{1}{\\sigma^2}\\sum y_i + \\frac{1}{\\sigma^2_0}\\mu_0\\right)\\)\n\nProperties:\n\nWith diffuse prior (\\(\\sigma^2_0 \\to \\infty\\)), posterior mean converges to sample mean\nAs \\(N \\to \\infty\\), prior influence diminishes and posterior approaches MLE\n\n\n\n\nBayesian Linear Regression\n\nLikelihood: \\(f(\\mathbf{y} | \\mathbf{X}, \\boldsymbol{\\beta}, \\sigma^2) \\sim \\mathcal{N}_N(\\mathbf{y} | \\mathbf{X}\\boldsymbol{\\beta}, \\sigma^2\\mathcal{I}_N)\\)\nPrior: \\(f(\\boldsymbol{\\beta}) \\sim \\mathcal{N}_P(\\boldsymbol{\\beta} | \\boldsymbol{\\mu}_0, \\boldsymbol{\\Sigma}_0)\\)\nPosterior: \\(f(\\boldsymbol{\\beta} | \\mathbf{X}, \\mathbf{y}, \\sigma^2) \\sim \\mathcal{N}_P(\\boldsymbol{\\beta} | \\hat{\\boldsymbol{\\mu}}, \\hat{\\boldsymbol{\\Sigma}})\\)\n\n\\(\\hat{\\boldsymbol{\\Sigma}} = \\left(\\frac{1}{\\sigma^2}\\mathbf{X}^T\\mathbf{X} + \\boldsymbol{\\Sigma}_0^{-1}\\right)^{-1}\\)\n\\(\\hat{\\boldsymbol{\\mu}} = \\hat{\\boldsymbol{\\Sigma}}\\left(\\frac{1}{\\sigma^2}\\mathbf{X}^T\\mathbf{y} + \\boldsymbol{\\Sigma}^{-1}_0\\boldsymbol{\\mu}_0\\right)\\)\n\nConnections to regularization:\n\nWith \\(\\boldsymbol{\\mu}_0 = \\mathbf{0}\\) and \\(\\boldsymbol{\\Sigma}_0 = \\tau^2\\mathcal{I}_P\\): \\(\\hat{\\boldsymbol{\\mu}} = (\\mathbf{X}^T\\mathbf{X} + \\lambda\\mathcal{I}_P)^{-1}\\mathbf{X}^T\\mathbf{y}\\)\nThis is equivalent to ridge regression with \\(\\lambda = \\frac{\\sigma^2}{\\tau^2}\\)\nL2 regularization (weight decay) can be interpreted as a Bayesian prior favoring simpler models"
  },
  {
    "objectID": "lectures/summaries/Lecture23Summary.html",
    "href": "lectures/summaries/Lecture23Summary.html",
    "title": "Lecture 23: Variational Autoencoders",
    "section": "",
    "text": "Density estimation: Determine probability \\(P(\\mathbf{x})\\) of observing data points\nSampling: Generate novel data from the model distribution\nRepresentation learning: Extract meaningful feature representations and manipulate specific features\n\n\n\n\n\nPCA and deterministic autoencoders can learn mappings between input space and latent space\nBoth fail to compute \\(P(\\mathbf{x}|\\boldsymbol{\\Theta}) = \\int P(\\mathbf{x}|\\boldsymbol{\\Theta}, \\mathbf{z})P(\\mathbf{z})d\\mathbf{z}\\)\nCannot properly generate new samples from latent space"
  },
  {
    "objectID": "lectures/summaries/Lecture23Summary.html#generative-models",
    "href": "lectures/summaries/Lecture23Summary.html#generative-models",
    "title": "Lecture 23: Variational Autoencoders",
    "section": "",
    "text": "Density estimation: Determine probability \\(P(\\mathbf{x})\\) of observing data points\nSampling: Generate novel data from the model distribution\nRepresentation learning: Extract meaningful feature representations and manipulate specific features\n\n\n\n\n\nPCA and deterministic autoencoders can learn mappings between input space and latent space\nBoth fail to compute \\(P(\\mathbf{x}|\\boldsymbol{\\Theta}) = \\int P(\\mathbf{x}|\\boldsymbol{\\Theta}, \\mathbf{z})P(\\mathbf{z})d\\mathbf{z}\\)\nCannot properly generate new samples from latent space"
  },
  {
    "objectID": "lectures/summaries/Lecture23Summary.html#factor-analysis",
    "href": "lectures/summaries/Lecture23Summary.html#factor-analysis",
    "title": "Lecture 23: Variational Autoencoders",
    "section": "Factor Analysis",
    "text": "Factor Analysis\n\nProbabilistic Approach\n\nAssumes each input follows a distribution: \\(P(\\mathbf{x}|\\mathbf{z}) = \\mathcal{P}(\\mathbf{x}|f(\\mathbf{z},\\boldsymbol{\\Theta}))\\)\n\\(f(\\mathbf{z})\\) maps latent variables to original feature space\n\\(\\boldsymbol{\\Theta}\\) contains parameters that define the mapping\n\n\n\nOptimization Challenge\n\nGoal: maximize likelihood \\(\\hat{\\boldsymbol{\\Theta}} = \\underset{\\boldsymbol{\\Theta}}{\\text{argmax}} \\prod_{i=1}^N P(\\mathbf{x}_i|\\mathbf{z}_i,\\boldsymbol{\\Theta})\\)\nSince \\(\\mathbf{z}\\) is unknown, must integrate: \\(\\hat{\\boldsymbol{\\Theta}} = \\underset{\\boldsymbol{\\Theta}}{\\text{argmax}} \\prod_{i=1}^N \\int P(\\mathbf{x}_i|\\mathbf{z},\\boldsymbol{\\Theta})P(\\mathbf{z}_i)d\\mathbf{z}\\)\nLog-likelihood involves intractable log-integral: \\(\\hat{\\boldsymbol{\\Theta}} = \\underset{\\boldsymbol{\\Theta}}{\\text{argmax}} \\sum_{i=1}^N \\log \\int P(\\mathbf{x}_i|\\mathbf{z},\\boldsymbol{\\Theta})P(\\mathbf{z}_i)d\\mathbf{z}\\)"
  },
  {
    "objectID": "lectures/summaries/Lecture23Summary.html#variational-inference",
    "href": "lectures/summaries/Lecture23Summary.html#variational-inference",
    "title": "Lecture 23: Variational Autoencoders",
    "section": "Variational Inference",
    "text": "Variational Inference\n\nBayesian Framework\n\nRearranging Bayes’ rule: \\(P(\\mathbf{x}_i|\\boldsymbol{\\Theta}) = \\frac{P(\\mathbf{x}_i|\\mathbf{z}_i,\\boldsymbol{\\Theta})P(\\mathbf{z}_i)}{P(\\mathbf{z}_i|\\mathbf{x}_i,\\boldsymbol{\\Theta})}\\)\nKnown components:\n\nDecoder (likelihood): \\(P(\\mathbf{x}_i|\\mathbf{z}_i,\\boldsymbol{\\Theta})\\)\nPrior: \\(P(\\mathbf{z}_i)\\)\n\nUnknown components:\n\nMarginal likelihood: \\(P(\\mathbf{x}_i|\\boldsymbol{\\Theta})\\)\nEncoder (posterior): \\(P(\\mathbf{z}_i|\\mathbf{x}_i,\\boldsymbol{\\Theta})\\)\n\n\n\n\nApproximation Strategy\n\nIntroduce approximate posterior: \\(Q(\\mathbf{z}_i|\\mathbf{x}_i,\\boldsymbol{\\Phi}) \\approx P(\\mathbf{z}_i|\\mathbf{x}_i,\\boldsymbol{\\Theta})\\)\nRearrange log-likelihood: \\(\\log P(\\mathbf{x}_i|\\boldsymbol{\\Theta}) = \\log P(\\mathbf{x}_i|\\mathbf{z}_i,\\boldsymbol{\\Theta}) - \\log \\frac{Q(\\mathbf{z}_i|\\mathbf{x}_i,\\boldsymbol{\\Phi})}{P(\\mathbf{z}_i)} + \\log \\frac{Q(\\mathbf{z}_i|\\mathbf{x}_i,\\boldsymbol{\\Phi})}{P(\\mathbf{z}_i|\\mathbf{x}_i,\\boldsymbol{\\Theta})}\\)\nTaking expectation with respect to \\(Q\\): \\(E_Q[\\log P(\\mathbf{x}_i|\\boldsymbol{\\Theta})] = E_Q[\\log P(\\mathbf{x}_i|\\mathbf{z}_i,\\boldsymbol{\\Theta})] - E_Q[\\log \\frac{Q(\\mathbf{z}_i|\\mathbf{x}_i,\\boldsymbol{\\Phi})}{P(\\mathbf{z}_i)}] + E_Q[\\log \\frac{Q(\\mathbf{z}_i|\\mathbf{x}_i,\\boldsymbol{\\Phi})}{P(\\mathbf{z}_i|\\mathbf{x}_i,\\boldsymbol{\\Theta})}]\\)\n\n\n\nEvidence Lower Bound (ELBO)\n\nRecognize KL divergence terms: \\(D_{KL}(Q(\\mathbf{z}_i|\\mathbf{x}_i) || P(\\mathbf{z}_i)) = \\int Q(\\mathbf{z}_i|\\mathbf{x}_i,\\boldsymbol{\\Phi}) \\log \\frac{Q(\\mathbf{z}_i|\\mathbf{x}_i,\\boldsymbol{\\Phi})}{P(\\mathbf{z}_i)} d\\mathbf{z}_i\\)\nSince \\(D_{KL}(Q(\\mathbf{z}_i|\\mathbf{x}_i) || P(\\mathbf{z}_i|\\mathbf{x}_i)) \\geq 0\\): \\(\\log P(\\mathbf{x}_i|\\boldsymbol{\\Theta}) \\geq E_Q[\\log P(\\mathbf{x}_i|\\mathbf{z}_i,\\boldsymbol{\\Theta})] - D_{KL}(Q(\\mathbf{z}_i|\\mathbf{x}_i) || P(\\mathbf{z}_i))\\)\nELBO interpretations:\n\n\\(E_Q[\\log P(\\mathbf{x}_i|\\mathbf{z}_i,\\boldsymbol{\\Theta})]\\): Expected reconstruction error\n\\(D_{KL}(Q(\\mathbf{z}_i|\\mathbf{x}_i) || P(\\mathbf{z}_i))\\): Regularization forcing approximate posterior to stay close to prior"
  },
  {
    "objectID": "lectures/summaries/Lecture23Summary.html#variational-autoencoders-vaes",
    "href": "lectures/summaries/Lecture23Summary.html#variational-autoencoders-vaes",
    "title": "Lecture 23: Variational Autoencoders",
    "section": "Variational Autoencoders (VAEs)",
    "text": "Variational Autoencoders (VAEs)\n\nModel Architecture\n\nPrior on latent variables: \\(P(\\mathbf{z}) \\sim \\mathcal{N}_K(\\mathbf{0},\\mathcal{I}_K)\\)\nDecoder (likelihood): \\(P(\\mathbf{x}|\\mathbf{z}) \\sim \\mathcal{N}_P(f(\\mathbf{z}),\\boldsymbol{\\Sigma}_x)\\)\nEncoder (approximate posterior): \\(Q(\\mathbf{z}|\\mathbf{x}) \\sim \\mathcal{N}_K(g(\\mathbf{x}),\\boldsymbol{\\Sigma}_z)\\)\nFunctions \\(f(\\mathbf{z})\\) and \\(g(\\mathbf{x})\\) implemented using neural networks\n\n\n\nImplementation Details\n\nEncoder network maps input to parameters of latent distribution (mean and variance)\nDecoder network maps latent samples to parameters of output distribution\nTraining objective: Maximize ELBO (minimize negative ELBO) \\(-E_Q[\\log P(\\mathbf{x}|\\mathbf{z})] + D_{KL}(Q(\\mathbf{z}|\\mathbf{x}) || P(\\mathbf{z}))\\)\nFor standard normal prior and diagonal normal proposal: \\(D_{KL}(Q(\\mathbf{z}_i|\\mathbf{x}_i) || P(\\mathbf{z}_i)) = \\frac{1}{2} \\sum_{k=1}^K [\\sigma^2_k + \\mu^2_k - 1 - \\log(\\sigma^2_k)]\\)\n\n\n\nPractical Considerations\n\nOften restrict covariance matrices to be diagonal for computational efficiency\nUse multivariate normal distributions for tractable KL divergence computation\nAmortized inference: Rather than learning separate posteriors for each data point, learn functions mapping inputs to distribution parameters\n\n\n\nAdvantages\n\nCreates probabilistic mappings between input and latent space\nFills gaps between training examples in a principled way\nEnables sampling new coherent images from the prior\nSupports image editing through latent space manipulation"
  },
  {
    "objectID": "lectures/summaries/Lecture25Summary.html",
    "href": "lectures/summaries/Lecture25Summary.html",
    "title": "Lecture 25: Generative Adversarial Networks",
    "section": "",
    "text": "Goal: Learn \\(P(\\mathbf{x})\\) without explicitly modeling the probability distribution\nGives up on explicit density estimation but maintains ability to generate high-quality samples\nAddresses limitations of previous approaches:\n\nAutoregressive models: Slow training and generation, no explicit latent code\nVAEs: Lower bound optimization, often produces blurry images due to averaging\n\n\n\n\n\n\nIntroduce latent variable \\(\\mathbf{z}\\) with simple prior \\(P(\\mathbf{z})\\)\nGenerator network: \\(\\hat{\\mathbf{x}} = g(\\mathbf{z})\\) maps from latent space to data space\nLearn transformation from easy-to-sample distribution to complex data distribution"
  },
  {
    "objectID": "lectures/summaries/Lecture25Summary.html#gans-overview",
    "href": "lectures/summaries/Lecture25Summary.html#gans-overview",
    "title": "Lecture 25: Generative Adversarial Networks",
    "section": "",
    "text": "Goal: Learn \\(P(\\mathbf{x})\\) without explicitly modeling the probability distribution\nGives up on explicit density estimation but maintains ability to generate high-quality samples\nAddresses limitations of previous approaches:\n\nAutoregressive models: Slow training and generation, no explicit latent code\nVAEs: Lower bound optimization, often produces blurry images due to averaging\n\n\n\n\n\n\nIntroduce latent variable \\(\\mathbf{z}\\) with simple prior \\(P(\\mathbf{z})\\)\nGenerator network: \\(\\hat{\\mathbf{x}} = g(\\mathbf{z})\\) maps from latent space to data space\nLearn transformation from easy-to-sample distribution to complex data distribution"
  },
  {
    "objectID": "lectures/summaries/Lecture25Summary.html#theoretical-foundation",
    "href": "lectures/summaries/Lecture25Summary.html#theoretical-foundation",
    "title": "Lecture 25: Generative Adversarial Networks",
    "section": "Theoretical Foundation",
    "text": "Theoretical Foundation\n\nDensity Ratio Perspective\n\nCompare two distributions: true data distribution \\(P(\\mathbf{x})\\) and model distribution \\(Q(\\mathbf{x})\\)\nDistributions are identical when: \\(\\frac{P(\\mathbf{x})}{Q(\\mathbf{x})} = 1\\) for all \\(\\mathbf{x}\\)\nBinary classification setup:\n\n\\(y_i = 1\\) for samples from \\(P(\\mathbf{x})\\) (real data)\n\\(y_i = 0\\) for samples from \\(Q(\\mathbf{x})\\) (fake data)\n\n\n\n\nDensity Ratio via Classification\n\nDensity ratio expressed as: \\(\\frac{P(\\mathbf{x})}{Q(\\mathbf{x})} = \\frac{p(\\mathbf{x}|y=1)}{p(\\mathbf{x}|y=0)}\\)\nUsing Bayes’ rule: \\(\\frac{P(\\mathbf{x})}{Q(\\mathbf{x})} = \\frac{p(y=1|\\mathbf{x})}{p(y=0|\\mathbf{x})} \\frac{p(y=0)}{p(y=1)}\\)\nFor equal sampling: \\(\\frac{P(\\mathbf{x})}{Q(\\mathbf{x})} = \\frac{p(y=1|\\mathbf{x})}{1-p(y=1|\\mathbf{x})}\\)\n\n\n\nOptimal Discriminator\n\nBinary classification objective: \\(\\boldsymbol{\\Phi} = \\underset{\\phi}{\\text{argmax}} E[y \\log D_{\\phi}(\\mathbf{x}) + (1-y) \\log(1-D_{\\phi}(\\mathbf{x}))]\\)\nOptimal discriminator: \\(D_{\\Phi}(\\mathbf{x}) = \\frac{P(\\mathbf{x})}{P(\\mathbf{x}) + Q(\\mathbf{x})}\\)\nMaximum value function: \\(V(P,Q) = \\underset{\\phi}{\\text{max}} E[y \\log D_{\\phi}(\\mathbf{x}) + (1-y) \\log(1-D_{\\phi}(\\mathbf{x}))]\\)\n\n\n\nJensen-Shannon Divergence\n\nOptimal classifier performance relates to Jensen-Shannon divergence: \\(V(P,Q) = \\frac{1}{2} D_{KL}\\left(P(\\mathbf{x}) || \\frac{1}{2}(Q(\\mathbf{x}) + P(\\mathbf{x}))\\right) + \\frac{1}{2} D_{KL}\\left(Q(\\mathbf{x}) || \\frac{1}{2}(Q(\\mathbf{x}) + P(\\mathbf{x}))\\right) - \\log 2\\)\nJensen-Shannon divergence is symmetric (unlike KL divergence)\nMeasures distance between distributions via common “middle point”"
  },
  {
    "objectID": "lectures/summaries/Lecture25Summary.html#gan-architecture-and-training",
    "href": "lectures/summaries/Lecture25Summary.html#gan-architecture-and-training",
    "title": "Lecture 25: Generative Adversarial Networks",
    "section": "GAN Architecture and Training",
    "text": "GAN Architecture and Training\n\nMinimax Game Formulation\n\nGenerator objective: Minimize Jensen-Shannon divergence between \\(P(\\mathbf{x})\\) and \\(Q(\\mathbf{x})\\)\nDiscriminator objective: Maximize classification accuracy between real and fake samples\nCombined objective: \\(\\underset{\\theta}{\\text{min}} \\underset{\\phi}{\\text{max}} \\frac{1}{2} E_{P(\\mathbf{x})}[\\log D_{\\phi}(\\mathbf{x})] + E_{Q(\\mathbf{z})}[\\log(1 - D_{\\phi}(g_{\\theta}(\\mathbf{z})))]\\)\n\n\n\nTraining Strategy\n\nDiscriminator update: Minimize detection loss (maximize ability to distinguish real from fake)\n\nCompute discriminator loss on real and fake images\nUpdate to minimize classification error\n\nGenerator update: Fool discriminator\n\nGenerate fake instances\nCompute discriminator loss treating all instances as real\nUpdate to minimize this loss\n\n\n\n\nTraining Challenges\n\nMode collapse: Generator converges to single mode even with multimodal true distribution\nMode hopping: Generator jumps between modes without convergence\nOptimization difficulties: No traditional loss function to optimize toward\nNash equilibrium: Theoretical convergence in zero-sum game setting"
  },
  {
    "objectID": "lectures/summaries/Lecture25Summary.html#practical-considerations",
    "href": "lectures/summaries/Lecture25Summary.html#practical-considerations",
    "title": "Lecture 25: Generative Adversarial Networks",
    "section": "Practical Considerations",
    "text": "Practical Considerations\n\nTraining Improvements\n\nLearning rate strategies: Careful tuning to prevent mode collapse and hopping\nGradient penalties: Clip gradients or add penalties to prevent large moves\nIterative training: Train discriminator multiple times before generator update\nSGD over ADAM: Sometimes momentum-based methods struggle without clear loss target\n\n\n\nWasserstein GAN Improvements\n\nLinear discriminator outputs: Predict “realness” scores \\(D(\\mathbf{x}) \\in \\mathbb{R}\\) instead of probabilities\nModified losses:\n\nDiscriminator: \\(D(\\mathbf{x}) - D(g(\\mathbf{z}))\\) (maximize realness difference)\nGenerator: \\(D(g(\\mathbf{z}))\\) (maximize realness of fakes)"
  },
  {
    "objectID": "lectures/summaries/Lecture25Summary.html#extensions-and-applications",
    "href": "lectures/summaries/Lecture25Summary.html#extensions-and-applications",
    "title": "Lecture 25: Generative Adversarial Networks",
    "section": "Extensions and Applications",
    "text": "Extensions and Applications\n\nConditional GANs\n\nGoal: Control generation with additional information (class labels)\nArchitecture modifications:\n\nGenerator: \\(g(\\mathbf{z}, \\mathbf{c})\\) where \\(\\mathbf{c}\\) is conditioning information\nDiscriminator: \\(D(\\mathbf{x}, \\mathbf{c})\\) considers both image and condition\n\n\n\n\nConditional Batch Normalization\n\nStandard batch normalization: \\(h_{ij}^* = \\gamma_j g_{ij} + \\beta_j\\)\nConditional version: \\(h_{ij}^* = \\gamma^{(y)}_j g_{ij} + \\beta^{(y)}_j\\)\nLearn separate normalization parameters for each class\nForces both networks to perform well across all classes\nApproximates \\(P(\\mathbf{x}|y)\\) instead of \\(P(\\mathbf{x})\\)\n\n\n\nStyleGAN\n\nApproach: Split latent space into initial space and style space\nFeature control: Learn explicit mappings from features to style vectors\nApplications: Particularly effective for generating realistic human faces"
  },
  {
    "objectID": "lectures/summaries/Lecture25Summary.html#advantages-and-limitations",
    "href": "lectures/summaries/Lecture25Summary.html#advantages-and-limitations",
    "title": "Lecture 25: Generative Adversarial Networks",
    "section": "Advantages and Limitations",
    "text": "Advantages and Limitations\n\nAdvantages\n\nHigh-quality generation: Can produce very realistic samples\nFast sampling: No iterative generation process required\nAssumption-free: Minimal distributional assumptions\nMathematically sound: Grounded in game theory and divergence minimization\n\n\n\nLimitations\n\nNo density estimation: Cannot compute \\(P(\\mathbf{x})\\) directly\nNo encoding capability: Difficult to map from data space back to latent space\nTraining instability: Sensitive to hyperparameters and training procedures\nLimited control: Basic GANs provide limited control over generated content"
  },
  {
    "objectID": "lectures/summaries/Lecture27Summary.html",
    "href": "lectures/summaries/Lecture27Summary.html",
    "title": "Lecture 27: Interpretable Machine Learning",
    "section": "",
    "text": "Model performance \\(\\neq\\) success: Classifiers can perform well for incorrect reasons\nSafety: Detect and prevent critical failures (e.g., autonomous driving misclassifications)\nFairness: Reveal and mitigate biases across different groups\nPrivacy/Security: Ensure protection of sensitive information\nLegal requirements: Compliance with regulations like GDPR’s “right to explanation”\n\n\n\n\n\nNeural networks are complicated “black-box” algorithms: \\(\\hat{y} \\propto \\mathbf{W} \\varphi(\\mathbf{W} \\varphi (...))\\)\nDefinition (Biran and Cotton, 2017): “Interpretability is the degree to which a human can understand the cause of a decision”\nAlternative definition (Kim et al., 2016): “… a user can correctly and efficiently predict the method’s results”"
  },
  {
    "objectID": "lectures/summaries/Lecture27Summary.html#motivation-for-interpretability",
    "href": "lectures/summaries/Lecture27Summary.html#motivation-for-interpretability",
    "title": "Lecture 27: Interpretable Machine Learning",
    "section": "",
    "text": "Model performance \\(\\neq\\) success: Classifiers can perform well for incorrect reasons\nSafety: Detect and prevent critical failures (e.g., autonomous driving misclassifications)\nFairness: Reveal and mitigate biases across different groups\nPrivacy/Security: Ensure protection of sensitive information\nLegal requirements: Compliance with regulations like GDPR’s “right to explanation”\n\n\n\n\n\nNeural networks are complicated “black-box” algorithms: \\(\\hat{y} \\propto \\mathbf{W} \\varphi(\\mathbf{W} \\varphi (...))\\)\nDefinition (Biran and Cotton, 2017): “Interpretability is the degree to which a human can understand the cause of a decision”\nAlternative definition (Kim et al., 2016): “… a user can correctly and efficiently predict the method’s results”"
  },
  {
    "objectID": "lectures/summaries/Lecture27Summary.html#glass-box-models",
    "href": "lectures/summaries/Lecture27Summary.html#glass-box-models",
    "title": "Lecture 27: Interpretable Machine Learning",
    "section": "Glass-Box Models",
    "text": "Glass-Box Models\n\nInherently Interpretable Models\n\nLinear models: \\(\\hat{y} = g(\\mathbf{X} \\hat{\\boldsymbol{\\beta}})\\) - Changes in inputs directly relate to changes in predictions\nSimple decision trees: Transparent decision paths\nRidge/LASSO: Regularized linear models with variable selection\nGeneralized Additive Models: Flexible but transparent functional forms\n\n\n\nTransparency-Performance Trade-off\n\nAs transparency increases, bias typically increases\nInterpretable models require functional specification, which means making assumptions\nModern machine learning needs universal approximators to model complex dependencies\nUniversal approximators (RFs, NNs, SVMs) tend to be local in nature and difficult to interpret globally"
  },
  {
    "objectID": "lectures/summaries/Lecture27Summary.html#post-hoc-interpretability-methods",
    "href": "lectures/summaries/Lecture27Summary.html#post-hoc-interpretability-methods",
    "title": "Lecture 27: Interpretable Machine Learning",
    "section": "Post-Hoc Interpretability Methods",
    "text": "Post-Hoc Interpretability Methods\n\nLIME (Local Interpretable Model-agnostic Explanations)\n\nGoal: Understand how changes in feature values affect predictions for individual instances\nProcess:\n\nCreate perturbed samples around target instance \\(\\mathbf{x}\\)\nGet predictions for perturbed samples from black box\nCompute proximity weights using kernel function\nFit weighted LASSO to create local linear approximation: \\(\\hat{\\boldsymbol{\\beta}} = \\underset{\\{\\beta_0, \\boldsymbol{\\beta}\\}}{\\text{argmin}} \\sum_{i=1}^N w_i(\\hat{y}_i - \\beta_0 - \\mathbf{x}_i' \\boldsymbol{\\beta})^2 + \\lambda \\sum_{j=1}^P |\\beta_j|\\)\n\nStrengths: Short, understandable explanations; easy implementation\nWeaknesses: Sensitive to neighborhood choice; unrealistic data points; limited to single-point analysis\n\n\n\nPermutation Importance\n\nMeasures feature importance as impact on model performance\nProcedure:\n\nFit model and compute baseline performance on validation set\nFor each feature, randomly permute its values and measure performance drop\nRank features by performance decrease\n\nAdvantage: Global measure of feature importance without refitting model\nApplication: Standard method for random forests, gradient boosting, tabular neural nets"
  },
  {
    "objectID": "lectures/summaries/Lecture27Summary.html#image-and-text-interpretability",
    "href": "lectures/summaries/Lecture27Summary.html#image-and-text-interpretability",
    "title": "Lecture 27: Interpretable Machine Learning",
    "section": "Image and Text Interpretability",
    "text": "Image and Text Interpretability\n\nSaliency Maps for Images\n\nVanilla Gradient Method:\n\nPerform forward pass to get class score\nCompute gradient of score with respect to input pixels: \\(\\frac{\\partial s_c}{\\partial \\mathbf{x}}\\)\nVisualize gradient as heatmap\n\n\nLocal linear approximation: \\(s_c(\\mathbf{x}) \\approx \\mathbf{w}^T \\mathbf{x} + b\\) where \\(\\mathbf{w} = \\frac{\\partial s_c}{\\partial \\mathbf{x}}|_{\\mathbf{x}_0}\\)\n\nSmoothGrad: Average gradients over multiple noise-added versions of image to reduce sensitivity\nGradient-weighted Class Activation Mapping (Grad-CAM):\n\nCompute gradient of score w.r.t. final convolutional layer activations\nCompute weights \\(\\alpha_d\\) by global average pooling gradients for each filter\nCreate coarse heatmap: \\(\\mathbf{x}_{ij} = \\text{ReLU}\\left(\\sum_{d=1}^D \\alpha_d x_{i,j,d}\\right)\\)\nUpscale to original image size\n\nGuided Grad-CAM: Combine vanilla gradients with Grad-CAM for finer detail\n\n\n\nAttention Visualization for Text\n\nGoal: Understand how words relate to each other and contribute to classification\nChallenge: Multiple attention heads and layers in transformer models\nAttention matrices: Visualization of attention weights between tokens\nAggregation methods:\n\nLayer-wise averaging across attention heads\nModel-wide averaging across layers\nRoll-out: Account for residual connections by adding identity matrix to attention matrices\n\nAugmented layer-wise attention: \\(\\tilde{\\mathbf{A}}^\\ell = \\mathbf{A}^{\\ell} + \\mathcal{I}\\)\nRolled-out attention: \\(\\mathbf{R} = \\hat{\\mathbf{A}}^{(1)} \\times \\hat{\\mathbf{A}}^{(2)} \\times ... \\times \\hat{\\mathbf{A}}^{(L)}\\)\n\n\nLimitation: Aggregation loses specificity and causal importance"
  },
  {
    "objectID": "lectures/summaries/Lecture27Summary.html#intrinsic-interpretability",
    "href": "lectures/summaries/Lecture27Summary.html#intrinsic-interpretability",
    "title": "Lecture 27: Interpretable Machine Learning",
    "section": "Intrinsic Interpretability",
    "text": "Intrinsic Interpretability\n\nLimitations of Post-Hoc Methods\n\nCannot expect models to learn interpretable structures without designing for interpretability\nPost-hoc explanation may not reflect actual decision process\nAnalogy: Training a puppy is more effective with immediate reinforcement than delayed feedback\n\n\n\nMixture of Experts (MoE)\n\nApproach: Divide hidden units into expert blocks, each specializing in specific input patterns\nImplementation:\n\nBreak feedforward layer into K equally sized expert blocks\nPass input through routing function that selects k &lt; K experts\nProcess input only through selected experts\n\nBenefits:\n\nEach expert specializes in meaningful aspects of inputs\nEnsemble learning advantage: reduced model variance\nEfficiency: Significant decrease in training time for large models\nApplications: Found in Switch Transformers, LLaMA, DeepSeek models\n\n\n\n\nOther Glass-Box Approaches\n\nConcept Bottleneck Models: Predict human-interpretable concepts as intermediary step\nKNN Retrieval Models: Retrieve similar training examples to justify predictions"
  },
  {
    "objectID": "lectures/summaries/Lecture27Summary.html#future-directions",
    "href": "lectures/summaries/Lecture27Summary.html#future-directions",
    "title": "Lecture 27: Interpretable Machine Learning",
    "section": "Future Directions",
    "text": "Future Directions\n\nResearch Priorities\n\nDesigning models with built-in interpretability without sacrificing performance\nIntegration of interpretability considerations into model training\nImproving efficiency and scalability of interpretable architectures\n\n\n\nIndustry Impact\n\nGrowing demand for interpretable AI in high-stakes applications\nCompanies developing specialized architectures with transparency built in\nCompliance with emerging regulatory requirements driving innovation"
  },
  {
    "objectID": "lectures/summaries/Lecture2Summary.html",
    "href": "lectures/summaries/Lecture2Summary.html",
    "title": "Lecture 2 Summary: Machine Learning Fundamentals",
    "section": "",
    "text": "This lecture lays the groundwork for understanding machine learning. We explore the core definition of learning, categorize algorithmic tasks, and then focus on predictive modeling, the role of loss functions, and the critical concept of generalization error."
  },
  {
    "objectID": "lectures/summaries/Lecture2Summary.html#what-is-machine-learning",
    "href": "lectures/summaries/Lecture2Summary.html#what-is-machine-learning",
    "title": "Lecture 2 Summary: Machine Learning Fundamentals",
    "section": "1. What is Machine Learning?",
    "text": "1. What is Machine Learning?\nAt its heart, machine learning involves algorithms that learn from data. &gt; “A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.” (Mitchell, 1997)\nLet’s break down these components:\n\nTasks (T): The objectives the algorithm aims to achieve.\n\nPrediction: Forecasting future or unknown values (e.g., stock prices, disease likelihood).\nDescription: Uncovering patterns or structures in data (e.g., customer segmentation via clustering, topic modeling in texts). This often involves dimensionality reduction or density estimation.\nInference/Explanation: Understanding the causal relationships and the impact of variables on one another.\n\nLoss focus: Accuracy of parameter estimates, e.g., for regression coefficients \\boldsymbol{\\beta}, a common loss is Mean Squared Error of the estimate: E[(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})^2].\n\n\nExperience (E): The data that fuels the learning process.\n\nSupervised Learning: Uses labeled data, where each data point has features and a known outcome (e.g., images labeled with “cat” or “dog”).\nUnsupervised Learning: Uses unlabeled data, focusing on finding inherent structure in the features themselves (e.g., grouping similar news articles).\nSemi-supervised Learning: A hybrid approach using a small amount of labeled data and a large amount of unlabeled data.\nReinforcement Learning: The algorithm learns by interacting with an environment and receiving feedback (rewards or penalties) for its actions (e.g., training a robot to navigate a maze).\n\nPerformance Measure (P): A metric to quantify how well the algorithm is performing its task. This is typically achieved through a loss function."
  },
  {
    "objectID": "lectures/summaries/Lecture2Summary.html#predictive-performance-loss-functions",
    "href": "lectures/summaries/Lecture2Summary.html#predictive-performance-loss-functions",
    "title": "Lecture 2 Summary: Machine Learning Fundamentals",
    "section": "2. Predictive Performance & Loss Functions",
    "text": "2. Predictive Performance & Loss Functions\nIn predictive modeling, we often assume an underlying true relationship: y = f(\\mathbf{x}) + \\epsilon where:\n\ny is the outcome variable.\nf(\\mathbf{x}) represents the systematic information (the “signal”) that features \\mathbf{x} provide about y.\n\\epsilon is irreducible random error or “noise.”\n\nThe goal is to learn an approximation, \\hat{f}(\\mathbf{x}), that accurately captures f(\\mathbf{x}) from the training data.\nCommon Loss Functions:\n\nFor Continuous Outcomes:\n\nMean Squared Error (MSE): Measures the average squared difference between predicted and actual values. \\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{f}(\\mathbf{x}_i))^2\n\nFor Discrete/Categorical Outcomes:\n\n0/1 Loss: Assigns a loss of 1 for an incorrect classification and 0 for a correct one. \\text{0/1 Loss} = \\frac{1}{N} \\sum_{i=1}^{N} I(\\hat{f}(\\mathbf{x}_i) \\neq y_i) where I(\\cdot) is the indicator function. In probability terms for a single instance: 1 - P(\\hat{f}(\\mathbf{x}) = y).\nCross-Entropy Loss (or Log Loss): Penalizes confident but incorrect predictions more heavily. For a multi-class problem with K classes: \\text{Cross-Entropy} = - \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{k=1}^{K} I(y_i = k) \\log P(\\hat{f}(\\mathbf{x}_i) = k)"
  },
  {
    "objectID": "lectures/summaries/Lecture2Summary.html#generalization-error-the-true-test-of-a-model",
    "href": "lectures/summaries/Lecture2Summary.html#generalization-error-the-true-test-of-a-model",
    "title": "Lecture 2 Summary: Machine Learning Fundamentals",
    "section": "3. Generalization Error: The True Test of a Model",
    "text": "3. Generalization Error: The True Test of a Model\nWhile performance on training data is informative, the ultimate goal is a model that performs well on unseen data. This is captured by generalization error.\n3.1. Defining Generalization Error\nAssuming our data samples \\lbrace\\mathbf{x}_i, y_i\\rbrace are independent and identically distributed (i.i.d.) draws from a true, underlying data-generating distribution \\mathcal{T}, the generalization error is the expected loss of our model \\hat{f}(\\mathbf{x}) over this entire distribution: \\text{Generalization Error} = E_{\\mathcal{T}}[\\mathcal{L}(\\hat{f}(\\mathbf{x}), y)]\n3.2. Training Error vs. Generalization Error\nIn practice, we can only directly compute the training error using our observed training dataset \\mathcal{D} = \\{(\\mathbf{x}_i, y_i)\\}_{i=1}^N: \\text{Training Error} = E_{\\mathcal{D}}[\\mathcal{L}(\\mathbf{y}, \\hat{\\mathbf{y}})] = \\frac{1}{N} \\sum_{i=1}^{N} \\mathcal{L}(y_i, \\hat{f}(\\mathbf{x}_i))\n3.3. The Relationship: The Role of Covariance\nA key insight connects these two error measures. Under certain assumptions (e.g., fixed features \\mathbf{X}, and outcomes y and y' differing only by noise for training and test sets respectively), for Mean Squared Error, the relationship is: \\text{Expected Test Error} \\approx \\text{Training Error} + \\frac{2}{N} \\sum_{i=1}^{N} \\text{Cov}(y_i, \\hat{y}_i) Here, \\text{Cov}(y_i, \\hat{y}_i) is the covariance between the true training outcomes and the model’s predictions at those training points.\n\nInterpretation: The gap between generalization (test) error and training error increases with the average covariance between the training outcomes and their predictions. A high covariance suggests the model is fitting the noise in the training data (overfitting).\n\n3.4. Implications for Linear Models\nFor linear models of the form \\hat{\\mathbf{y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}}, where \\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}, the sum of covariances can be further analyzed.\n\nThe term \\sum_{i=1}^{N} \\text{Cov}(y_i, \\hat{y}_i) simplifies to \\sigma^2 \\text{tr}(\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T).\nThe trace of the “hat matrix” \\mathbf{H} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T is equal to P, the number of parameters (features) in the model.\nThus, the generalization error offset becomes: \\frac{2P\\sigma^2}{N}\n\n3.5. Key Factors Influencing Generalization Error:\nThis leads to a crucial understanding of what drives generalization:\n\nTraining Error: The baseline error on the data used to build the model.\nModel Complexity (P): More complex models (larger P) tend to increase the covariance term, widening the gap between training and generalization error.\nInherent Noise Variance (\\sigma^2): Higher irreducible error in the data-generating process naturally leads to a larger error gap.\nSample Size (N): Increasing the training sample size N helps to reduce the impact of the covariance term, thereby shrinking the gap and improving generalization.\n\nConclusion:\nThe lecture demonstrates through examples (decision trees, polynomial regression) that finding the right model involves balancing its flexibility against the amount of training data available. An overly complex model might achieve low training error but generalize poorly (overfitting), while an overly simple model might not capture the underlying patterns even in the training data (underfitting). The optimal model complexity often scales with the size of the training dataset."
  },
  {
    "objectID": "lectures/summaries/Lecture4Summary.html",
    "href": "lectures/summaries/Lecture4Summary.html",
    "title": "Lecture 4 Summary: Linear Models and Likelihood",
    "section": "",
    "text": "This summary bridges machine learning fundamentals with practical implementation of linear models, connecting empirical risk minimization with likelihood-based approaches within a unified statistical framework."
  },
  {
    "objectID": "lectures/summaries/Lecture4Summary.html#from-predictive-models-to-hypothesis-spaces",
    "href": "lectures/summaries/Lecture4Summary.html#from-predictive-models-to-hypothesis-spaces",
    "title": "Lecture 4 Summary: Linear Models and Likelihood",
    "section": "1. From Predictive Models to Hypothesis Spaces",
    "text": "1. From Predictive Models to Hypothesis Spaces\nIn supervised learning, we seek to approximate an unknown function f(\\mathbf{x}) that relates features to outcomes:\ny = f(\\mathbf{x}) + \\epsilon\nWhile the ideal goal is to minimize the true expected risk, this is generally impossible without knowing the true data-generating distribution. Instead, we work within a constrained hypothesis space \\mathcal{H} - the set of candidate functions we consider.\n1.1. Linear Models as a Hypothesis Space\nThe most common hypothesis space is the set of linear models, characterized as:\n\\hat{y}_0 = g(\\phi(\\mathbf{x}_0)^T \\hat{\\boldsymbol{\\beta}})\nWhere:\n\n\\phi(\\mathbf{x}) is a feature extractor that can transform input features\ng(\\cdot) is a function mapping inputs to outputs\n\\hat{\\boldsymbol{\\beta}} are the model parameters to be learned\n\nAn important insight: linear models don’t necessarily produce linear decision boundaries. Through clever feature extraction, they can represent highly complex functions. Examples include polynomial expansions and kernel functions (polynomial and RBF kernels) that can capture non-linear patterns in data."
  },
  {
    "objectID": "lectures/summaries/Lecture4Summary.html#empirical-risk-minimization-framework",
    "href": "lectures/summaries/Lecture4Summary.html#empirical-risk-minimization-framework",
    "title": "Lecture 4 Summary: Linear Models and Likelihood",
    "section": "2. Empirical Risk Minimization Framework",
    "text": "2. Empirical Risk Minimization Framework\nGiven that we can’t minimize the true risk directly, we resort to empirical risk minimization (ERM) using our observed training data:\n\\hat{\\boldsymbol{\\beta}} = \\underset{\\boldsymbol{\\beta}^*}{\\text{argmin}} \\frac{1}{N} \\sum_{i=1}^{N} L(y_i, g(\\phi(\\mathbf{x}_i)^T \\boldsymbol{\\beta}^*))\nSince empirical risk typically underestimates generalization error, regularization serves as a correction:\n\\hat{\\boldsymbol{\\beta}} = \\underset{\\boldsymbol{\\beta}^*}{\\text{argmin}} \\frac{1}{N} \\sum_{i=1}^{N} L(y_i, g(\\phi(\\mathbf{x}_i)^T \\boldsymbol{\\beta}^*)) + \\lambda C(\\boldsymbol{\\beta}^*)\nWhere C(\\cdot) measures model complexity and \\lambda is a tuning parameter that controls the strength of the penalty."
  },
  {
    "objectID": "lectures/summaries/Lecture4Summary.html#common-linear-models-and-their-losses",
    "href": "lectures/summaries/Lecture4Summary.html#common-linear-models-and-their-losses",
    "title": "Lecture 4 Summary: Linear Models and Likelihood",
    "section": "3. Common Linear Models and Their Losses",
    "text": "3. Common Linear Models and Their Losses\nLinear models can be categorized based on outcome types and their corresponding loss functions:\n\nLinear Regression (Continuous Outcomes):\nUses mean squared error loss: \\hat{\\boldsymbol{\\beta}} = \\underset{\\boldsymbol{\\beta}^*}{\\text{argmin}} \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\phi(\\mathbf{x}_i)^T \\boldsymbol{\\beta}^*)^2\nBinary Logistic Regression (Binary Outcomes):\nUses probability loss with the logistic function: \\sigma(z) = \\frac{\\exp[z]}{1 + \\exp[z]}\nMultinomial Logistic Regression (Categorical Outcomes):\nUses probability loss with the softmax function: \\sigma_k(\\mathbf{z}) = \\frac{\\exp[z_k]}{\\sum_{h=1}^{K} \\exp[z_h]}"
  },
  {
    "objectID": "lectures/summaries/Lecture4Summary.html#maximum-likelihood-estimation",
    "href": "lectures/summaries/Lecture4Summary.html#maximum-likelihood-estimation",
    "title": "Lecture 4 Summary: Linear Models and Likelihood",
    "section": "4. Maximum Likelihood Estimation",
    "text": "4. Maximum Likelihood Estimation\nThese common linear models can all be viewed through the lens of maximum likelihood estimation (MLE) from probability distributions in the exponential family.\n4.1. The Normal Distribution and Linear Regression\nFor linear regression, assuming normally distributed errors:\nPr(y_i | \\mathbf{x}_i, \\boldsymbol{\\beta}, \\sigma^2) = \\mathcal{N}(y_i | \\mathbf{x}_i^T \\boldsymbol{\\beta}, \\sigma^2)\nThe negative log-likelihood simplifies to mean squared error plus a constant, showing that maximizing the likelihood for Gaussian regression is equivalent to minimizing MSE:\n-\\ell\\ell(\\boldsymbol{\\beta}^* | \\mathbf{X}, \\mathbf{y}, \\sigma^2) = C + \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\mathbf{x}_i^T \\boldsymbol{\\beta})^2\n4.2. The Bernoulli Distribution and Logistic Regression\nFor binary logistic regression, assuming Bernoulli-distributed outcomes:\nPr(y_i | \\mathbf{x}_i, \\boldsymbol{\\beta}) = \\theta_i^{y_i} (1 - \\theta_i)^{1-y_i}\nWhere \\theta_i = \\sigma(\\mathbf{x}_i^T \\boldsymbol{\\beta}) is the probability that y_i = 1.\nUnlike linear regression, logistic regression has no analytical solution, necessitating gradient-based optimization methods."
  },
  {
    "objectID": "lectures/summaries/Lecture4Summary.html#regularization-for-linear-models",
    "href": "lectures/summaries/Lecture4Summary.html#regularization-for-linear-models",
    "title": "Lecture 4 Summary: Linear Models and Likelihood",
    "section": "5. Regularization for Linear Models",
    "text": "5. Regularization for Linear Models\nRegularization improves model generalization by helping to bridge the gap between training error and generalization error.\n5.1. Common Regularization Approaches\nTwo primary norm-based penalties:\n\nL2 Regularization (Ridge Regression):\n\\text{NLL} + \\lambda \\|\\boldsymbol{\\beta}\\|_2^2\nL1 Regularization (LASSO):\n\\text{NLL} + \\lambda \\|\\boldsymbol{\\beta}\\|_1\n\n5.2. Effects of Regularization\nEmpirical examples demonstrate:\n\nAs \\lambda increases, coefficients shrink toward zero\nL1 regularization promotes sparsity (some coefficients become exactly zero)\nL2 regularization tends to distribute the penalty across all coefficients\nThere often exists an optimal \\lambda that minimizes generalization error\n\nConclusion:\nThis framework unifies linear models through empirical risk minimization and maximum likelihood estimation. Regularization techniques integrate into this framework to improve model generalization. This sets the foundation for optimization methods needed when analytical solutions don’t exist, particularly for logistic regression and other non-linear models."
  },
  {
    "objectID": "lectures/summaries/Lecture6Summary.html",
    "href": "lectures/summaries/Lecture6Summary.html",
    "title": "Lecture 6 Summary: Adaptive Methods for Minimization",
    "section": "",
    "text": "This summary introduces advanced optimization techniques for training machine learning models, focusing on adaptive methods that intelligently adjust learning rates to accelerate convergence and improve performance in complex loss landscapes."
  },
  {
    "objectID": "lectures/summaries/Lecture6Summary.html#gradient-descent-the-foundation",
    "href": "lectures/summaries/Lecture6Summary.html#gradient-descent-the-foundation",
    "title": "Lecture 6 Summary: Adaptive Methods for Minimization",
    "section": "1. Gradient Descent: The Foundation",
    "text": "1. Gradient Descent: The Foundation\nWhen analytical solutions for minimizing empirical risk are unavailable, gradient-based optimization methods become essential. The standard gradient descent algorithm iteratively updates parameters:\n\\boldsymbol{\\beta}_{t+1} = \\boldsymbol{\\beta}_t - \\eta_t \\mathbf{g}(\\boldsymbol{\\beta}_t)\nWhere:\n\n\\mathbf{g}(\\boldsymbol{\\beta}_t) is the gradient of the loss function evaluated at the current parameter values\n\\eta_t is the step size (learning rate) at iteration t\n\nFor most machine learning problems, the loss takes the form of an average across observations:\n\\mathcal{L}(\\boldsymbol{\\beta}) = \\frac{1}{N} \\sum_{i=1}^N \\mathcal{L}_i(\\boldsymbol{\\beta})\nConsequently, the gradient calculation requires evaluating gradients for all N observations, becoming computationally expensive as datasets grow."
  },
  {
    "objectID": "lectures/summaries/Lecture6Summary.html#stochastic-gradient-descent-sgd",
    "href": "lectures/summaries/Lecture6Summary.html#stochastic-gradient-descent-sgd",
    "title": "Lecture 6 Summary: Adaptive Methods for Minimization",
    "section": "2. Stochastic Gradient Descent (SGD)",
    "text": "2. Stochastic Gradient Descent (SGD)\nStochastic Gradient Descent addresses the computational burden by approximating the full gradient using small batches of observations:\n\\mathbf{g}(\\boldsymbol{\\beta}) \\approx \\frac{1}{B} \\sum_{i \\in \\mathcal{B}} \\mathbf{g}_i(\\boldsymbol{\\beta})\nWhere \\mathcal{B} represents a randomly sampled batch of B observations.\nEven with batch size as small as 1, SGD converges to the minimum with appropriate learning rate schedules, drastically reducing computational requirements at the cost of introducing noise into the optimization path.\n2.1. Challenges with SGD\nWhile computationally efficient, SGD faces several challenges:\n\nHigh variance around the true minimum\nSensitivity to learning rate selection\nSlow convergence in flat regions of the loss landscape\nPoor performance in ravines and saddle points"
  },
  {
    "objectID": "lectures/summaries/Lecture6Summary.html#second-order-methods-leveraging-curvature",
    "href": "lectures/summaries/Lecture6Summary.html#second-order-methods-leveraging-curvature",
    "title": "Lecture 6 Summary: Adaptive Methods for Minimization",
    "section": "3. Second-Order Methods: Leveraging Curvature",
    "text": "3. Second-Order Methods: Leveraging Curvature\nSecond-order methods incorporate information about the curvature of the loss function through the Hessian matrix. Using a multivariate Taylor series expansion, the update rule becomes:\n\\boldsymbol{\\beta}_{t+1} = \\boldsymbol{\\beta}_t - \\eta \\mathbf{H}(\\boldsymbol{\\beta}_t)^{-1}\\mathbf{g}(\\boldsymbol{\\beta}_t)\nWhere \\mathbf{H}(\\boldsymbol{\\beta}_t) is the Hessian matrix containing second derivatives of the loss function.\nThis approach “deskews” the loss space, accounting for curvature rather than following straight-line paths from each position. For logistic regression, the Hessian can be expressed as:\n\\mathbf{H}(\\boldsymbol{\\beta}_t) = \\frac{1}{N} \\sum_{i=1}^N \\mathbf{x}_i \\mathbf{x}_i^T \\sigma(\\mathbf{x}_i^T \\boldsymbol{\\beta}_t)(1 - \\sigma(\\mathbf{x}_i^T \\boldsymbol{\\beta}_t))\nWhile theoretically powerful, computing and inverting the full Hessian becomes prohibitively expensive in high dimensions."
  },
  {
    "objectID": "lectures/summaries/Lecture6Summary.html#momentum-the-heavy-ball-approach",
    "href": "lectures/summaries/Lecture6Summary.html#momentum-the-heavy-ball-approach",
    "title": "Lecture 6 Summary: Adaptive Methods for Minimization",
    "section": "4. Momentum: The Heavy Ball Approach",
    "text": "4. Momentum: The Heavy Ball Approach\nMomentum methods accelerate optimization by accumulating a running average of past gradients, analogous to a heavy ball rolling down a hill:\n\\mathbf{m}_{t+1} = b \\mathbf{m}_t + \\mathbf{g}(\\boldsymbol{\\beta}_t) \\boldsymbol{\\beta}_{t+1} = \\boldsymbol{\\beta}_t - \\eta \\mathbf{m}_{t+1}\nWhere b \\in [0,1) is the momentum coefficient, typically set around 0.9.\nMomentum provides two key advantages:\n\nAccelerated progress in consistent gradient directions (flat regions)\nDampened oscillations in regions with rapidly changing gradients\n\nWhen gradient directions remain consistent, momentum effectively multiplies the learning rate by a factor approaching \\frac{1}{1-b}. With b=0.9, this can yield a 10x speedup in consistent gradient regions."
  },
  {
    "objectID": "lectures/summaries/Lecture6Summary.html#adaptive-gradient-methods",
    "href": "lectures/summaries/Lecture6Summary.html#adaptive-gradient-methods",
    "title": "Lecture 6 Summary: Adaptive Methods for Minimization",
    "section": "5. Adaptive Gradient Methods",
    "text": "5. Adaptive Gradient Methods\nAdaptive methods dynamically adjust learning rates for each parameter based on their historical gradient information.\n5.1. AdaGrad: Parameter-Specific Learning Rates\nAdaGrad scales the learning rate inversely proportional to the accumulated squared gradients:\n\\boldsymbol{\\beta}_{t+1} = \\boldsymbol{\\beta}_t - \\eta \\frac{\\mathbf{g}(\\boldsymbol{\\beta}_t)}{\\sqrt{\\mathbf{s}_t + \\epsilon}}\nWhere:\n\n\\mathbf{s}_t = \\sum_{h=1}^t \\mathbf{g}(\\boldsymbol{\\beta}_h)^2 (elementwise squaring)\n\\epsilon is a small constant to prevent division by zero\n\nThis approximates the diagonal of the Hessian, allowing larger updates for parameters with small gradients and smaller updates for parameters with large gradients. However, AdaGrad’s accumulation of squared gradients can cause learning rates to diminish too quickly, stalling progress.\n5.2. RMSprop: Exponential Moving Average\nRMSprop addresses AdaGrad’s diminishing learning rates by replacing the sum with an exponential moving average:\n\\mathbf{s}_t = a \\mathbf{s}_{t-1} + (1-a) \\mathbf{g}(\\boldsymbol{\\beta}_t)^2\nWhere a \\in [0,1] controls the decay rate of historical information.\n5.3. Adam: Combining Momentum and Adaptive Learning Rates\nAdam (Adaptive Moment Estimation) combines the benefits of momentum and adaptive learning rates:\n\\mathbf{m}_t = b_1 \\mathbf{m}_{t-1} + (1-b_1) \\mathbf{g}(\\boldsymbol{\\beta}_t) \\mathbf{v}_t = b_2 \\mathbf{v}_{t-1} + (1-b_2) \\mathbf{g}(\\boldsymbol{\\beta}_t)^2 \\boldsymbol{\\beta}_{t+1} = \\boldsymbol{\\beta}_t - \\eta \\frac{\\mathbf{m}_t}{\\sqrt{\\mathbf{v}_t + \\epsilon}}\nCommon hyperparameter values are:\n\n\\eta = 0.001 (initial learning rate)\nb_1 = 0.9 (momentum decay)\nb_2 = 0.999 (squared gradient decay)\n\\epsilon = 10^{-8} (numerical stability constant)"
  },
  {
    "objectID": "lectures/summaries/Lecture6Summary.html#practical-considerations",
    "href": "lectures/summaries/Lecture6Summary.html#practical-considerations",
    "title": "Lecture 6 Summary: Adaptive Methods for Minimization",
    "section": "6. Practical Considerations",
    "text": "6. Practical Considerations\nThe choice of optimization method significantly impacts model performance:\n\nStandard SGD: Simple but requires careful learning rate tuning and may converge slowly\nMomentum: Faster convergence in flat regions but still sensitive to learning rate\nAdaGrad/RMSprop: Parameter-specific adaptation but may require tuning of decay rates\nAdam: Generally robust performance across a variety of problems, particularly in high-dimensional spaces\n\nThese adaptive methods form the foundation of modern deep learning optimization, enabling effective training across diverse architectures and problem domains, especially for models with numerous parameters and complex loss landscapes."
  },
  {
    "objectID": "lectures/summaries/Lecture8Summary.html",
    "href": "lectures/summaries/Lecture8Summary.html",
    "title": "Lecture 8 Summary: Introduction to Neural Networks",
    "section": "",
    "text": "This summary introduces neural networks as powerful universal approximators, explaining their structure, functioning, and ability to model complex nonlinear relationships through learnable feature transformations."
  },
  {
    "objectID": "lectures/summaries/Lecture8Summary.html#the-limitations-of-linear-models",
    "href": "lectures/summaries/Lecture8Summary.html#the-limitations-of-linear-models",
    "title": "Lecture 8 Summary: Introduction to Neural Networks",
    "section": "1. The Limitations of Linear Models",
    "text": "1. The Limitations of Linear Models\nLinear models fail to capture complex decision boundaries in classification problems, as exemplified by the XOR problem:\n\\mathbf{X} =\n\\begin{bmatrix}\n0 & 0 \\\\\n0 & 1 \\\\\n1 & 0 \\\\\n1 & 1\n\\end{bmatrix},\n\\mathbf{y} =\n\\begin{bmatrix}\n0 \\\\\n1 \\\\\n1 \\\\\n0\n\\end{bmatrix}\nWhile polynomial expansions can solve this problem (e.g., a 5th-degree polynomial), they quickly become unwieldy as dimensionality increases, requiring \\binom{P+d}{d} \\approx \\frac{P^d}{d!} parameters. For high-dimensional data, this approach suffers from:\n\nExponential parameter growth\nPoor generalization\nHigh computational complexity\nLimited flexibility"
  },
  {
    "objectID": "lectures/summaries/Lecture8Summary.html#neural-network-architecture",
    "href": "lectures/summaries/Lecture8Summary.html#neural-network-architecture",
    "title": "Lecture 8 Summary: Introduction to Neural Networks",
    "section": "2. Neural Network Architecture",
    "text": "2. Neural Network Architecture\nNeural networks overcome these limitations through a hierarchical architecture that transforms the input features through multiple processing layers.\n2.1. The Single Hidden Layer Network\nThe simplest neural network consists of:\n\nAn input layer representing the features\nA hidden layer that performs feature transformations\nAn output layer that produces the final prediction\n\nMathematically, this is represented as:\n\\theta_i = \\boldsymbol{\\beta}^T \\phi(\\mathbf{x}_i; \\mathbf{W}, \\mathbf{c}) + b\nWhere:\n\n\\mathbf{W} is a weight matrix with dimensions P \\times K (features × hidden units)\n\\mathbf{c} is a vector of bias terms for the hidden layer\n\\phi() is a nonlinear activation function\n\\boldsymbol{\\beta} is a vector of weights connecting the hidden layer to the output\nb is a bias term for the output layer\n\n2.2. The Critical Role of Activation Functions\nThe key innovation in neural networks is the introduction of nonlinearity through activation functions. Without nonlinear activation, multi-layer networks would reduce to simple linear models:\n\\theta_i = \\boldsymbol{\\beta}^T(\\mathbf{W}^T \\mathbf{x}_i + \\mathbf{c}) + b = \\boldsymbol{\\beta}^T\\mathbf{W}^T \\mathbf{x}_i + (\\boldsymbol{\\beta}^T\\mathbf{c} + b)\nActivation functions transform linear combinations of inputs into nonlinear representations. The Rectified Linear Unit (ReLU) is a common activation function:\n\\varphi(x) = \\max(0, x)\nApplied element-wise, ReLU introduces “kinks” in the function space, allowing neural networks to approximate complex, nonlinear decision boundaries."
  },
  {
    "objectID": "lectures/summaries/Lecture8Summary.html#learning-representations",
    "href": "lectures/summaries/Lecture8Summary.html#learning-representations",
    "title": "Lecture 8 Summary: Introduction to Neural Networks",
    "section": "3. Learning Representations",
    "text": "3. Learning Representations\nNeural networks learn useful feature transformations from data, engineering new feature spaces where classification or regression becomes more tractable.\n3.1. Hidden Representations\nFor each input \\mathbf{x}_i, the network computes a hidden representation:\n\\mathbf{h}_i = \\varphi(\\mathbf{W}^T \\mathbf{x}_i + \\mathbf{c})\nThis transformation maps the original features to a new space where:\n\nPoints that were not linearly separable might become separable\nComplex relationships might be simplified\nRelevant patterns are emphasized while noise is suppressed\n\n3.2. Geometric Interpretation\nEach hidden unit in a ReLU network creates a partition in the feature space:\n\nOn one side of the partition, the unit outputs zero\nOn the other side, it outputs a linear function of the input\nThe combination of multiple hidden units creates complex, piecewise linear decision boundaries\n\nWith sufficient hidden units, a single-layer neural network can approximate any continuous function on a bounded domain."
  },
  {
    "objectID": "lectures/summaries/Lecture8Summary.html#deep-neural-networks",
    "href": "lectures/summaries/Lecture8Summary.html#deep-neural-networks",
    "title": "Lecture 8 Summary: Introduction to Neural Networks",
    "section": "4. Deep Neural Networks",
    "text": "4. Deep Neural Networks\nWhile single-layer networks are universal approximators, they may require an impractically large number of hidden units for complex functions. Deep neural networks (with multiple hidden layers) offer a more efficient alternative.\n4.1. Architecture of Deep Networks\nA two-layer network is represented as:\n\\theta_i = g(\\boldsymbol{\\beta}^T \\varphi(\\mathbf{W}_2^T \\varphi(\\mathbf{W}_1^T \\mathbf{x}_i + \\mathbf{c}_1) + \\mathbf{c}_2) + b)\nWhere:\n\n\\mathbf{W}_1 is the weight matrix for the first hidden layer\n\\mathbf{c}_1 is the bias vector for the first hidden layer\n\\mathbf{W}_2 is the weight matrix for the second hidden layer\n\\mathbf{c}_2 is the bias vector for the second hidden layer\n\n4.2. Benefits of Depth Over Width\nDeep networks offer several advantages over equivalently-sized wide networks:\n\nHierarchical Feature Learning: Each layer builds upon representations from previous layers\nParameter Efficiency: Deep networks can represent complex functions with fewer parameters\nCompositional Structure: The nested structure allows for representing compositional relationships\nInductive Bias: The hierarchical structure aligns well with many real-world data types"
  },
  {
    "objectID": "lectures/summaries/Lecture8Summary.html#training-neural-networks",
    "href": "lectures/summaries/Lecture8Summary.html#training-neural-networks",
    "title": "Lecture 8 Summary: Introduction to Neural Networks",
    "section": "5. Training Neural Networks",
    "text": "5. Training Neural Networks\nNeural networks are trained through empirical risk minimization, typically using variants of gradient descent.\n5.1. Loss Functions\nCommon loss functions include:\n\nMean Squared Error for regression tasks\nCross-Entropy Loss for classification tasks: \\mathcal{L}(\\boldsymbol{\\theta} | \\mathbf{X}, \\mathbf{y}) = -\\frac{1}{N} \\sum_{i=1}^N [y_i \\log(\\theta_i) + (1-y_i)\\log(1-\\theta_i)]\n\n5.2. Optimization Challenges\nNeural networks pose unique optimization challenges:\n\nNon-convex loss landscapes with multiple local minima\nGradient vanishing or explosion in deep networks\nHigh sensitivity to initialization and hyperparameters\nLarge numbers of parameters requiring efficient optimization techniques\n\nModern neural network training relies on:\n\nStochastic gradient descent and adaptive optimization methods\nProper initialization techniques\nRegularization strategies\nBatch normalization and other stabilization techniques"
  },
  {
    "objectID": "lectures/summaries/Lecture8Summary.html#universal-approximation-properties",
    "href": "lectures/summaries/Lecture8Summary.html#universal-approximation-properties",
    "title": "Lecture 8 Summary: Introduction to Neural Networks",
    "section": "6. Universal Approximation Properties",
    "text": "6. Universal Approximation Properties\nNeural networks with just a single hidden layer of sufficient width can approximate any continuous function on a bounded domain with arbitrary precision. This theoretical result, known as the Universal Approximation Theorem, explains why neural networks can model highly complex relationships.\nThe ability to learn representations directly from data, combined with their universal approximation properties, makes neural networks particularly powerful for complex tasks where feature engineering is difficult, such as image recognition, speech processing, and natural language understanding."
  },
  {
    "objectID": "lectures/toc.html",
    "href": "lectures/toc.html",
    "title": "Course Content Map",
    "section": "",
    "text": "This page organizes all lecture materials by topic area to help you navigate the course content.\n\n\n\nLecture 1: Introduction to Machine Learning\nLecture 2: Machine Learning Fundamentals\nLecture 3: Generalization: The Predictive Goal\nLecture 4: Bias-Variance Tradeoff\n\n\n\n\n\nLecture 5: Linear Regression & Regularization\nLecture 6: Logistic Regression\nLecture 7: Specialized Regularizers\nLecture 8: Generalized Linear Models\n\n\n\n\n\nLecture 9: Decision Trees\nLecture 10: Backpropagation and Gradient Problems\nLecture 11: Vanishing Gradients and Generalization for Deep Neural Networks\nLecture 12: Boosting and Ensembles\n\n\n\n\n\nLecture 13: Neural Networks I\nLecture 14: Neural Networks II\nLecture 15: Neural Networks III - Optimization\nLecture 16: Computer Vision and CNNs\nLecture 17: Advanced CNNs\nLecture 18: Recurrent Neural Networks\nLecture 19: Transformers\n\n\n\n\n\nLecture 20: Generative Models: Autoregressives and Autoencoders\nLecture 21: Variational Autoencoders (VAEs)\nLecture 22: Bayesian Machine Learning\n\n\n\n\n\nLecture 23: Generative Adversarial Networks\nLecture 24: Diffusion Models\nLecture 25: Self-supervised Learning\nLecture 26: Reinforcement Learning I\nLecture 27: Reinforcement Learning II\nLecture 28: The Future of AI"
  },
  {
    "objectID": "lectures/toc.html#course-structure",
    "href": "lectures/toc.html#course-structure",
    "title": "Course Content Map",
    "section": "",
    "text": "This page organizes all lecture materials by topic area to help you navigate the course content.\n\n\n\nLecture 1: Introduction to Machine Learning\nLecture 2: Machine Learning Fundamentals\nLecture 3: Generalization: The Predictive Goal\nLecture 4: Bias-Variance Tradeoff\n\n\n\n\n\nLecture 5: Linear Regression & Regularization\nLecture 6: Logistic Regression\nLecture 7: Specialized Regularizers\nLecture 8: Generalized Linear Models\n\n\n\n\n\nLecture 9: Decision Trees\nLecture 10: Backpropagation and Gradient Problems\nLecture 11: Vanishing Gradients and Generalization for Deep Neural Networks\nLecture 12: Boosting and Ensembles\n\n\n\n\n\nLecture 13: Neural Networks I\nLecture 14: Neural Networks II\nLecture 15: Neural Networks III - Optimization\nLecture 16: Computer Vision and CNNs\nLecture 17: Advanced CNNs\nLecture 18: Recurrent Neural Networks\nLecture 19: Transformers\n\n\n\n\n\nLecture 20: Generative Models: Autoregressives and Autoencoders\nLecture 21: Variational Autoencoders (VAEs)\nLecture 22: Bayesian Machine Learning\n\n\n\n\n\nLecture 23: Generative Adversarial Networks\nLecture 24: Diffusion Models\nLecture 25: Self-supervised Learning\nLecture 26: Reinforcement Learning I\nLecture 27: Reinforcement Learning II\nLecture 28: The Future of AI"
  }
]